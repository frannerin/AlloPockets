{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214a36bc-3bf0-415a-a8c9-718fb6ed5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a35505-4e1f-46fa-9d83-f001819bc43a",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426d9154-c7e7-4e4c-be28-6d28dd82a371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Pockets</th>\n",
       "      <th>Label</th>\n",
       "      <th colspan=\"4\" halign=\"left\">FPocket</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">HHBlits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pdb</th>\n",
       "      <th>pocket</th>\n",
       "      <th>nres</th>\n",
       "      <th>site_in_pocket</th>\n",
       "      <th>pocket_in_site</th>\n",
       "      <th>label</th>\n",
       "      <th>Pocket Score</th>\n",
       "      <th>Drug Score</th>\n",
       "      <th>Number of alpha spheres</th>\n",
       "      <th>Mean alpha-sphere radius</th>\n",
       "      <th>...</th>\n",
       "      <th>M-&gt;M</th>\n",
       "      <th>M-&gt;I</th>\n",
       "      <th>M-&gt;D</th>\n",
       "      <th>I-&gt;M</th>\n",
       "      <th>I-&gt;I</th>\n",
       "      <th>D-&gt;M</th>\n",
       "      <th>D-&gt;D</th>\n",
       "      <th>Neff</th>\n",
       "      <th>Neff_I</th>\n",
       "      <th>Neff_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6ta3</td>\n",
       "      <td>pocket16</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0160</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>79.0</td>\n",
       "      <td>3.4016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971722</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>0.176411</td>\n",
       "      <td>0.823637</td>\n",
       "      <td>0.151071</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>9.994750</td>\n",
       "      <td>1.156583</td>\n",
       "      <td>2.557083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6ta3</td>\n",
       "      <td>pocket18</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0390</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.5600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974105</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.013511</td>\n",
       "      <td>0.187106</td>\n",
       "      <td>0.812923</td>\n",
       "      <td>0.253341</td>\n",
       "      <td>0.746686</td>\n",
       "      <td>9.824308</td>\n",
       "      <td>1.154846</td>\n",
       "      <td>2.152077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6ta3</td>\n",
       "      <td>pocket11</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.4367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.491783</td>\n",
       "      <td>0.508309</td>\n",
       "      <td>0.206654</td>\n",
       "      <td>0.793426</td>\n",
       "      <td>10.325625</td>\n",
       "      <td>1.141812</td>\n",
       "      <td>1.770438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6ta3</td>\n",
       "      <td>pocket15</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.3932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888404</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>0.274259</td>\n",
       "      <td>0.725673</td>\n",
       "      <td>0.135536</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>10.183727</td>\n",
       "      <td>1.275091</td>\n",
       "      <td>3.316091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ta3</td>\n",
       "      <td>pocket7</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>244.0</td>\n",
       "      <td>3.5880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957620</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>0.020451</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>0.627444</td>\n",
       "      <td>0.219810</td>\n",
       "      <td>0.780225</td>\n",
       "      <td>10.475976</td>\n",
       "      <td>1.187833</td>\n",
       "      <td>1.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8136</th>\n",
       "      <td>3d2p</td>\n",
       "      <td>pocket5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2167</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>41.0</td>\n",
       "      <td>3.4642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989698</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.505737</td>\n",
       "      <td>0.494248</td>\n",
       "      <td>0.289794</td>\n",
       "      <td>0.710212</td>\n",
       "      <td>12.009636</td>\n",
       "      <td>1.059273</td>\n",
       "      <td>1.135545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8137</th>\n",
       "      <td>3d2p</td>\n",
       "      <td>pocket8</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0886</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3.5617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912140</td>\n",
       "      <td>0.040368</td>\n",
       "      <td>0.047440</td>\n",
       "      <td>0.444674</td>\n",
       "      <td>0.502660</td>\n",
       "      <td>0.245143</td>\n",
       "      <td>0.754912</td>\n",
       "      <td>8.720632</td>\n",
       "      <td>1.300158</td>\n",
       "      <td>2.198632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>3d2p</td>\n",
       "      <td>pocket3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3030</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.4781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954826</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.390967</td>\n",
       "      <td>0.432574</td>\n",
       "      <td>0.214760</td>\n",
       "      <td>0.785243</td>\n",
       "      <td>8.712882</td>\n",
       "      <td>1.092588</td>\n",
       "      <td>1.600118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>3d2p</td>\n",
       "      <td>pocket12</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0406</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.6300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978512</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.558250</td>\n",
       "      <td>0.298870</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.790977</td>\n",
       "      <td>8.564143</td>\n",
       "      <td>0.943143</td>\n",
       "      <td>2.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>3d2p</td>\n",
       "      <td>pocket6</td>\n",
       "      <td>16</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1532</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.5363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938259</td>\n",
       "      <td>0.039010</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.460395</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>0.635664</td>\n",
       "      <td>9.773813</td>\n",
       "      <td>1.138000</td>\n",
       "      <td>1.508125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4112 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pockets                                              Label      FPocket  \\\n",
       "         pdb    pocket nres site_in_pocket pocket_in_site label Pocket Score   \n",
       "0       6ta3  pocket16   12       0.000000         0.0000     0      -0.0160   \n",
       "1       6ta3  pocket18   13       0.000000         0.0000     0      -0.0390   \n",
       "2       6ta3  pocket11   16       0.000000         0.0000     0       0.0606   \n",
       "3       6ta3  pocket15   11       0.000000         0.0000     0      -0.0097   \n",
       "4       6ta3   pocket7   42       0.000000         0.0000     0       0.2518   \n",
       "...      ...       ...  ...            ...            ...   ...          ...   \n",
       "8136    3d2p   pocket5   11       0.000000         0.0000     0       0.2167   \n",
       "8137    3d2p   pocket8   19       0.000000         0.0000     0       0.0886   \n",
       "8138    3d2p   pocket3   17       0.000000         0.0000     0       0.3030   \n",
       "8139    3d2p  pocket12    7       0.000000         0.0000     0      -0.0406   \n",
       "8140    3d2p   pocket6   16       0.043478         0.0625     0       0.1532   \n",
       "\n",
       "                                                                  ...  \\\n",
       "     Drug Score Number of alpha spheres Mean alpha-sphere radius  ...   \n",
       "0        0.0145                    79.0                   3.4016  ...   \n",
       "1        0.0001                    48.0                   3.5600  ...   \n",
       "2        0.0180                    74.0                   3.4367  ...   \n",
       "3        0.0023                    37.0                   3.3932  ...   \n",
       "4        0.9883                   244.0                   3.5880  ...   \n",
       "...         ...                     ...                      ...  ...   \n",
       "8136     0.0033                    41.0                   3.4642  ...   \n",
       "8137     0.0057                   120.0                   3.5617  ...   \n",
       "8138     0.0287                    75.0                   3.4781  ...   \n",
       "8139     0.0007                    36.0                   3.6300  ...   \n",
       "8140     0.0075                    92.0                   3.5363  ...   \n",
       "\n",
       "       HHBlits                                                              \\\n",
       "          M->M      M->I      M->D      I->M      I->I      D->M      D->D   \n",
       "0     0.971722  0.011249  0.017076  0.176411  0.823637  0.151071  0.848930   \n",
       "1     0.974105  0.012294  0.013511  0.187106  0.812923  0.253341  0.746686   \n",
       "2     0.968439  0.015852  0.015674  0.491783  0.508309  0.206654  0.793426   \n",
       "3     0.888404  0.030708  0.080922  0.274259  0.725673  0.135536  0.864436   \n",
       "4     0.957620  0.021926  0.020451  0.348753  0.627444  0.219810  0.780225   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8136  0.989698  0.006181  0.004065  0.505737  0.494248  0.289794  0.710212   \n",
       "8137  0.912140  0.040368  0.047440  0.444674  0.502660  0.245143  0.754912   \n",
       "8138  0.954826  0.028198  0.016987  0.390967  0.432574  0.214760  0.785243   \n",
       "8139  0.978512  0.008635  0.012686  0.558250  0.298870  0.209000  0.790977   \n",
       "8140  0.938259  0.039010  0.022726  0.460395  0.352079  0.364345  0.635664   \n",
       "\n",
       "                                     \n",
       "           Neff    Neff_I    Neff_D  \n",
       "0      9.994750  1.156583  2.557083  \n",
       "1      9.824308  1.154846  2.152077  \n",
       "2     10.325625  1.141812  1.770438  \n",
       "3     10.183727  1.275091  3.316091  \n",
       "4     10.475976  1.187833  1.982500  \n",
       "...         ...       ...       ...  \n",
       "8136  12.009636  1.059273  1.135545  \n",
       "8137   8.720632  1.300158  2.198632  \n",
       "8138   8.712882  1.092588  1.600118  \n",
       "8139   8.564143  0.943143  2.130000  \n",
       "8140   9.773813  1.138000  1.508125  \n",
       "\n",
       "[4112 rows x 194 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../training_data/6.Training_sets/pockets_train.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd30ddbb-5492-4633-812e-27a17344ff54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Pockets</th>\n",
       "      <th>Label</th>\n",
       "      <th colspan=\"4\" halign=\"left\">FPocket</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">HHBlits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pdb</th>\n",
       "      <th>pocket</th>\n",
       "      <th>nres</th>\n",
       "      <th>site_in_pocket</th>\n",
       "      <th>pocket_in_site</th>\n",
       "      <th>label</th>\n",
       "      <th>Pocket Score</th>\n",
       "      <th>Drug Score</th>\n",
       "      <th>Number of alpha spheres</th>\n",
       "      <th>Mean alpha-sphere radius</th>\n",
       "      <th>...</th>\n",
       "      <th>M-&gt;M</th>\n",
       "      <th>M-&gt;I</th>\n",
       "      <th>M-&gt;D</th>\n",
       "      <th>I-&gt;M</th>\n",
       "      <th>I-&gt;I</th>\n",
       "      <th>D-&gt;M</th>\n",
       "      <th>D-&gt;D</th>\n",
       "      <th>Neff</th>\n",
       "      <th>Neff_I</th>\n",
       "      <th>Neff_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5kwj</td>\n",
       "      <td>pocket1</td>\n",
       "      <td>46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5169</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>298.0</td>\n",
       "      <td>3.4265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943829</td>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.036645</td>\n",
       "      <td>0.374693</td>\n",
       "      <td>0.538332</td>\n",
       "      <td>0.188763</td>\n",
       "      <td>0.811216</td>\n",
       "      <td>12.796000</td>\n",
       "      <td>1.211870</td>\n",
       "      <td>4.778174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5kwj</td>\n",
       "      <td>pocket2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4387</td>\n",
       "      <td>0.3067</td>\n",
       "      <td>86.0</td>\n",
       "      <td>3.3908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981054</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.325240</td>\n",
       "      <td>0.674772</td>\n",
       "      <td>0.140677</td>\n",
       "      <td>0.859313</td>\n",
       "      <td>13.194263</td>\n",
       "      <td>1.298421</td>\n",
       "      <td>3.082842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5kwj</td>\n",
       "      <td>pocket4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0222</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.7301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982741</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.008821</td>\n",
       "      <td>0.355841</td>\n",
       "      <td>0.585323</td>\n",
       "      <td>0.447145</td>\n",
       "      <td>0.552787</td>\n",
       "      <td>12.890706</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>3.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5kwj</td>\n",
       "      <td>pocket5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0381</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.5630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939657</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>0.046819</td>\n",
       "      <td>0.290462</td>\n",
       "      <td>0.709510</td>\n",
       "      <td>0.161189</td>\n",
       "      <td>0.838811</td>\n",
       "      <td>13.071500</td>\n",
       "      <td>1.273100</td>\n",
       "      <td>5.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5kwj</td>\n",
       "      <td>pocket3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.6264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945818</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.346038</td>\n",
       "      <td>0.654003</td>\n",
       "      <td>0.080462</td>\n",
       "      <td>0.919533</td>\n",
       "      <td>13.144091</td>\n",
       "      <td>1.502727</td>\n",
       "      <td>4.729273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>4op0</td>\n",
       "      <td>pocket10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.6797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965050</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.476154</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.371946</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>8.875000</td>\n",
       "      <td>1.278600</td>\n",
       "      <td>1.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>4op0</td>\n",
       "      <td>pocket5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.5080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937658</td>\n",
       "      <td>0.029391</td>\n",
       "      <td>0.033036</td>\n",
       "      <td>0.435269</td>\n",
       "      <td>0.564699</td>\n",
       "      <td>0.380508</td>\n",
       "      <td>0.619474</td>\n",
       "      <td>8.783900</td>\n",
       "      <td>1.317900</td>\n",
       "      <td>1.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>4op0</td>\n",
       "      <td>pocket8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.6047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.036707</td>\n",
       "      <td>0.076581</td>\n",
       "      <td>0.679256</td>\n",
       "      <td>0.320693</td>\n",
       "      <td>0.594901</td>\n",
       "      <td>0.405162</td>\n",
       "      <td>8.536545</td>\n",
       "      <td>1.264727</td>\n",
       "      <td>1.994273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>4op0</td>\n",
       "      <td>pocket3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3124</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>109.0</td>\n",
       "      <td>3.6421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977747</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.390611</td>\n",
       "      <td>0.434866</td>\n",
       "      <td>0.565146</td>\n",
       "      <td>8.767200</td>\n",
       "      <td>0.862850</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8122</th>\n",
       "      <td>4op0</td>\n",
       "      <td>pocket6</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>0.6417</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.6336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950238</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>0.032596</td>\n",
       "      <td>0.595731</td>\n",
       "      <td>0.345453</td>\n",
       "      <td>0.552496</td>\n",
       "      <td>0.329852</td>\n",
       "      <td>8.337294</td>\n",
       "      <td>1.154706</td>\n",
       "      <td>1.500765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pockets                                              Label      FPocket  \\\n",
       "         pdb    pocket nres site_in_pocket pocket_in_site label Pocket Score   \n",
       "22      5kwj   pocket1   46       1.000000       0.217391     1       0.5169   \n",
       "23      5kwj   pocket2   19       0.000000       0.000000     0       0.4387   \n",
       "24      5kwj   pocket4   17       0.000000       0.000000     0      -0.0222   \n",
       "25      5kwj   pocket5   10       0.000000       0.000000     0      -0.0381   \n",
       "26      5kwj   pocket3   11       0.000000       0.000000     0       0.0289   \n",
       "...      ...       ...  ...            ...            ...   ...          ...   \n",
       "8118    4op0  pocket10   10       0.000000       0.000000     0       0.0518   \n",
       "8119    4op0   pocket5   10       0.000000       0.000000     0       0.1234   \n",
       "8120    4op0   pocket8   11       0.000000       0.000000     0       0.1032   \n",
       "8121    4op0   pocket3   20       0.139535       0.300000     0       0.3124   \n",
       "8122    4op0   pocket6   17       0.000000       0.000000     0       0.1055   \n",
       "\n",
       "                                                                  ...  \\\n",
       "     Drug Score Number of alpha spheres Mean alpha-sphere radius  ...   \n",
       "22       0.9745                   298.0                   3.4265  ...   \n",
       "23       0.3067                    86.0                   3.3908  ...   \n",
       "24       0.0002                    65.0                   3.7301  ...   \n",
       "25       0.0066                    52.0                   3.5630  ...   \n",
       "26       0.0029                    44.0                   3.6264  ...   \n",
       "...         ...                     ...                      ...  ...   \n",
       "8118     0.0375                    36.0                   3.6797  ...   \n",
       "8119     0.0849                    43.0                   3.5080  ...   \n",
       "8120     0.1044                    58.0                   3.6047  ...   \n",
       "8121     0.0052                   109.0                   3.6421  ...   \n",
       "8122     0.6417                    92.0                   3.6336  ...   \n",
       "\n",
       "       HHBlits                                                              \\\n",
       "          M->M      M->I      M->D      I->M      I->I      D->M      D->D   \n",
       "22    0.943829  0.019534  0.036645  0.374693  0.538332  0.188763  0.811216   \n",
       "23    0.981054  0.011601  0.007315  0.325240  0.674772  0.140677  0.859313   \n",
       "24    0.982741  0.008372  0.008821  0.355841  0.585323  0.447145  0.552787   \n",
       "25    0.939657  0.013509  0.046819  0.290462  0.709510  0.161189  0.838811   \n",
       "26    0.945818  0.024418  0.029730  0.346038  0.654003  0.080462  0.919533   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8118  0.965050  0.021347  0.013608  0.476154  0.523834  0.371946  0.627957   \n",
       "8119  0.937658  0.029391  0.033036  0.435269  0.564699  0.380508  0.619474   \n",
       "8120  0.886737  0.036707  0.076581  0.679256  0.320693  0.594901  0.405162   \n",
       "8121  0.977747  0.008794  0.013359  0.359375  0.390611  0.434866  0.565146   \n",
       "8122  0.950238  0.017191  0.032596  0.595731  0.345453  0.552496  0.329852   \n",
       "\n",
       "                                     \n",
       "           Neff    Neff_I    Neff_D  \n",
       "22    12.796000  1.211870  4.778174  \n",
       "23    13.194263  1.298421  3.082842  \n",
       "24    12.890706  1.117647  3.610000  \n",
       "25    13.071500  1.273100  5.003400  \n",
       "26    13.144091  1.502727  4.729273  \n",
       "...         ...       ...       ...  \n",
       "8118   8.875000  1.278600  1.569400  \n",
       "8119   8.783900  1.317900  1.763600  \n",
       "8120   8.536545  1.264727  1.994273  \n",
       "8121   8.767200  0.862850  1.343000  \n",
       "8122   8.337294  1.154706  1.500765  \n",
       "\n",
       "[1236 rows x 194 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../training_data/6.Training_sets/pockets_test.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56004ef-4e36-4553-bfa8-50024bac7afb",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93f959e0-205c-43d5-80e3-16398a132f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3685474/2525020029.py:19: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = corr_func(series1, series2) # series1[valid], series2[valid]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>feature1</th>\n",
       "      <th>group2</th>\n",
       "      <th>feature2</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Pocket Score</td>\n",
       "      <td>Amino acids</td>\n",
       "      <td>label_comp_id_A</td>\n",
       "      <td>0.052135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Pocket Score</td>\n",
       "      <td>Amino acids</td>\n",
       "      <td>label_comp_id_C</td>\n",
       "      <td>0.018248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Pocket Score</td>\n",
       "      <td>Amino acids</td>\n",
       "      <td>label_comp_id_D</td>\n",
       "      <td>-0.095329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Pocket Score</td>\n",
       "      <td>Amino acids</td>\n",
       "      <td>label_comp_id_E</td>\n",
       "      <td>-0.149457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Pocket Score</td>\n",
       "      <td>Amino acids</td>\n",
       "      <td>label_comp_id_F</td>\n",
       "      <td>0.073096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Flexibility</td>\n",
       "      <td>HHBlits</td>\n",
       "      <td>D-&gt;M</td>\n",
       "      <td>-0.048194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Flexibility</td>\n",
       "      <td>HHBlits</td>\n",
       "      <td>D-&gt;D</td>\n",
       "      <td>-0.014649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Flexibility</td>\n",
       "      <td>HHBlits</td>\n",
       "      <td>Neff</td>\n",
       "      <td>-0.010939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Flexibility</td>\n",
       "      <td>HHBlits</td>\n",
       "      <td>Neff_I</td>\n",
       "      <td>-0.072048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Flexibility</td>\n",
       "      <td>HHBlits</td>\n",
       "      <td>Neff_D</td>\n",
       "      <td>-0.038339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3652 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       group1      feature1       group2         feature2  correlation\n",
       "0     FPocket  Pocket Score  Amino acids  label_comp_id_A     0.052135\n",
       "1     FPocket  Pocket Score  Amino acids  label_comp_id_C     0.018248\n",
       "2     FPocket  Pocket Score  Amino acids  label_comp_id_D    -0.095329\n",
       "3     FPocket  Pocket Score  Amino acids  label_comp_id_E    -0.149457\n",
       "4     FPocket  Pocket Score  Amino acids  label_comp_id_F     0.073096\n",
       "...       ...           ...          ...              ...          ...\n",
       "3647  FPocket   Flexibility      HHBlits             D->M    -0.048194\n",
       "3648  FPocket   Flexibility      HHBlits             D->D    -0.014649\n",
       "3649  FPocket   Flexibility      HHBlits             Neff    -0.010939\n",
       "3650  FPocket   Flexibility      HHBlits           Neff_I    -0.072048\n",
       "3651  FPocket   Flexibility      HHBlits           Neff_D    -0.038339\n",
       "\n",
       "[3652 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def cross_group_correlations(df, method='pearson'):\n",
    "    corr_func = {'pearson': pearsonr, 'spearman': spearmanr}[method]\n",
    "    # cols = df.columns\n",
    "    cols = set( df.columns.get_level_values(0).unique() ) - {\"Pockets\", \"Label\", \"FPocket\"}\n",
    "    results = []\n",
    "\n",
    "    g1 = \"FPocket\"\n",
    "    # for i, col1 in enumerate(cols):\n",
    "    for g2 in cols:\n",
    "        for f1 in df[g1]:\n",
    "            for f2 in df[g2]:\n",
    "                series1 = df[(g1, f1)]\n",
    "                series2 = df[(g2, f2)]\n",
    "                # valid = series1.notna() & series2.notna()\n",
    "                # if valid.sum() < 2:\n",
    "                #     continue\n",
    "                corr, _ = corr_func(series1, series2) # series1[valid], series2[valid]\n",
    "                results.append({\n",
    "                    'group1': g1, 'feature1': f1,\n",
    "                    'group2': g2, 'feature2': f2,\n",
    "                    'correlation': corr\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "corrs = cross_group_correlations(train_data)\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15ed5f03-3ddf-48c9-ad55-1c095bd32c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>feature1</th>\n",
       "      <th>group2</th>\n",
       "      <th>feature2</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_miyazawa</td>\n",
       "      <td>0.973125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_3</td>\n",
       "      <td>0.965673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>transmembranetendency</td>\n",
       "      <td>0.955661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_manavalan</td>\n",
       "      <td>0.955134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_sweet</td>\n",
       "      <td>0.952752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_rose</td>\n",
       "      <td>0.949307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_mobility</td>\n",
       "      <td>0.947806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_2</td>\n",
       "      <td>0.946838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_4</td>\n",
       "      <td>0.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_fauchere</td>\n",
       "      <td>0.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_ph7_5</td>\n",
       "      <td>0.929956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_doolittle</td>\n",
       "      <td>0.926777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_black</td>\n",
       "      <td>0.921951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>totalbeta_strand</td>\n",
       "      <td>0.918558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_argos</td>\n",
       "      <td>0.915143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>beta_sheetfasman</td>\n",
       "      <td>0.907414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>refractivity</td>\n",
       "      <td>0.903469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_roseman</td>\n",
       "      <td>0.902358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_leo</td>\n",
       "      <td>0.897080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_wilson</td>\n",
       "      <td>0.894406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>beta_sheetroux</td>\n",
       "      <td>0.892774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_chothia</td>\n",
       "      <td>0.891894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_ph3_4</td>\n",
       "      <td>0.890940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>molecularweight</td>\n",
       "      <td>0.888099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_eisenberg</td>\n",
       "      <td>0.876136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hplctfa</td>\n",
       "      <td>0.873613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>antiparallelbeta_strand</td>\n",
       "      <td>0.869895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hplc2_1</td>\n",
       "      <td>0.867483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_tanford</td>\n",
       "      <td>0.864674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>parallelbeta_strand</td>\n",
       "      <td>0.855245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       group1                       feature1    group2  \\\n",
       "899   FPocket           Hydrophobicity Score  Graphein   \n",
       "1008  FPocket  Amino Acid based volume Score  Graphein   \n",
       "912   FPocket           Hydrophobicity Score  Graphein   \n",
       "892   FPocket           Hydrophobicity Score  Graphein   \n",
       "889   FPocket           Hydrophobicity Score  Graphein   \n",
       "918   FPocket           Hydrophobicity Score  Graphein   \n",
       "909   FPocket           Hydrophobicity Score  Graphein   \n",
       "1007  FPocket  Amino Acid based volume Score  Graphein   \n",
       "873   FPocket           Hydrophobicity Score  Graphein   \n",
       "896   FPocket           Hydrophobicity Score  Graphein   \n",
       "908   FPocket           Hydrophobicity Score  Graphein   \n",
       "891   FPocket           Hydrophobicity Score  Graphein   \n",
       "894   FPocket           Hydrophobicity Score  Graphein   \n",
       "932   FPocket           Hydrophobicity Score  Graphein   \n",
       "900   FPocket           Hydrophobicity Score  Graphein   \n",
       "923   FPocket           Hydrophobicity Score  Graphein   \n",
       "1022  FPocket  Amino Acid based volume Score  Graphein   \n",
       "901   FPocket           Hydrophobicity Score  Graphein   \n",
       "893   FPocket           Hydrophobicity Score  Graphein   \n",
       "905   FPocket           Hydrophobicity Score  Graphein   \n",
       "926   FPocket           Hydrophobicity Score  Graphein   \n",
       "917   FPocket           Hydrophobicity Score  Graphein   \n",
       "907   FPocket           Hydrophobicity Score  Graphein   \n",
       "1017  FPocket  Amino Acid based volume Score  Graphein   \n",
       "888   FPocket           Hydrophobicity Score  Graphein   \n",
       "911   FPocket           Hydrophobicity Score  Graphein   \n",
       "933   FPocket           Hydrophobicity Score  Graphein   \n",
       "913   FPocket           Hydrophobicity Score  Graphein   \n",
       "902   FPocket           Hydrophobicity Score  Graphein   \n",
       "934   FPocket           Hydrophobicity Score  Graphein   \n",
       "\n",
       "                     feature2  correlation  \n",
       "899            hphob_miyazawa     0.973125  \n",
       "1008                    dim_3     0.965673  \n",
       "912     transmembranetendency     0.955661  \n",
       "892           hphob_manavalan     0.955134  \n",
       "889               hphob_sweet     0.952752  \n",
       "918                hphob_rose     0.949307  \n",
       "909            hphob_mobility     0.947806  \n",
       "1007                    dim_2     0.946838  \n",
       "873                     dim_4     0.940516  \n",
       "896            hphob_fauchere     0.940516  \n",
       "908               hphob_ph7_5     0.929956  \n",
       "891           hphob_doolittle     0.926777  \n",
       "894               hphob_black     0.921951  \n",
       "932          totalbeta_strand     0.918558  \n",
       "900               hphob_argos     0.915143  \n",
       "923          beta_sheetfasman     0.907414  \n",
       "1022             refractivity     0.903469  \n",
       "901             hphob_roseman     0.902358  \n",
       "893                 hphob_leo     0.897080  \n",
       "905              hphob_wilson     0.894406  \n",
       "926            beta_sheetroux     0.892774  \n",
       "917             hphob_chothia     0.891894  \n",
       "907               hphob_ph3_4     0.890940  \n",
       "1017          molecularweight     0.888099  \n",
       "888           hphob_eisenberg     0.876136  \n",
       "911                   hplctfa     0.873613  \n",
       "933   antiparallelbeta_strand     0.869895  \n",
       "913                   hplc2_1     0.867483  \n",
       "902             hphob_tanford     0.864674  \n",
       "934       parallelbeta_strand     0.855245  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs.sort_values(\"correlation\", ascending=False).iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156d928d-79dd-4ab6-a5af-048357ad311f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='(FPocket, Hydrophobicity Score)', ylabel='(Graphein, hphob_miyazawa)'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABoEklEQVR4nO3dd3xUVfo/8M8kmXQSJgUhlBQSagApiUAgEFzEFUFg19VYKCKrUoJdWXVRVsSG7o+iflEWRDTYUMBFxUIvJhCCNCGQBgmY3khIm/P7IzvjTKbeyUym5PN+vfJ6kTt37py5QObJOc/zHJkQQoCIiIjIAbnZewBEREREhjBQISIiIofFQIWIiIgcFgMVIiIiclgMVIiIiMhhMVAhIiIih8VAhYiIiByWh70H0BZKpRKFhYXo1KkTZDKZvYdDREREZhBCoLq6GmFhYXBzMz5n4tSBSmFhIXr27GnvYRAREZEFLl26hB49ehg9x6kDlU6dOgFoeaMBAQF2Hg0RERGZo6qqCj179lR/jhvj1IGKarknICCAgQoREZGTMSdtg8m0RERE5LAYqBAREZHDYqBCREREDouBChERETksBipERETksBioEBERkcNioEJEREQOi4EKEREROSwGKkREROSwGKgQERGRw3LqFvpERERkO9nFNcgrq0VEsB8iQ/zsMgYGKkRERKSlorYBKamZ2JdVrD6WGBOK1clDEegrb9excOmHiIiItKSkZuLghRKtYwcvlGBR6vF2HwsDFSIiIlLLLq7BvqxiNAuhdbxZCOzLKkZOybV2HQ8DFSIiIlLLK6s1+nhuKQMVIiIispPwIF+jj0cEt29SLQMVIiIiUosK9UdiTCjcZTKt4+4yGRJjQtu9+oeBChERkQvKLq7B7nNFFuWUrE4eioToEK1jCdEhWJ081FrDMxvLk4mIiFyINUqLA33l2DQ3Hjkl15Bbes2ufVQ4o0JERORCrFlaHBnih6S+XewWpAAMVIiIiFyGo5UWWwMDFSIiIhfhaKXF1sAcFSIiIidjaA8eRysttgYGKkRERE7CVKKsqrT44IUSreUfd5kMCdEhds01sRSXfoiIiJyEOYmyjlRabA2cUSEiInJge88VIfNyBboFeGvNpKhoJspGhvg5VGmxNTBQISIishFDuSTmyCu9hmlrD6K8ttGs83NLr2m9RmSIcwcoKgxUiIiIrMwaTdekBCmAcybKmoM5KkRERFbW1qZre88VmR2k2GsPnvbCQIWIiMiKrNF0LfNyhdmvN6xXZ6dNlDUHAxUiIiIrskbTtRt7dDb6eJ8b/pg9Sc8rx6LU46iUsEzkTBioEBERWZE1mq6N69sFCgO5LB5uMlws0g6GDmQV48EP080fpBNhoEJERGRFqqZr7jKZ1nGpuSTbF4zRCVYCvD3QpBQ6y0pKtMys3PneIZebWZEJ0erdOpGqqioEBgaisrISAQEB9h4OERERAKCythGLUo+3qepHZX9WMTLyy9Gniz/W7r6IU4VVBs91kwFjokOxaW68xWNvD1I+vxmoEBER2Yg1m67NXJ+GA1nFUJpx7u4nxzt0FZCUz2/2USEiIrIRazVdU1USmat18zdnxkCFiIjIhLZ0mLUGU5VErblS8zcGKkRERAZYo8OsNZiqJFJx5l2SDWHVDxERkQFt7TBrLYYqiVpz5l2SDWGgQkREpIc1Osxa0+rkoUiIDjH4eFy4ot1netoDAxUiIurwsotrsPtckVbw0dYOs/qu2RaBvnJsmhuP3U+OR2z3ALi1mlzJyK9o95me9sAcFSIi6rCM5aBY2mHW1nktQgicKtDtpaI508McFSIiIgckdRbDWA6KpR1mbZ3XYo29hJwJZ1SIiMjpWTKLYag3iebMxOrkoTodZo0lrJpzTc0Ax5KyZ2vsJeRMGKgQEZHTMzaLYaidvDkzE5Ehftg0N97sDrPmXrMty0OqmZ6DF0q0En1dsTQZ4NIPERE5OUurc6TMTESG+CGpbxeTQYC512zr8pC+CiBXLE0GOKNCREROztxZjNZsMTNhzjWlLg/po6oAsuZeQo6KMypEROTU2pKzYWpmwpzk3NbnmLqmNZNhzZ3pcWacUSEiIqfWlpkRQzMTFbUNmLk+TW8OSem1euSV1SLI1xMrd53Xe46x2Y6OlgzbVjIhWi3qOREp20QTEZHrqqxt1KnOaUvvkpnr03QCHze0BDbltY0Gn6cKjgwl8KqqfN75+QIy8iv0BlaGnutKpHx+c0aFiIicnjVzNgzlkCgBo0EKoJtnogpM9M2+KFoFPa6aDNtWDFSIiMhlRIa0PanUVA6JOU4XVGLpttN6Ax6VqromxEUoMD8pGhHBfhBCIONSuUsnxlqCgQoREXUY5jRYM5VDYo4PD+UiI7/C6DnNQiA9txxBvnKdoMaaLfednV2rfpqamvD8888jMjISPj4+iIqKwrJly6BUKu05LCIicjGq5NgJK/dizoZ0JL25BzPXp6FSz1KOKjnXkg9INxkQF6FAel65Tl8XQ5776pRNW+47O7sGKq+99hree+89rFmzBmfPnsXrr7+ON954A6tXr7bnsIiIyMWYarCmr8R4QJj0Ig2lAGrqmyQ951RhleRmdR2JXZd+Dh8+jDvuuAOTJ08GAERERCA1NRVHjx7Ve359fT3q6+vV31dV6e4eSUREpMlUg7U73zuE9Nxy9XHVssuq5KGYsHKv5Nc7d7XarPPcZTL0D+ukdydkFUPN6joSu86ojBkzBj/99BPOnz8PADhx4gQOHDiA2267Te/5K1asQGBgoPqrZ8+e7TlcIiJyQqaSY4/llWt9b2r3ZFOU/5scMfUBmxAdguXTYo2ew54qdp5ReeaZZ1BZWYl+/frB3d0dzc3NWL58OZKTk/Wev2TJEjz++OPq76uqqhisEBGRUaaSY5WtUklM7Z5srgFhAThV+MdsSWJMKJ68pQ9Kaxu0knk70gaDlrBroPLpp59i8+bN+OSTTzBw4EBkZmbi0UcfRVhYGGbNmqVzvpeXF7y8vOwwUiIiclaGOteaorl7cmpaPpZsPSnpdVffM0x9HWNVRvqCIfZU+YNdA5WnnnoKzz77LO6++24AwKBBg5CXl4cVK1boDVSIiIgsYcnMiOayy02RQWY/r/VsCDcYbBu75qjU1tbCzU17CO7u7ixPJiIiqwr0lePFqQPMOtddJkNiTKhWsCAlX8XS2ZCOsMGgJew6ozJlyhQsX74cvXr1wsCBA3H8+HG89dZbeOCBB+w5LCIickHmdpwdFt4Zf4vroW6Dr6JvViYuQoFZoyMQ6CNHk1IYnA0xp9Ec6WfXQGX16tV44YUXMH/+fBQVFSEsLAwPPfQQ/vnPf9pzWERE5IJMJdX+8/b++PbkVaTnlqvLlTU7xOaU1KCstl7rOT5yD/RU+KKsVYKsSkVtA1JSM9l1tg24ezIREXUYf333EDLyyqGZYKDKKQGgt/rmpqggeLi5mZXf0joI0bcLc0faJdkQKZ/fds1RISIiag+qFvpHWwUpQEtOyRO3xGBfVrHeDrGHLpZiv5lJuK273Rq6JrvOmo+BChERuTx9LfRV+/JsmhuPMj17/mgyd+lBFYTszyo2mROTW8pAxRzcPZmIiFyaoRb6SgGk55Zjf1axVXZM1nT/+jSMCFcYPYddZ83DQIWIiJyCvsoZc6ppTM1s3L8+DbFhAbixZ2ecvFyptVQjg/mzKa0dz6+AwleOqromdp1tAwYqRETkUFoHH/oqZ0b3DoYQwOHsUvUxfdU0mfnleOW/Z0y+pqrVvcJXjnKNZaDOrb6XolkIlNc2Ii5CobXpIbvOSsNAhYiIHIKhUt4mpRK/ZJdpnXvoYmnrp6sTWTfNjdd7LXNU1jViUPcA/GnADegW6I2nv5DWNl+f+UnRiAj2Y9dZCzFQISIih6Av4fVAVrFOlY4hmtU0S7edxgELNhJUCuBkQRVOFlSZPtlMquCEAYplGKgQEZHdGUx4teBaR7JLLNrt2NqYi2IdLE8mIiK7M7e9vXlM78fTFm6yllwWU/v+MBfFOhioEBGR3ZkqDzZjL0D1ZoJSdjq2hFIA3Tv7oM8N/lrHFWyJbxMMVIiIyO5M7U7cerOX0b2DMSoqWOuY5gxGbFiATT/gThVW4ezV6pbX6h6Aob06o6quSesczS61ZDnmqBARkUPQtzuxPjf27IxP5o0EAOSUXFNX0yh85UafH9PFH1lFNVYf95mCKr25NJrJvcxTsRwDFSIicgiBvnJsmhuPnJJrOJJdgiVbT+k9L/NShfrDX7Oa5p73j+gtW+7ftRPeuW84IkP8MOSl71HZauajrUwl/OaWMlBpCy79EBGRQ2n5UDeelHIkWzsgyS6u0RukAFAv0ew9V2T1IMUcbJXfNpxRISIih6DZkdZU4/rWYcwvOfqDFJU5G9KQWyq9skjhK0e/rgFIyynT2QW5NTdZS6KtCsuTrYOBChER2ZW+LrJxEcY39BsQFtDqiPEZGEuClEHdA7B5bksujLHcF3eZDPGRQZC7u2mdw/Jk62CgQkREdqWvI21GXgU6+8hRUad/n503vz+PTXPj1d8HeFv/4+zxW/qq9w1S5c6cLqzEh4dy9e7dE+gr10ru5UyKdTBQISIiuzHUkbZZCINBCgCdapp39160+thUuSWaS1K3Dw7D7YPDDAYkbJVvfRYFKpcuXUJubi5qa2sRGhqKgQMHwsvLy9pjIyIiF9eWjrSqaprs4hqcsuLePCpVdY2YuT5NZ5PE1clDGZC0I7OrfvLy8rBkyRJEREQgIiIC48aNw5///GeMGDECgYGBmDhxIj7//HMolZbszEBERB2RqY60xqhmPKzbfv8Pz319UmdJik3c2p9ZgcrixYsxaNAgZGVlYdmyZTh9+jQqKyvR0NCAq1evYufOnRgzZgxeeOEFDB48GOnp6bYeNxEROZHs4hrsPleEnJJrWsdVHWndJGzP44aWmY3IED9U1DZg7c8XLBrTS1MGGH38VEGVTqWPZhM3ah9mLf14enri4sWLCA0N1XmsS5cumDBhAiZMmIClS5di586dyMvLQ1xcnNUHS0REzkVfRY9q+STQV46K2gaU19ZrlfWaogTQpFSisrYRKamZOJ5fYdHYeoX4ITEmFAcvlGgFJO4yGfp364RThYaXk9jErf3IhDBRGO7AqqqqEBgYiMrKSgQEtC5VIyIie5u5Pg37s4q1uqKo+ousSr4RSW/uQXmt4aRZQ9xkQO/QtrXE3/3keAT5euqUHifGhOKJW/rgjrUHjT6XgYrlpHx+s+qHiIgsplkR0/qDe//5IoMVPfuyinH/+l8sClKAlsZqlgYprRuxvTh1ANJyyiAAjIwKVh83NNvCJm7ty6JA5YsvvsBnn32G/Px8NDQ0aD2WkZFhlYEREZHjMtSkbdboCAwMC4TCV47ZG4znK560QaWOOVR9T0wtS+nbJJFN3Nqf5EBl1apVeO655zBr1ixs27YNc+bMwcWLF5Geno4FCxbYYoxERORg9DVpS88tVzdC8/dyR7MDJRa4oaWb7ep7hqlnQ2auTzNY1bNpbrzWJols4mY/kjclfOedd7Bu3TqsWbMGnp6eePrpp/HDDz8gJSUFlZWVthgjERE5EFWTNmN739TUNxu9Rhd/T2sPyygloJUca+g96KvqiQzxQ1LfLgxS7ERyoJKfn4/Ro0cDAHx8fFBd3bIr5f3334/U1FTrjo6IiOxGs6RY88/W6FsyccANVhihdLmlLQGIqfegOo/sT/LST9euXVFaWorw8HCEh4fjyJEjGDJkCHJycuDEBURERPQ/+nI3NI0IN75hoDk+TrvU5mtYQtUkzlSjOdV5ZH+SZ1QmTJiAHTt2AADmzp2Lxx57DBMnTsRdd92F6dOnW32ARETUvvTln2jKyCs3+Jij0mwSB/zRaM5dpt1pzl0m0zqP7E9yHxWlUgmlUgkPj5bJmM8++wwHDhxAdHQ0Hn74YXh6tt+6I/uoEBGZZqyEWN+5E1bubaeRtV1chAIfzIzD96ev4OkvTxo+L1yBD2bFqXdDBoDK2ka9PVRUVT9kO1I+v9nwjYjIRZkqv9UXwOw+V4Q5JsqKHcWC8b3x1xE91a309TWPc5MBw8MV+Pzh0Qavw6qe9mfTQCUhIQHjxo3D+PHjkZCQAD8/+/2lMlAhIjJMVX7bumFZfGQQ5O5uegOY0mv1TjWjArSM/XpjM9Jyy3Qe6+wjx96nkjhD4mCkfH5LzlG5/fbbkZGRgb/+9a9QKBQYNWoUnn32WXz33XeoqbG8lTEREVmPsfLbw9mlOHBBO1FW1T/EUO6GIzuQVaw3SAGAirpGnLhcoXdDRHIOFi/9NDc3Iz09HXv27MGePXvw888/QyaTob6+3tpjNIgzKkRE+lm6hGNo/xtNAd4eqLreZPHYPNxkaJKyC6EVMQfFMbTLXj9ZWVk4ceIETpw4gV9//RUBAQEYO3aspZcjIiIrMlV+a0hu6TUIITBnTATmJUbicnkdZAC6K3zQpBSICPbDmYJKLEg9Lvna04aGIa/kGo5fsl9zUM3Os+QcJAcqd911F/bt2welUonExEQkJiZiyZIlGDx4sC3GR0REFlAt4bTOUXFDS5dWQ975+QLSDZQfq2Yjuit8LBpTTKg/vj5eaNFzrUWz8ywTZ52D5KUfNzc3hISEYPbs2UhKSsLYsWPh7+9vq/EZxaUfIiLDDJXfNimV+CW7TDuAkQHecnfUNTTD0IeCaudgAAaXhQyRAQavaw8b5sQhqW8Xew+jw7Jp1U9FRQX27duHPXv2YO/evTh9+jSGDBmC8ePHY/z48fjzn//cpsFLwUCFiMi01uW3+gKYjmb3k+M5o2JH7dpH5eLFi3j55ZexefNmKJVKNDcb34jKmhioEBFZLqfkGhZ9kqG1WZ+reHXGIOw8eVVveXZCdAhzVOzMpsm0ZWVl2Lt3r7ra5/Tp0wgKCsIdd9yBpKQkiwdNRES2o6+5mxDCJYMUALgpKhh/ju2mM3OUEB2C1clD7TgykkpyoBIaGoqQkBCMHTsW8+bNw/jx4xEbG2uLsRERURvp606raidvjV2QrSEuQoHbB3XD2t0XUFTTYPbzOvvIUX29Se+MiSoY2zQ3np1nnZzkQOXEiRMMTIiInERKaiYOtMpFSc8rx/g3d2PjnDg7jUpbem45fOQe+PKRBDzwYTqyikw3D02MCcXyabF47utTJmdMIkMYoDgz7vVDROSiTG0wGBehANASKNibaiZk7pgIzDLSqG7FjEEYGRWsFXhwxsT52Lzh2xdffIHPPvsM+fn5aGjQnqbLyMiw5JJERGQGKTshm1raSc8tx/YFCbj/P2morGs0eq6tqfqbGKtEGhUVjOT4XjrHOWPi2iTv9bNq1SrMmTMHXbp0wfHjxxEfH4/g4GBkZ2e3a2kyEVFHUlHbgJnr0zBh5V7M2ZCOpDf3YOb6NFRq7BacXVyjtaeNOd1p88tqMTDM8WekE2NC8d59w+09DLIDyUs//fr1w9KlS5GcnIxOnTrhxIkTiIqKwj//+U+UlZVhzZo1thqrDi79EJGrMTRjYmgn5IToEKxKvlEnYTa2ewBemT4Iy3acwVEDnWaBlsTajPwKres6WnO2j+bGY2xMqNFzpMw0kf3ZdOknPz8fo0ePBgD4+PiguroaAHD//fdj5MiR7RqoEBG5Cn3VOaqW9aXX6vUuiaiWS+ZtOoqMvAqtx04VVGHqmoMY1D0AbjKg9R6AbjJgeLhCb36KIwUpAIxuYGjsvnHjQdcgeemna9euKC0tBQCEh4fjyJEjAICcnBw4cV4uEZFdpaRm4uCFEq1jqg30zMk1aTbw8/dkQZVOkAIA/l4eqDGxA7KbzPiY20tEsOEZEmP3jVyD5EBlwoQJ2LFjBwBg7ty5eOyxxzBx4kTcddddmD59utUHSETk6rKLa7Avq1gn2FDNmLjbIGCout6E365WGz1nSI/O1n9hCdzQMjtiaCnH1H1T5eqQc5O89LNu3ToolS17bz788MMICgrCgQMHMGXKFDz88MNWHyARkaszNWPSLFoqXg5nl1r1dQ3NgbvJgDHRLTkh9sxXGfO/JRxDTN233FLukOwKJAcqbm5ucHP7YyLmb3/7G/72t79ZdVBERB2JqeqciGA/XG9sv33UBoQF4IlbYnDH2kM2fy13GRDgI0e5RvWSKhF4sIkZHXPuGzk/yYFKVFQUxo0bh/feew9eXl7q4yUlJYiPj0d2drZVB0hE5OqiQv2RGBOKAxeKtfJJVAmvQggcv1TRbuPJLr6Gc78bXxayloTollmTstoGyU3bVPfNUDUUZ1Ncg+TyZDc3N0RHR6Nz587Ytm0bunXrBgD4/fffERYWxt2TiYgkqqhtwCObMwwu7fTr2slkPom1+Xu5o6betj/PzSk7NqWytlFn40FW/Tg+KZ/fkpNpZTIZvvvuO/To0QMjRoxAerrhVsemREREQCaT6XwtWLDA4msSETmblNRMpOWUGXy8vYMUAKipb4aP3A22LPzpoTDdkM6UQF85Ns2Nx+4nx2PDnDjsfnI8Ns2NZ5DiQiQHKkII+Pv7Y+vWrZg5cybGjRuHzZs3W/Ti6enpuHLlivrrhx9+AADceeedFl2PiMjZGKpccQR1jUqbJtLmllqvKicyxA9JfbtwuccFSc5Rkcn+iK9XrFiBgQMHYt68eUhOTpb84qGh2lN+r776Knr37o1x48ZJvhYRkTMyVbniypjsSuaQHKi0Tmm577770Lt37zb3UGloaMDmzZvx+OOPawVDmurr61FfX6/+vqqqqk2vSURkb+bsx+OKjPVHIdIkeelHqVSiS5cuWsdGjRqFEydO4Oeff7Z4IF9//TUqKiowe/Zsg+esWLECgYGB6q+ePXta/HpERO2h9UaBrakqV9wN/ILmqqquN2ptqEhkiOSqH1uZNGkSPD091V1v9dE3o9KzZ09W/RBRuzG0+V3r4+buQZNdXIMzV6rw4aFcvfvuAMCIcAUuFNWgos51Ptjd0NLQbdPceHsPhezAppsSAsAXX3yBzz77DPn5+WhoaNB6LCMjQ/L18vLy8OOPP2Lr1q1Gz/Py8tLq3UJE1F4MBR4vTxuI578+rXO8SanEL9nalTyqPWg2zY3Xe724cAVmj45AJx8PFFZchwAwMioYS7edRtV11wlSAEAJqNvccwmIjJEcqKxatQrPPfccZs2ahW3btmHOnDm4ePEi0tPTLS4r3rBhA7p06YLJkydb9HwiIlsztPndHWsPoqpOe3O/A1nFUOq5huYeNEu3nda53tG8cpy9WqXVvyQuQv8Ox7bWycsD1fXGNy00RNVwDYBOE7vW2OaeTJGco/LOO+9g3bp1WLNmDTw9PfH000/jhx9+QEpKCiorKyUPQKlUYsOGDZg1axY8PCya4CEisiljm9+V1zbqHNcXpGj6+6ajeq8nAJ0ma0ftEKQAQHV9EwK8PczaQdnP013r+4ToEKxOHorVyUMxPFxh9Lms/CFTJEcG+fn5GD16NADAx8cH1dUtjYjuv/9+jBw5EmvWrJF0vR9//BH5+fl44IEHpA6FiKhdWLuEOKuoxuxz7ZlEWHXdvBmVuoZmxIUrMH9CtE7uzucPj8ad7x7CsbxyrQCObe7JXJJnVLp27YrS0pY2z+Hh4Thy5AgAICcnR6d02Ry33HILhBDo06eP5OcSEbUHS0uIzZmNcAVKAOl55fi98rrexz+YFYcxrVrlq2ZdiEyRPKMyYcIE7NixA8OGDcPcuXPx2GOP4YsvvsDRo0cxY8YMW4yRiMiujG1+F+Djgaq6Jp3j8ZFBqK5vxKmCjtPv6dmtJwHoVjep2tznlFyTvPEgkeTyZKVSCaVSqc4n+eyzz3DgwAFER0fj4Ycfhqenp00Gqg83JSQiazJUegwY3vxu+bRYPPf1Kb1lyKXX6jFh5d52G7+1ubsBEDLJ7f1VyzosPSZDpHx+O0wfFUswUCEiazC35wkAg7MCrY9X1DZg3qajdqnYsSY/T3dca7BsF+XdT47nzAnpZdPdkyMiIrBs2TLk5+dbPEAiIkdiqPR47ofpOl1lDW1+1/p4SmomjuU5d5ACwOIgBbDupoPUcUkOVJ544gls27YNUVFRmDhxIrZs2aLVLZaIyJkYKz0+mleOORvSkfTmHsxcn2Z2y3fVNY31D+kIWHpM1iA5UFm0aBGOHTuGY8eOYcCAAUhJSUG3bt2wcOFCi7rSEhHZk7mlx6qusqa0LCOZPs9V+Hu56+xT5C6TcdNBshrJgYrKkCFD8P/+3/9DQUEBli5dig8++ABxcXEYMmQI/vOf/1hUqkxE1N7MLT3W7Cqrom/DwZTUTJwu7DiVPuvuH6HuQqvC0mOyJotbwTY2NuKrr77Chg0b8MMPP2DkyJGYO3cuCgsL8dxzz+HHH3/EJ598Ys2xEhFZnaHSY0NyS69B4SvXm3w7KipI65ijk6FtDeUSY0IxOjoEo6NDWHpMNiO56icjIwMbNmxAamoq3N3dcf/99+PBBx9Ev3791Oekp6cjMTERdXV1Vh+wJlb9EJE16Cs9NmT3k+Pxj60ncTi7tB1GZluJMaGoa2hCuomk3wBvD50utaN7B+Pde4frVEURmcOmuyfHxcVh4sSJePfddzFt2jTI5br/SAcMGIC7775b6qWJiOxCsyHZok8ycMrA0s3o3sEQQrhEkPLqjEG4O76XySBNVaZdVtuAI9mlkAG4KSqYsybUbiQHKtnZ2QgPDzd6jp+fHzZs2GDxoIiI7EEIYTBIAYCTlyuw55zzLO0Yc1NUMADtIE0ViHRX+KBJKbSWcQJ95QxOyC4kByqmghQiImdlqgKour4Zmw7nts9gDIgM9kVOads3SVy67bS6oV1FbQOWbjutt+Edkb2ZVfUTFBSEkpKWZkgKhQJBQUEGv4iInJU5FUC5VggS2sJar3/wQgke3NTS0G7epqN6G96ZU45NZGtmzai8/fbb6NSpEwDg3//+ty3HQ0RkN1Gh/rixRyAyL1caPa9LJ08UVTe006i0WavxQ7MQSM9taWhn6PF9WcXYn1WMsa12PiZqT9zrh4icirGNA63hnveP4NBF50+WtSZD+x4RWcqmVT8qRUVFKCoqglKp1Do+ePBgSy9JRGSQlI0DW8sursEvOaUAZBhppGIlu7iGQYoeqmUg7oZM9iA5UDl27BhmzZqFs2fP6nSflclkaG62fAMrIiJDDG0caOwDtKK2AfM/ztAJPkZFBeO9+4aj9Fo9fskpU5fctgQz1JpmV15W/lB7kxyozJkzB3369MH69etxww03QNZqjwciImtTbfLXmqkP0JTUTL0zJIezSzHmtZ9QXe+av1j5ebrjm5SxOJJdiiVbT1rturmlDFSo/UkOVHJycrB161ZER0fbYjxERDpMlQ23/gBVLfUY6zTrqkGKmwz4bnEiegb7Irf0mukn/I+7TIYB3TrhpJE+MtwNmexB8qaEN998M06cOGGLsRAR6WWqbPhq5XXklFxDRW0DZq5Pw4SVe7Fk66l2Gp1jUQpg6toDuFRaa/aGi0DLRoKbHxyJxJhQ7oZMDkVy1U9JSQlmzZqF+Ph4xMbG6rTQnzp1qlUHaAyrfog6jpnr00xuHKjwlaOqrhHNTlvLaD2dfeTIXHqL3vvmLpMhIToEL90xUGcjQX0t9Vn1Q9Ym5fNbcqCyfft23H///aiurta9WDsn0zJQIeo4pGwcSC0+mhuPwd07WxR4cDdksiWbBioRERG4/fbb8cILL+CGG25o00DbioEKUceg2TsFAI5kl3TYpR0p5oyOwNKpAwEw8CDHYtM+KqWlpXjsscfsHqQQkesz1DvlrhE97Dgq57H7XBEere2j3lCQAQo5I8nJtDNmzMDu3bttMRYiIi2GeqdsOJRrnwE5kLBAL7w0ZYDRc/LLarlfDzk9yTMqffr0wZIlS3DgwAEMGjRIJ5k2JSXFaoMjoo7LWO+Uo3nliItQID233A4jcwyFlfVYuuMMArw9UHW9Se85SgE2aiOnJzlQ+eCDD+Dv74+9e/di7969Wo/JZDIGKkRkFaZ6p0R38evQgYpKzfUm+Hu5o8ZIXxg2aiNnZlHDNyIiWzPVAyQ17XI7jcSxKQGjQQrARm3k3CTnqJgrICAA2dnZtro8ERFp8PdyZ6M2ckkW755sisSqZyIiAH+UIl+tvG7voTiVmvpmnbydhOgQrE4easdREbWdzQIVIiIp9JUikzTzk6IREezHfinkUhioEJFD0FeKTNKoghMGKORKGKgQkd0ZKkXuaGQALFk0V+3dwwCFXJHNkmllrZK6iIgMMVWKPDoquJ1GYh9uAOIjFOhs4aZ/zEUhV8ZkWiKyO4WP8Q/oQ9ml7TQS+/CSu0EIoKpOu3GbG4BeQT7ILasz+NxXZwzC3fG9bDxCIvtpU6CiCkb0zZ58++236N69e1suT0QuTlXh887uC/Yeil3VNSqRnqfbvE4JGA1SAOAmF59tIrJo6Wf9+vWIjY2Ft7c3vL29ERsbiw8++EDrnDFjxsDLy8sqgyQi11JR24CZ69MwYeVezNmQzg6zJsR2D2CPFOqwJAcqL7zwAhYvXowpU6bg888/x+eff44pU6bgsccew/PPP2+LMRKRi2GFjzSvTB+EhOgQrWPMS6GOQiYkJpOEhIRg9erVSE5O1jqempqKRYsWoaSk/X74VFVVITAwEJWVlQgICGi31yUiy2UX12DCyr2mTyR1Nc+mufEAgJySa+yRQi5Byue35ByV5uZmjBgxQuf48OHD0dSkfwdPInJ9qnwTzQ9Rfcd+yXHtxFhraj1rwh4p1BFJDlTuu+8+vPvuu3jrrbe0jq9btw733nuv1QZGRM5BX0fZUVHBkMmAQxf/CEpG9w6GEMBhF6/gsZS7DEiIDsVLdwzkrAmRBrMClccff1z9Z5lMhg8++AC7du3CyJEjAQBHjhzBpUuXMHPmTNuMkogclr58E33BiGbQoo8bgN5d/DB3TBQ+/iUPJwuqrDlMh9L/Bn94e3ngeH6F+lhCdChWJw9FoK+cAQqRBrNyVJKSksy7mEyGn3/+uc2DMhdzVIjsi/kmpnXp5IWi6nr193ERCnwwMw6BvnLmnFCHZfUcld27d1tlYETkWkx1lCVoBSkAkJ5bjkc+PoZP5o1kzgmRGdrUQv/y5csoKCiw1liIyMmEB/naewhO6dDFUuSUXLP3MIicguRARalUYtmyZQgMDER4eDh69eqFzp0741//+heUSqUtxkhEDioq1B+JMaE6zcjItF+YVExkFslVP8899xzWr1+PV199FQkJCRBC4ODBg3jxxRdx/fp1LF++3BbjJCIHtTp5KOZ+mI6jelrAk2HcDY3IPJIDlQ8//BAffPABpk6dqj42ZMgQdO/eHfPnz2egQtTBCAhcb2y29zDsyg0t+/JIMZJ79BCZRfLST1lZGfr166dzvF+/figrK7PKoIjIeaSkZuJMoeuWEpvSO9QPY2JCJT1nVFQwhBDYfa6IuSpEJkieURkyZAjWrFmDVatWaR1fs2YNhgwZYrWBEZFjyy6uwS85pVqN3jqipyb1xa2x3bDvfBF+/q0IGw/lGT2/X7dOuN7UrFXWnRjzRw8VItImea+fvXv3YvLkyejVqxdGjRoFmUyGQ4cO4dKlS9i5cyfGjh1rq7HqYB8VovanrxNtR6fwlaO8ttHg4zIAfl7uqKnXv0TWek8fIlcn5fNb8tLPuHHjcP78eUyfPh0VFRUoKyvDjBkzcO7cuXYNUojIPrjzsS5jQQoAdPaVo67BcB5PsxDYl1XMZSAiPSTPqDgSzqgQtZ+WpZ4yLNl60t5DcRorZgxC984+mPmfNLPOjw0LwMcPjuQSELk8m+6eDAAVFRVIS0tDUVGRTu8U7vdD5Fq41GO5roHeaJbwu+CZwiosSj3OJSAiDZIDlR07duDee+/FtWvX0KlTJ8g0Gj3JZDLJgUpBQQGeeeYZfPvtt6irq0OfPn2wfv16DB8+XOrQiMgGuNRjuYhgP0iZtFYC6iUgttYnaiE5R+WJJ57AAw88gOrqalRUVKC8vFz9JbU8uby8HAkJCZDL5fj2229x5swZrFy5Ep07d5Y6LCKysuziGqSm5WNfVrGkWQFqSY5NjAlFZIifRd17c0uZq0KkInlGpaCgACkpKfD1bfseH6+99hp69uyJDRs2qI9FRES0+bpEJJ0qB6WuoQnfnryKdHaaNZuHmwxNyj+CuYToEKxOHqr+fnXyUCxKPW728llEMGdTiFQkByqTJk3C0aNHERUV1eYX3759OyZNmoQ777wTe/fuVXe3nTdvnt7z6+vrUV//x06kVVUdt8kUkbVU1Dbgkc0ZOMy9ZywmBBAXocD8pGhEBOvuiBzoK8emufHIKbmG3NJriAj2w9Jtp3HwQonWbJWqTJnLPkR/MKvqZ/v27eo/FxcXY9myZZgzZw4GDRoEuVw7O12ztb4p3t7eAIDHH38cd955J9LS0vDoo4/i//7v//Tmurz44ot46aWXdI6z6oeoZUYkr6xW7welMTPXpzFR1kp2Pzne7HtfWduoM8vCxm/UUUip+jErUHFzMy+VRSaTobnZ/D0/PD09MWLECBw6dEh9LCUlBenp6Th8+LDO+fpmVHr27MlAhTo0fVU55n7gZRfXaHVIpbbZMCcOSX27SHqO5iwLZ1Koo7B6wzelUmnWl5QgBQC6deuGAQMGaB3r378/8vPz9Z7v5eWFgIAArS+ijk5fVc7BCyVYlHrc6PMqahuw4OMMWw6tw7EktyQyxA9JfbswSCEyQHLVjzUlJCTg3LlzWsfOnz+P8PBwO42IyLlkF9forcoxp9NpSmomzl6ttvUQXYabrGWmSl8Fj2aVDxFZl0WByk8//YTbb78dvXv3RnR0NG6//Xb8+OOPkq/z2GOP4ciRI3jllVdw4cIFfPLJJ1i3bh0WLFhgybCIOpy8slqjjxsqc1UFOGS+4eEKrE4eitXJQ5EQHaL1WOsqHyKyHslVP2vWrMFjjz2Gv/71r1i8eDEA4MiRI7jtttvw1ltvYeHChWZfKy4uDl999RWWLFmCZcuWITIyEv/+979x7733Sh0WUYcUHmS8TYChpQhTAQ7pmp8Urc75aV3Bw5kUItuRvNdP9+7dsWTJEp2AZO3atVi+fDkKCwutOkBjuNcPUUvVjqEy101z49X9UWQAbooKRmSIH05cKscdaw8ZvijpkFLRQ0TG2XSvn6qqKtx66606x2+55RY888wzUi9HRG2kr5lYQnQIXp42EMnrjuj0RxndOxh1DU3tPUynxd4mRPYlOVCZOnUqvvrqKzz11FNax7dt24YpU6ZYbWBEZJxm35RNc+Ox73wRjl+qwLBeCnTv7IP5H2fgVKFuU8RDFztmY7dAHw/IIENFXaOk5zH/hMi+JAcq/fv3x/Lly7Fnzx6MGjUKQEuOysGDB/HEE09g1apV6nNTUlKsN1IiAqC/b4rCV47yWmkfwB2JG4DUB0eiu8IXD25KR3quedsDfDQ3HmNjQm07OCIySnKOSmRkpHkXlsmQnZ1t0aDMxRwVcnX6us3qy0kh0xS+chz/5y0A/miydrXyOpZsPWnwOZY0cCMi02yao5KTk2PxwIjIPIa6zT5xSwzLii1UXtuI/VnFGBsTCtXvZ907+xh9DjcHJLI/yYEKEdmeoW6zZbX1Bp5B5jh0sQTv78vRWTarqmtEs8YEFRNoiRyH5EClubkZGzduxE8//YSioiIolUqtx3/++WerDY6oIzLUjK1ZCJwq4I7hbXEgqwRnCrW78VbWNiKwVY4PE2iJHIfkQGXx4sXYuHEjJk+ejNjYWMhatZImIstlF9dgx6/GexHFhgXg7JVq5qhIFODtgZN6Aj0lWpaFPpobjyalYAM3IgcjOVDZsmULPvvsM9x22222GA+Ry9OXIKsvJ8WQhUnR+OBADo7mmVe5Qi3LO//4c3889eWvBs9pUgomzhI5IMmBiqenJ6Kjo20xFiKXoxmUKHzlehNkVycP1ZuTYkijELjeJG2n8o4mMSYU99zUE+d+r8awXgqMjQlFdnGN0ecwcZbIMUkuT165ciWys7OxZs0auy/7sDyZHJWhXif6kjaH9uosaXYkLkKBY3nlUHLlR68Abw/8+uIkvY+Z2m6AiNqHlM9vswKVGTNmaH3/888/IygoCAMHDoRcLtd6bOvWrRYM2TIMVMhR2aLXiSVBTUdlqFFbZW2jznYDqlkt1YaDRGR7Vu+jEhgYqPX99OnTLR8dkYszVLXTVsN6dcafY7syUDFDRn653kAl0FfOnY+JnIxZgcqGDRtsPQ4il5FXVmuT66bnlSOdQYpZhvVSGH08MoQBCpGzcLP3AIhcDf9T2ZeHm4z78xC5EHamJbISUyXGbgB8vdxRU8+KHVtqUgrklFzjjAmRi+Avf0RWkpKaiQNGclPGxITi4wdHtuOIOq4zBZX2HgIRWQkDFSIrUCXQKg08Pqh7ADbNjceQnp2RGBPK/3gmxEUYzzExZeOhXOsMhIjsjj8viazAVALtyYIq7M8qRkVtA5qUSoMBDQH+Xu6YPSoC7903DLHdLWs7kJ5XjpySa1YeGRHZg1VzVDZt2oSEhAT07t3bmpclcnjhQb4mz7l/fRoUvnJUamx+R7pq6puxIPU4gJYeJ9sXJqD0WgOC/Tzx5vfnzS79zi1lngqRK7BqoDJ79mzI5XL8/e9/x+rVq615aSKHFhXqjxHhCpM9TsoZpEii2lZA1TVWsweKuwyY+Z90g89lS3wi12DVpR+lUolz584hNjbWmpclcgrrZ8VBwe6mVtUsBPZlFWst40SG+CGpbxck9umCxJhQuLfaysNdJkNiTChnU4hchNVzVCIiIvDQQw9Z+7JEDi/QV449TyZhQLdO9h6Ky8kt1Z9vsjp5KBKiQ7SOJUSHYHXy0PYYFhG1A4uWfpRKJS5cuICioiIoldppgYmJiVYZGJEzCvSVw83Om3W6IkPLOGyJT+T6JAcqR44cwT333IO8vDy03s9QJpOhuZnNrKjjyi6uwanCKnsPw6XEdg8wGXywJT6R65IcqDz88MMYMWIE/vvf/6Jbt26Q8bdHIgD/60y75bjZ58d08UdhRR2uNTC4N+bO4T3sPQQisiPJgUpWVha++OILREdH22I8RE4rJTUTZyTMpmQV1dhwNK6joo6VUkQdmeRk2ptuugkXLlywxViInJa6M60wfS5JY2onZCJybZJnVBYtWoQnnngCV69exaBBgyCXa5djDh482GqDI3IWpjrTkmUUvnLuhEzUwUkOVP7yl78AAB544AH1MZlMBiEEk2nJpWQX1yCvrNasShLuRdF2Hm4yNGlMSSl85di+YIwdR0REjkByoJKTk2OLcRA5jIraBqSkZmq1ak+MCcXq5KEIbNXQTd+5JN2oqGC8d99w/FpQgYz8cgzrpeBMChEBsCBQCQ8Pt8U4iNqFObMkKamZ6tbtKgcvlGBR6nF1K3fVtVJSj+M0y5HbxE0GyN3dEPi/ZR4GKESkyaxAZfv27fjzn/8MuVyO7du3Gz136tSpVhkYkTWZO0uiSoptTbOVu8JXjnmbjiI91/i+PmQepYD63rIXChG1ZlagMm3aNFy9ehVdunTBtGnTDJ7HHBVyVObOkphKin1k8zFcrbzOklkb4G7HRKSPWYGKZpv81i3ziRydObMkqg/I8CBfo9f67Wq1TcZI3O2YiPRrU7HC9evXrTUOIpsxNUuy/USBenfeqFB/JMaEwo0Nl60iwNsD7ibuJXc7JiJjJAcqzc3N+Ne//oXu3bvD398f2dnZAIAXXngB69evt/oAidrK1CzJ2z9kIenNPRi94ie8+d1vePKWPhgQFtBOo3NtNdebEOCjXSmlaFU5xd2OicgYyVU/y5cvx4cffojXX38d8+bNUx8fNGgQ3n77bcydO9eqAyRqL4WV17Fmz0Ws2XMR/p7sjGINSgDltY34aG48mpRCXW3F3Y6JyFySfxpv2rQJ69atw7333gt3d3f18cGDB+O3336z6uCIrMGSrrE1DczFsqYmpUBS3y7qoCQyxE/reyIiQyQHKgUFBXo3JFQqlWhsZCUEOR5TSz9knu0LEpAYEwr3Vjumu8sAP093A89qwURZIrKU5EBl4MCB2L9/v87xzz//HEOHcp2ZHI8qQbb1ByyZb0C3AAzu2Rmrk4ciITpE67EAHznqGvS3JWCiLBG1leQclaVLl+L+++9HQUEBlEoltm7dinPnzmHTpk345ptvbDFGojZbnTwUi1KPs9W9hc5cqcLM9WlYnTwUm+bGq3NM3GXAzP+kG3zesPDOTJQlojaRPKMyZcoUfPrpp9i5cydkMhn++c9/4uzZs9ixYwcmTpxoizEStVmgrxyb5sZj95Pj8djEGHsPxyntyyrGotTjAP7IMWkWxp8zPylaZ38kIiIpJM+oAMCkSZMwadIka4+FyOYiQ/wwZXAY3v4hy95DcUr7sopx6EIJRv9v+cdU/g9zU4iorSyuwWxoaMDly5eRn5+v9UXk6ArKpVcB0R/+/tFR9Z8N5f8wN4WIrEVyoJKVlYWxY8fCx8cH4eHhiIyMRGRkJCIiIhAZGWmLMRIZlV1cg93nitTdZU155OMMG4/ItdXUN2O/Rq6PvgRbNnEjImuRvPQze/ZseHh44JtvvkG3bt0gYyUF2Ym5OyJr2nuuCDX13DizrTLyyzE2JhTAH/k/bOJGRLYgOVDJzMzEsWPH0K9fP1uMh8hs5uyInF1cg7yyWvWHZ+blCjuM1PUM66XQORYZ8keA0vq+ExFZSnKgMmDAAJSUlJg+kTo0W39QmdoR+cSlcqzclaV1TmxYAP46vIfVx9LRKHzl6tmU1iyZ5SIiMkYmhDBRYAhUVVWp/3z06FE8//zzeOWVVzBo0CDI5do/fAIC2m8zt6qqKgQGBqKysrJdX5cMa68Pqt3nijBng+H+HWGB3rhadR1KPf+6PdxkaNL3AJmk8JVj+4Ix6Bmsv9pn5vo0HLxQgmaNHyvuMhkSokPUs1xERFI+v82aUencubNWLooQAjfffLPWOUIIyGQyNDdz/b8jM2c5xhKtZ2hMlcUWVl43+BiDFOlmDO2O6cO6G5xJAUzPcuWUXOMyEBFJZlagsnv3bluPg1yALT6ojM3QJMaE6vz2TrYx5cYwo0EKYHrzx9xSBipEJJ1Zgcq4ceNsPQ5yAbb4oDI2Q8O2+O1H1bjNWO4Rm78RkS1Y1Jm2vLwc69evx9mzZyGTydC/f3/MmTMHQUFB1h4fORFrf1CZmqEpq23Ai1MHYMLKvZKuS+ZT5ZcofOWYuT7NaO6RqvmboRwVzqYQkSUkN3zbu3cvIiIisGrVKpSXl6OsrAyrVq1CZGQk9u6V9oHx4osvQiaTaX117dpV6pDIQVi7S6k5MzSmziHzjYoKxujewVrHVI3bjM1saWLzNyKyNskzKgsWLMBdd92Fd999F+7u7gCA5uZmzJ8/HwsWLMCpU6ckXW/gwIH48ccf1d+rrknOSd9yjKUfVObM0JhRtEZGfDQ3Hk1KobWU07pxm5TcIzZ/IyJrkxyoXLx4EV9++aVWQOHu7o7HH38cmzZtkj4ADw/OorgQa35QGVpKcJMBA8Jaytnc3WQsN7aAajlGX4KsZuM2wLLco9bXICKylOSln2HDhuHs2bM6x8+ePYsbb7xR8gCysrIQFhaGyMhI3H333cjOzjZ4bn19PaqqqrS+yDFFhvghqW+XNn9Y6VtKUArgVEEVkt7cg5tX7mWQYgEps1xMkiUie5I8o5KSkoLFixfjwoULGDlyJADgyJEjWLt2LV599VX8+uuv6nMHDx5s9Fo33XQTNm3ahD59+uD333/Hyy+/jNGjR+P06dMIDg7WOX/FihV46aWXpA6ZnJjmDM2iTzJwprAKSo3HGaSYZ3TvYDx7az+U1jZInuVikiwR2ZNZnWk1ubkZn4SRyWQWN3+7du0aevfujaeffhqPP/64zuP19fWor69Xf19VVYWePXuyM20HkF1cw+oeC300N95kDxRTKmsbdXKP2BqfiCxl9c60mnJyciwemCl+fn4YNGgQsrKy9D7u5eUFLy8vm70+OS5W91ju/X05GNy9c5sCCibJEpG9SA5UwsPDbTEOAC0zJmfPnsXYsWNt9hrknEzlSZBh1tjCQIVJskTU3ixq+AYAZ86cQX5+PhoaGrSOT5061exrPPnkk5gyZQp69eqFoqIivPzyy6iqqsKsWbMsHRY5GGvtonyprBbhQT7IL6sDs1Kk4V47ROTMJAcq2dnZmD59Ok6ePKnORwGg3rRQSl7K5cuXkZycjJKSEoSGhmLkyJE4cuSITWdtqH1YaxflvNJrmLb2IMprG20xzA6Fe+0QkTOSXJ68ePFiREZG4vfff4evry9Onz6Nffv2YcSIEdizZ4+ka23ZsgWFhYVoaGhAQUEBvvzySwwYMEDqkMgBzf84Q6dJ2L6sYjzy8TEALTMtu88VIafkmtHrMEixHpYRE5EzkjyjcvjwYfz8888IDQ2Fm5sb3NzcMGbMGKxYsQIpKSk4fvy46YuQS8sursGhi6V6Hzt0sRR3vnsI6Xnl6mMjwhWYMzoCA7oHqjuh5pXVoqjqOoMUK2AZMRE5M8mBSnNzM/z9/QEAISEhKCwsRN++fREeHo5z585ZfYDkfH7J0R+kqBzVCFJU36uOKXzlDE4MkKElqHv9ziHILb2G4qp6PP3lryafx712iMiZSQ5UYmNj8euvvyIqKgo33XQTXn/9dXh6emLdunWIioqyxRjJ6ciMPmosGZZBimECUM9EJfXtAgD4/NglpOeWG3yONXqoEBHZk+Qcleeffx5KZUtv0Jdffhl5eXkYO3Ysdu7ciVWrVll9gOR8booMsvcQXFpu6R95PbNGRxg9l517icjZSZ5RmTRpkvrPUVFROHPmDMrKyqBQKNSVP9SxRYX6Y1RUMA5nG18CIstoJsUO6Ga8oyMTaInI2UmaUWlqaoKHhwdOnTqldTwoKIhBCml5777hSOSSg1W5y2RIjAnVSopV7cPj3ur/n75ziYickaRAxcPDA+Hh4ZL38KGOR9VyPS5CATfGsFZhKClW3w7TTKAlIlcheenn+eefx5IlS7B582YEBTEXgQzLLq4xmuhJ5vnn7QOQ1K+LwdkR7sNDRK5McqCyatUqXLhwAWFhYQgPD4efn/YPxIyMDKsNjpwbNxJsu7hwBR4YE2nWudyHh4hckeRAZdq0aTYYBrmC1vv6mLuRYE+FNy6VX7fx6JyPwleOD2bF2XsYRER2JTlQWbp0qS3GQU7M2L4+iTGhOJBVDKWR5zNI0e/DB+K19kWy1gaPRETOxOLdk6urq9UbEgKAm5ubumMtdSwpqZk4eKFE69jBCyVYlHocq5OH4q/vHUJWUY2dRue8Sq+17ExurQ0eiYickdlVP5mZmZg8ebL6+7CwMCgUCvVX586dkZ6ebpNBkv1pbiK491wR/t9P57E/qxjZxTXYl1WMZqHdWKxZCOzLKmaQ0gaqHijGAkEiIldn9ozK6tWrMWbMGK1jH330Ebp37w4hBP7zn/9g1apV+Oijj6w+SLIffb/Na/L3cjf6fAYp0mluIqgKBFtTBYI5Jde4DERELs3sQOXgwYOYPXu21rGRI0eq9/fx8fHB3/72N6sOjuxP32/zmmrq2VPHHH5e7rhm5r3S7IFiqnIqt5SBChG5NrMDlUuXLqFXr17q75ctW4aQkD+aTHXr1g2///67dUdHdmXot3l9ZDC+2WBHFxXih5MFVQYf/2huPJqUQidR1lTlFFvkE5GrMztHxcvLC5cvX1Z//9hjjyEg4I99Ri5dugRfX/PKUck5SOmD0ivIx4YjcX5nCquh8JXr/IdTtbofGxOKpL66Td3YIp+IOjqzA5WhQ4fi66+/Nvj41q1bMXQoW3a7EnP7oADAgqRozB4dbsPROLdmIVBe24jh4Qqt4+a0umeLfCLqyMxe+pk/fz7uvvtuRERE4JFHHoGbW0uM09zcjHfeeQerV6/GJ598YrOBUvtT/TZ/8EKJTlVPa09/ebKdRuXc5k+IRkSwn6RW92yRT0QdmUwIE59AGp555hm88cYb6NSpE6KioiCTyXDx4kXU1NTg8ccfxxtvvGHLseqoqqpCYGAgKisrtZahqO1UzcUaGpux4JPjaFIyA0VTfEQQGpqakXm5Un0sNiwAC5Ki8cjHhreR2P3keAYZRNThSfn8ltTw7bXXXsP06dORmpqKrKwsAMDYsWORnJyMkSNHWj5ispvW3U5NlSN3ZF0DvPD3xN4YHt4ZK3dlIS23TP1YXIQCH8yMQ6CvXO8slGbJMRERmU/SjIqj4YyK5Qx1O21sViItp8zkUk9HpJoNmbk+zWAgsmluPE5cqsBzX53EqcI/qnzYSZaI6A9Wn1HJz8/XKk02paCgAN27dzf7fGp/+vqjHLhQDK7w6BcbFmBWA7Y73zuE9NzyP57XPQCvTB+EwT06t+NoiYhch1lVP3FxcZg3bx7S0tIMnlNZWYn3338fsbGx2Lp1q9UGSNZnqO09gxTDXpk+CIDpku1jeeVa358trMab35+32biIiFydWTMqZ8+exSuvvIJbb70VcrkcI0aMQFhYGLy9vVFeXo4zZ87g9OnTGDFiBN544w38+c9/tvW4qQ2k9EehFrM2pGH7gjEmS7ZbB3tsdU9E1DZmzagEBQXhzTffRGFhId5991306dMHJSUl6oTae++9F8eOHcPBgwcZpDiBIOZJSFZe24ipaw8YbMBm6j9Sbuk12w2OiMiFSar68fb2xowZMzBjxgxbjYdsLLu4Bs99fcrg4z0VPrhUXteOI3Ie5bWN2J9VjNXJQ7Eo9bhWrsqwcAWOtlr20cRW90RElpEUqJDzMrfsmEGKcRn55RgbE6q3AZuxaiAu+xARWcbsFvrk3EztgkzmGdbrjxb4kSF+WvvzsNU9EZH1cUalA5CyCzIZpvCVY2xMqMHH2eqeiMj6OKPSAbDKxzxLbx+AuFabBqoofOXYvmCMWddpPdNCRESW44yKi2ndEh+QtgtyR5UYE4o5YyIxZ0ykekakpLoehZV1GNZLYXQmhYiIbIeBiosw1BJ/dfJQg7sguwFQ2mGsjmZ072CtPJLIEC7ZEBE5Cu7144T0zZqY2n+msrZRp6R2hImSWkc0KCwAZ65UW7QX0aszBuGmqGAAwJHsUsgA3BQVzKCEiKid2Wz3ZLIvQ7MmT9wSY3T/GVVX1NaJnrml1zBnQ3p7voU2O1lYBYWvHOW1jepjpgIuNwBjYkJxd/wf+1UxOCEicg4MVJyIvhLjgxdKUFxz3ejzckv/aN+uuazhrJNpVXVNiItQYH5StNEeJipj/rcERkREzodVP07C0EaCzULg7JVqo8/V7IqaXVyD3eeKkFNyTZ274myahUB6brnW0pe+Hiax3QOwfWECNs2NRyC3DSAickqcUXESlpYYx4UrEBniZ3DZaOqQrk7bY0Vzpog9TIiIXBMDFSdhaYnx7NERAPQvG+3LKnbaIAXQv3+OvoodfcnHRETkHBioOAmDJcYyQGkk1WRA98AO25nWWMl26bV6Bi9ERE6AgYoDaz0ToG/X3jHRoWhsVuKX7FKdnigKXzmCfD2Rccm5SpDNteNEAQRgsCGbvlmkA1nFGP/mbq2qIVXwwjwWIiLHw0DFARmaCXh52kC957/+l8GYuvaA1ocvAFTVtfROmTyoq83G6uXhhvom67SN8/QAGprMP/+tH7LUf1a1uO8Z3LJEZmgWSQno3KeDF0qwKPU4Ns2Nt2jcRERkO6z6cUCGypDvWHtQ7/HHPsvU+fAFgGbRkofy/37O0nnMWkwFKTIzr+Mjd8NPjyUhLkIBN3OfpKG8thFT1x5Qfy8l+Viz3wwRETkWBip2olkm3Pq4oTLk8tpGvcdNdZe9UmG8z4otuMlaKo46eZs3aVffqMRzX5/CBzPjMCbaspLp8tpG7P/fLIolyce5pQxUiIgcDZd+2pmxBM9AX7lNdjq2R1u3nkG+qG9Souq6eWs5SrTM/pTVNmDT3Hi8uP0UNh7Kk/y6GfnlGBsTajD52Bh9VURERGRfnFFpZ4aWdRalHgfgOjsd55XW4teCSsnPO/O/5yT17WLR6w7rpVD/WV8TOIWvHO6tlpbcZTIkxoSy+oeIyAFxRqUdGUrw1MyRMDQT4C6TIcDHA1V1TS69A/LGQ7mYPCQM4/p20dnTxxSFr1yr+kdfE7ggX0+dyqmE6BC22CciclCcUWlHppZ1VDkS+mYCEqJDsH3BGJ3jA8Jca9fo9Lxydd7O9gVjoGhVMqzwlePTeSP1Ht++YIzea0aG+CGpbxdEhvipg5fdT47Hhjlx2P3keLbYJyJyYDLhrDvTQdo20Y4gu7gGE1buNfj47ifHay0/GGoHr3lcCGH0ms4oNiwAHz84Uh087M8qRkZ+uU6/FEPHiYjIsUn5/Gag0s707fLrLpMhITpEUh8PzWZwz311Eocullp1nAHeHmYnwlqbG1p2PGZfEyIi1yTl85s5Ku1MX3dZKTkS+qqGAn2s89c4tGcgUv7UBxHBfjhTUIkF/0vwNcZUC39LqCqAckquMcGViKiDY6DSztq6y6++qqHKOsMzH+Ym2yp85dg45yb1cou5E21jokNRfb0Bxy9Jr/AxRXN3ZCIi6pgYqLQDfbv36tvl15zrSN1csGewD/JK60yeV17biLLaBnWgEhXqj7gIBY7mlaN1zOLuBrw6fTBuCPRCswByimtsEqiwrwkRETFQsSFTzd300RfUqFjSDC6pbxezG6epZjBU407P1d/xtlkJLPnqJJosWPNZMWMQugZ642rldSzZetLgebHdAzibQkREjlOevGLFCshkMjz66KP2HorVmGrupqmitgEz16dhwsq9mLMhHUlv7sHM9Wmo1OgjIqUZnKqJmZTGaaoZDH3jbs2SIAUARkYFI6lvF9wUGWT0vFemD7Lo+kRE5FocIlBJT0/HunXrMHjwYHsPxWqM7dmjbwM8c4IaVTM4d5l2a1U3QKeviCpBV9U4zZRBYS0zGHvPFekdd1u17v5q7L0kxoRicI/OVn19IiJyTnYPVGpqanDvvffi/fffh0KhMP0EJ2FuczdAWlCjrxncmJhQ7HkyyWATs+0LxsCrdd/4Vvrc4I+Z69Mwa0O6We9PKn2VTYbeC7vEEhGRit1zVBYsWIDJkyfjT3/6E15++WWj59bX16O+vl79fVVVla2HZzFTyzSaiaLmBDWqmQhjVUOBvnK9eR09g30R07UTThUYvl9fHi80OgZLqPJRDFU2tbUCioiIXJ9dA5UtW7YgIyMD6enm/Ra/YsUKvPTSSzYelXUY27MnITpE6wNZSlCjolk1lF1cg19yyiADENbZBwUVtQBkGBkVjMgQP2QX1xgNUqTycJNBKYTJ/imq1zfFkgooIiLqGOwWqFy6dAmLFy/Grl274O3tbdZzlixZgscff1z9fVVVFXr27GmrIbaZuc3dpAQ1mipqG/DI5gwczjbclXZUVDDuu6lXG98JMKh7AMbEhGB07xAM7t5Z531pMjVuIiIic9mthf7XX3+N6dOnw93dXX2subkZMpkMbm5uqK+v13pMH2dpoW/O0kZlbaPOh7+pUuaZ69PM6qsyIrylH4qlPpobr3cvnZySazhTUImNh3KRrnF9U+MmIqKOzSn2+qmurkZennZ/jzlz5qBfv3545plnEBsba/IazhKoSGEqqFH1WalraML8j023uFeJi1DgWF65pHb3UvYgYp4JERGZyyn2+unUqZNOMOLn54fg4GCzghRXZShfQ1/zOClmjY5ATX0Tzl6pNvs5UvYgYp4JERHZgt2rfpyFsY6x7cGcJmzGDAwLhL+X8b/uQd0DMH1oD/h4upudCEtERGRLDhWo7Nmzx95D0GFJG3xrs2SPH02JMaEQQhhsiQ8Afp7uOFlQhZMFZ9TPYZ4JERHZm90bvjk6KW3wbcWSPX5URkUFY3XyUJPXqGts1vq+vd8jERGRPgxUjJDaBt9WpOzxoykiyAfv3Tccgb5yk3/RrZNs2/s9EhER6cNAxQgpbfBtydC+OKbkltXhkY+PAQCUFr52e71HIiIifRioGGFJx1hryy6uwe5zRXjylj46++LcaMbGfYculiKn5JrlszLt8B6JiIgMcahkWkdjacdYlbZUChlK4t2+MAGnC6ugmlvJvFxh8lq/ZJfi7vheBt9LgI8HquqaLHqPREREtsRAxQRz2+BrskalkL4k3gNZxThZUIHy2kZJ70EVfhh6L8unxeK5r09Jeo9ERETtwW6daa2hPTvTSum8OnN9msFZGHO6vGYX12DCyr1tHrPK7ifHa43Z0Hthd1kiImoPTtGZ1tmY23nVUM8TzSoaU9dpSzlya6P0NG4z9F7YXZaIiBwNk2mtzBqVQgof6U3Wlt7eH7Fh2lFpYkwo3rtvuORrEREROQrOqBhgaSJsWyuFsotr8Py2U2a/nsr4fjdgzpgoLt8QEZFLYaDSitRE2NYBjaWVQuZuOugm027O1vq6XL4hIiJXwqWfVsxtmV9R24CZ69MwYeVezNmQjqQ392Dm+jRU1jZidfJQnZ4npqpozN10cECr5R1W5xARkSvjjIoGKYmwxgKaTXPjsWluvNnLMFI2HVydPAwAzL6uPXd8JiIiaisGKhrMSYSNDPHD3nNFZgU05i7DmFPlo2+JxxBH2PGZiIjIGrj0o8FUImyQrxwz16dh1oZ0o+dJ3R/HnPb2UpZ4HGHHZyIiImtgoKLB0OZ/bgDiwhVYuSvLrDwSqfvjGHvd2LAA7H5yPDbNjTdrNsRRdnwmIiKyBgYqrehLhFUCSM8r1xsAaHKXyZAYE2pRPoi+1x0TE4qPHxwp6XqOsuMzERGRNTBHpZVAXzk2zY3Hne8dwrG8cq1SYFPaUoGjet229kFxhB2fiYiIrIWBih7ZxTVIzy2X9JyP5sZjbExom1+7rX1Q2rrjMxERkSPh0o8eUvbaUS33WCNIsRZL+rgQERE5Is6o6GFOFY6KIwYA1lpGIiIisjcGKnqYWj556Y6BThEAsJ0+ERE5OwYqBjxxSwzKautxqqBKfUw1exLoK2cAQERE1A4YqLSir6trbFgAXpk+CIN7drbfwIiIiDogJtO2oq+r69kr1Xhz13k7jYiIiKjjYqCigV1diYiIHAsDFQ3s6kpERORYGKhoYFdXIiIix8JARYOhzQHbsocPERERWY6BSivs6kpEROQ4WJ7cCru6EhEROQ4GKgawqysREZH9cemHiIiIHBYDFSIiInJYDFSIiIjIYTFQISIiIofFQIWIiIgcFgMVIiIiclgMVIiIiMhhMVAhIiIih8VAhYiIiBwWAxUiIiJyWE7dQl8IAQCoqqqy80iIiIjIXKrPbdXnuDFOHahUV1cDAHr27GnnkRAREZFU1dXVCAwMNHqOTJgTzjgopVKJwsJCdOrUCTKZzN7DsYmqqir07NkTly5dQkBAgL2H4zB4XwzjvTGM90Y/3hfDeG8Ma8u9EUKguroaYWFhcHMznoXi1DMqbm5u6NGjh72H0S4CAgL4n0QP3hfDeG8M473Rj/fFMN4bwyy9N6ZmUlSYTEtEREQOi4EKEREROSwGKg7Oy8sLS5cuhZeXl72H4lB4XwzjvTGM90Y/3hfDeG8Ma69749TJtEREROTaOKNCREREDouBChERETksBipERETksBioEBERkcNioOKAcnNzMXfuXERGRsLHxwe9e/fG0qVL0dDQoHVefn4+pkyZAj8/P4SEhCAlJUXnHFf1zjvvIDIyEt7e3hg+fDj2799v7yG1qxUrViAuLg6dOnVCly5dMG3aNJw7d07rHCEEXnzxRYSFhcHHxwfjx4/H6dOn7TRi+1mxYgVkMhkeffRR9bGOfG8KCgpw3333ITg4GL6+vrjxxhtx7Ngx9eMd8d40NTXh+eefV//MjYqKwrJly6BUKtXndJT7sm/fPkyZMgVhYWGQyWT4+uuvtR435z7U19dj0aJFCAkJgZ+fH6ZOnYrLly9bPihBDufbb78Vs2fPFt9//724ePGi2LZtm+jSpYt44okn1Oc0NTWJ2NhYkZSUJDIyMsQPP/wgwsLCxMKFC+048vaxZcsWIZfLxfvvvy/OnDkjFi9eLPz8/EReXp69h9ZuJk2aJDZs2CBOnTolMjMzxeTJk0WvXr1ETU2N+pxXX31VdOrUSXz55Zfi5MmT4q677hLdunUTVVVVdhx5+0pLSxMRERFi8ODBYvHixerjHfXelJWVifDwcDF79mzxyy+/iJycHPHjjz+KCxcuqM/piPfm5ZdfFsHBweKbb74ROTk54vPPPxf+/v7i3//+t/qcjnJfdu7cKZ577jnx5ZdfCgDiq6++0nrcnPvw8MMPi+7du4sffvhBZGRkiKSkJDFkyBDR1NRk0ZgYqDiJ119/XURGRqq/37lzp3BzcxMFBQXqY6mpqcLLy0tUVlbaY4jtJj4+Xjz88MNax/r16yeeffZZO43I/oqKigQAsXfvXiGEEEqlUnTt2lW8+uqr6nOuX78uAgMDxXvvvWevYbar6upqERMTI3744Qcxbtw4daDSke/NM888I8aMGWPw8Y56byZPniweeOABrWMzZswQ9913nxCi496X1oGKOfehoqJCyOVysWXLFvU5BQUFws3NTXz33XcWjYNLP06isrISQUFB6u8PHz6M2NhYhIWFqY9NmjQJ9fX1WtO4rqahoQHHjh3DLbfconX8lltuwaFDh+w0KvurrKwEAPW/kZycHFy9elXrPnl5eWHcuHEd5j4tWLAAkydPxp/+9Cet4x353mzfvh0jRozAnXfeiS5dumDo0KF4//331Y931HszZswY/PTTTzh//jwA4MSJEzhw4ABuu+02AB33vrRmzn04duwYGhsbtc4JCwtDbGysxffKqTcl7CguXryI1atXY+XKlepjV69exQ033KB1nkKhgKenJ65evdreQ2w3JSUlaG5u1nnvN9xwg0u/b2OEEHj88ccxZswYxMbGAoD6Xui7T3l5ee0+xva2ZcsWZGRkID09XeexjnxvsrOz8e677+Lxxx/HP/7xD6SlpSElJQVeXl6YOXNmh703zzzzDCorK9GvXz+4u7ujubkZy5cvR3JyMoCO/W9Gkzn34erVq/D09IRCodA5x9Kf0ZxRaUcvvvgiZDKZ0a+jR49qPaewsBC33nor7rzzTjz44INaj8lkMp3XEELoPe5qWr/HjvK+9Vm4cCF+/fVXpKam6jzWEe/TpUuXsHjxYmzevBne3t4Gz+uI90apVGLYsGF45ZVXMHToUDz00EOYN28e3n33Xa3zOtq9+fTTT7F582Z88sknyMjIwIcffog333wTH374odZ5He2+GGLJfWjLveKMSjtauHAh7r77bqPnREREqP9cWFiIpKQkjBo1CuvWrdM6r2vXrvjll1+0jpWXl6OxsVEn2nUlISEhcHd314nMi4qKXPp9G7Jo0SJs374d+/btQ48ePdTHu3btCqDlt5tu3bqpj3eE+3Ts2DEUFRVh+PDh6mPNzc3Yt28f1qxZo66O6oj3plu3bhgwYIDWsf79++PLL78E0HH/3Tz11FN49tln1T+fBw0ahLy8PKxYsQKzZs3qsPelNXPuQ9euXdHQ0IDy8nKtWZWioiKMHj3aotfljEo7CgkJQb9+/Yx+qX4DLCgowPjx4zFs2DBs2LABbm7af1WjRo3CqVOncOXKFfWxXbt2wcvLS+sHtKvx9PTE8OHD8cMPP2gd/+GHHyz+T+CMhBBYuHAhtm7dip9//hmRkZFaj0dGRqJr165a96mhoQF79+51+ft088034+TJk8jMzFR/jRgxAvfeey8yMzMRFRXVYe9NQkKCThn7+fPnER4eDqDj/rupra3V+Rnr7u6uLk/uqPelNXPuw/DhwyGXy7XOuXLlCk6dOmX5vbIoBZdsqqCgQERHR4sJEyaIy5cviytXrqi/VFTlyTfffLPIyMgQP/74o+jRo0eHKk9ev369OHPmjHj00UeFn5+fyM3NtffQ2s0jjzwiAgMDxZ49e7T+fdTW1qrPefXVV0VgYKDYunWrOHnypEhOTnbJckpzaFb9CNFx701aWprw8PAQy5cvF1lZWeLjjz8Wvr6+YvPmzepzOuK9mTVrlujevbu6PHnr1q0iJCREPP300+pzOsp9qa6uFsePHxfHjx8XAMRbb70ljh8/rm7/YM59ePjhh0WPHj3Ejz/+KDIyMsSECRNYnuxqNmzYIADo/dKUl5cnJk+eLHx8fERQUJBYuHChuH79up1G3b7Wrl0rwsPDhaenpxg2bJi6LLejMPTvY8OGDepzlEqlWLp0qejatavw8vISiYmJ4uTJk/YbtB21DlQ68r3ZsWOHiI2NFV5eXqJfv35i3bp1Wo93xHtTVVUlFi9eLHr16iW8vb1FVFSUeO6550R9fb36nI5yX3bv3q33Z8usWbOEEObdh7q6OrFw4UIRFBQkfHx8xO233y7y8/MtHpNMCCEsm4shIiIisi3mqBAREZHDYqBCREREDouBChERETksBipERETksBioEBERkcNioEJEREQOi4EKEREROSwGKkREROSwGKhQh1FaWoouXbogNzfX3kMBAOTm5kImkyEzM9PeQ7Gq9nxfGzduROfOndt0DXPGu2fPHshkMlRUVJh1zfHjx+PRRx9t07icTVxcHLZu3WrvYZALYqBCHcaKFSswZcoU9Q7Vqg+o1l/33Xef3scVCgUSExOxd+9eO74LbTKZDF9//bXk582ePRvTpk3TOS71A7mjGD16NK5cuYLAwECzzt+6dSv+9a9/qb+PiIjAv//97zaPo6ioCA899BB69eoFLy8vdO3aFZMmTcLhw4fbfO22euGFF/Dss8+qN/IjshYGKtQh1NXVYf369XjwwQd1Hvvxxx9x5coV9dfatWv1Pr53714EBATgtttuQ05OTnsN3eUIIdDU1GTvYUji6emJrl27QiaTmXV+UFAQOnXqZPVx/OUvf8GJEyfw4Ycf4vz589i+fTvGjx+PsrIyq7+WSkNDg1nnTZ48GZWVlfj+++9tNhbqmBioUIfw7bffwsPDA6NGjdJ5LDg4GF27dlV/tf6tWfX44MGD8X//93+ora3Frl27AAB79+5FfHw8vLy80K1bNzz77LNaH8JKpRKvvfYaoqOj4eXlhV69emH58uV6x6hUKjFv3jz06dMHeXl5AIAdO3Zg+PDh8Pb2RlRUFF566SX19VUzQ9OnT4dMJlN/by3Xrl1DQEAAvvjiC63jO3bsgJ+fH6qrqwEAaWlpGDp0KLy9vTFixAgcP35c63zVLM3333+PESNGwMvLC/v370d9fT1SUlLQpUsXeHt7Y8yYMUhPT9d53n//+18MGTIE3t7euOmmm3Dy5EmdsX7//ffo378//P39ceutt+LKlSvqx5RKJZYtW4YePXrAy8sLN954I7777juda/z2228YPXo0vL29MXDgQOzZs0dnLJozTQcPHsS4cePg6+sLhUKBSZMmoby8HID20s/48eORl5eHxx57TD07Z+691VRRUYEDBw7gtddeQ1JSEsLDwxEfH48lS5Zg8uTJWuf9/e9/xw033ABvb2/Exsbim2++UT/+5ZdfYuDAgfDy8kJERARWrlyp9ToRERF4+eWXMXv2bAQGBmLevHkAgEOHDiExMRE+Pj7o2bMnUlJScO3aNfXz3N3dcdtttyE1NVVn7ERtYvF2hkROZPHixeLWW2/VOpaTkyMAiOPHj+t9jr7Hy8rKBACxevVqcfnyZeHr6yvmz58vzp49K7766isREhIili5dqj7/6aefFgqFQmzcuFFcuHBB7N+/X7z//vs616+vrxd/+ctfxI033ih+//13IYQQ3333nQgICBAbN24UFy9eFLt27RIRERHixRdfFEIIUVRUpN4x+cqVK6KoqMjs+zFr1ixxxx136BxX7ZxaXl4uhBBi3rx54rbbbtM6Z/r06WLmzJlCCCFqampEaGiouOuuu8SpU6fEjh07RFRUlNZ9U11z8ODBYteuXeLChQuipKREpKSkiLCwMLFz505x+vRpMWvWLKFQKERpaanW8/r37y927dolfv31V3H77beLiIgI0dDQIIRo2WlcLpeLP/3pTyI9PV0cO3ZM9O/fX9xzzz3q8b711lsiICBApKamit9++008/fTTQi6Xi/Pnz2v9PfTo0UN88cUX4syZM+LBBx8UnTp1EiUlJXrvy/Hjx4WXl5d45JFHRGZmpjh16pRYvXq1KC4uFkJo79ZcWloqevToIZYtWyauXLkirly5Yta9ba2xsVH4+/uLRx991OAu6c3NzWLkyJFi4MCBYteuXeLixYtix44dYufOnUIIIY4ePSrc3NzEsmXLxLlz58SGDRuEj4+P1q7b4eHhIiAgQLzxxhsiKytLZGVliV9//VX4+/uLt99+W5w/f14cPHhQDB06VMyePVvr9d955x0RERGhd2xElmKgQh3CHXfcIR544AGtY6oPKB8fH+Hn56f+ysjI0Hpc9YFbU1MjHnroIeHu7i5+/fVX8Y9//EP07dtXKJVK9TXXrl0r/P39RXNzs6iqqhJeXl7qwKQ11fX3798v/vSnP4mEhARRUVGhfnzs2LHilVde0XrORx99JLp166b+HoD46quvJN+PWbNmCXd3d6337efnJ7y9vbU+kH/55Rfh7u4uCgoKhBBCFBcXC7lcLvbs2SOEEOL//u//RFBQkLh27Zr62u+++67eQOXrr79Wn1NTUyPkcrn4+OOP1ccaGhpEWFiYeP3117Wet2XLFvU5paWlwsfHR3z66adCiJZABYC4cOGC+py1a9eKG264Qf19WFiYWL58udb7j4uLE/PnzxdC/PH38Oqrr6ofb2xsFD169BCvvfaa1lhU9yU5OVkkJCQYvL+agYoQLR/+b7/9ttY5pu6tPl988YVQKBTC29tbjB49WixZskScOHFC/fj3338v3NzcxLlz5/Q+/5577hETJ07UOvbUU0+JAQMGaI112rRpWufcf//94u9//7vWsf379ws3NzdRV1enPrZt2zbh5uYmmpubDb4HIqm49EMdQl1dHby9vfU+9umnnyIzM1P9NWDAAK3HR48eDX9/f3Tq1Ak7duzAxo0bMWjQIJw9exajRo3SyltISEhATU0NLl++jLNnz6K+vh4333yz0bElJyejpqYGu3bt0lp2OnbsGJYtWwZ/f3/117x583DlyhXU1ta24W60SEpK0nrfmZmZ+OCDD7TOiY+Px8CBA7Fp0yYAwEcffYRevXohMTERAHD27FkMGTIEvr6+6ufoW14DgBEjRqj/fPHiRTQ2NiIhIUF9TC6XIz4+HmfPntV6nub1goKC0LdvX61zfH190bt3b/X33bp1Q1FREQCgqqoKhYWFWq8DtPw9GXsdDw8PjBgxQucclczMTJN/r6aYurf6/OUvf0FhYSG2b9+OSZMmYc+ePRg2bBg2btyoHlePHj3Qp08fvc8/e/as3nuRlZWF5uZm9THNvyug5d/ixo0btf4tTpo0CUqlUitfy8fHB0qlEvX19ZLuBZExHvYeAFF7CAkJUecPtNazZ09ER0cbfO6nn36KAQMGoHPnzggODlYfF0LoJFcKIQC0VOP4+PiYNbbbbrsNmzdvxpEjRzBhwgT1caVSiZdeegkzZszQeY6hoEsKPz8/nfd9+fJlnfMefPBBrFmzBs8++yw2bNiAOXPmqN+36v2a+3oqmvdJk757qo/mOXK5XOex1uOyxutoMvfv1hRj99YQb29vTJw4ERMnTsQ///lPPPjgg1i6dClmz55tclzG/s1q0vy7Alr+LT700ENISUnRObdXr17qP5eVlcHX19dq94cIYDItdRBDhw7FmTNnLHpuz5490bt3b60gBQAGDBiAQ4cOaf2gP3ToEDp16oTu3bsjJiYGPj4++Omnn4xe/5FHHsGrr76KqVOnapU+Dxs2DOfOnUN0dLTOl5tby39duVyu9ZuwLdx3333Iz8/HqlWrcPr0acyaNUv92IABA3DixAnU1dWpjx05csTkNaOjo+Hp6YkDBw6ojzU2NuLo0aPo37+/1rma1ysvL8f58+fRr18/s8YeEBCAsLAwrdcBWv6ejL1OU1MTjh07ZvB1Bg8ebPLvVZOnp6fevydj99ZcAwYMUCe1Dh48GJcvX8b58+cNnqvvXvTp0wfu7u4GX2PYsGE4ffq03n+Lnp6e6vNOnTqFYcOGSX4PREbZa82JqD39+uuvwsPDQ5SVlamPWZJMq0mVTLtgwQJx9uxZ8fXXX+sk07744otCoVCIDz/8UFy4cEEcPnxYfPDBB3qv//bbbwt/f3+xf/9+IURLMq2Hh4dYunSpOHXqlDhz5ozYsmWLeO6559TXj4mJEY888oi4cuWK1nszxdxkWpV77rlHeHp66iQkV1dXi5CQEJGcnCxOnz4t/vvf/4ro6Gi9OSqtr7l48WIRFhYmvv32W61kWtX7UD1v4MCB4scffxQnT54UU6dOFb169RL19fVCiJYclcDAQK3rfvXVV0LzR9vbb78tAgICxJYtW8Rvv/0mnnnmGb3JtL169RJbt24VZ8+eFX//+9+Fv7+/Ojm29Xs4d+6c8PT0FI888og4ceKEOHv2rHjnnXf0JtMKIcTEiRPF1KlTxeXLl9XnmLq3rZWUlIikpCTx0UcfiRMnTojs7Gzx2WefiRtuuEEr/2r8+PEiNjZW7Nq1S2RnZ4udO3eKb7/9VgghxLFjx7SSaTdu3Kg3mbZ1Ps2JEyeEj4+PmD9/vjh+/Lg4f/682LZtm1i4cKHWeePGjRPLli0z+j6IpGKgQh3GyJEjxXvvvaf+vq2BihBC7NmzR8TFxQlPT0/RtWtX8cwzz4jGxkb1483NzeLll18W4eHhQi6Xi169eqkTZPVdf+XKlaJTp07i4MGDQoiWYGX06NHCx8dHBAQEiPj4eLFu3Tr1+du3bxfR0dHCw8NDhIeHa1139+7dBsctNVD56aefBADx2Wef6Tzn8OHDYsiQIcLT01PceOON4ssvvzQrUKmrqxOLFi0SISEhwsvLSyQkJIi0tDSdsezYsUMMHDhQeHp6iri4OJGZmak+x5xApbm5Wbz00kuie/fuQi6XiyFDhqg/uDXv1yeffCJuuukm4enpKfr37y9++ukno/dlz549YvTo0cLLy0t07txZTJo0Sf1460Dl8OHDYvDgwcLLy0u0/v3Q2L3VdP36dfHss8+KYcOGicDAQOHr6yv69u0rnn/+eVFbW6s+r7S0VMyZM0cEBwcLb29vERsbK7755hv141988YUYMGCA+t/jG2+8ofU6+gIVIYRIS0sTEydOFP7+/sLPz08MHjxYK0n58uXLQi6Xi0uXLhl9H0RSyYSQsMhM5MR27tyJJ598EqdOnVIvnbiiPXv2YPr06cjOzoZCobDKNT/++GMsXrwYhYWFWlP9trRnzx4kJSWhvLy8zW3yHZk97q0tPPXUU6isrMS6devsPRRyMUympQ7jtttuQ1ZWFgoKCtCzZ097D8dmvvvuO/zjH/+wSpBSW1uLnJwcrFixAg899JBTf5A6Gle7t126dMGTTz5p72GQC+KMChEZ9OKLL2L58uVITEzEtm3b4O/v326v7eozKva8t0TOhIEKEREROSzXXagnIiIip8dAhYiIiBwWAxUiIiJyWAxUiIiIyGExUCEiIiKHxUCFiIiIHBYDFSIiInJYDFSIiIjIYf1/1Wo5Ywn2OJ0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data[[(\"FPocket\", \"Hydrophobicity Score\"), (\"Graphein\", \"hphob_miyazawa\")]].plot((\"FPocket\", \"Hydrophobicity Score\"), (\"Graphein\", \"hphob_miyazawa\"), kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7197bb23-f322-4447-942a-909b8ada7232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3685474/2525020029.py:19: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = corr_func(series1, series2) # series1[valid], series2[valid]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>feature1</th>\n",
       "      <th>group2</th>\n",
       "      <th>feature2</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_miyazawa</td>\n",
       "      <td>0.964340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_3</td>\n",
       "      <td>0.961953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_manavalan</td>\n",
       "      <td>0.944778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>transmembranetendency</td>\n",
       "      <td>0.943616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_2</td>\n",
       "      <td>0.941875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_sweet</td>\n",
       "      <td>0.939917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_rose</td>\n",
       "      <td>0.935967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_mobility</td>\n",
       "      <td>0.932711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_fauchere</td>\n",
       "      <td>0.922489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_4</td>\n",
       "      <td>0.922489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_doolittle</td>\n",
       "      <td>0.910350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_ph7_5</td>\n",
       "      <td>0.908787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>totalbeta_strand</td>\n",
       "      <td>0.905873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_black</td>\n",
       "      <td>0.902452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_argos</td>\n",
       "      <td>0.897789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>refractivity</td>\n",
       "      <td>0.896427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>beta_sheetfasman</td>\n",
       "      <td>0.891402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_roseman</td>\n",
       "      <td>0.881857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Charge Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_5</td>\n",
       "      <td>0.879985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>beta_sheetroux</td>\n",
       "      <td>0.876520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>molecularweight</td>\n",
       "      <td>0.870896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_leo</td>\n",
       "      <td>0.870786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_chothia</td>\n",
       "      <td>0.870030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_wilson</td>\n",
       "      <td>0.868411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_ph3_4</td>\n",
       "      <td>0.861565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_eisenberg</td>\n",
       "      <td>0.855867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>antiparallelbeta_strand</td>\n",
       "      <td>0.851154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hplctfa</td>\n",
       "      <td>0.844937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_tanford</td>\n",
       "      <td>0.842770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hplc2_1</td>\n",
       "      <td>0.840071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       group1                       feature1    group2  \\\n",
       "899   FPocket           Hydrophobicity Score  Graphein   \n",
       "1008  FPocket  Amino Acid based volume Score  Graphein   \n",
       "892   FPocket           Hydrophobicity Score  Graphein   \n",
       "912   FPocket           Hydrophobicity Score  Graphein   \n",
       "1007  FPocket  Amino Acid based volume Score  Graphein   \n",
       "889   FPocket           Hydrophobicity Score  Graphein   \n",
       "918   FPocket           Hydrophobicity Score  Graphein   \n",
       "909   FPocket           Hydrophobicity Score  Graphein   \n",
       "896   FPocket           Hydrophobicity Score  Graphein   \n",
       "873   FPocket           Hydrophobicity Score  Graphein   \n",
       "891   FPocket           Hydrophobicity Score  Graphein   \n",
       "908   FPocket           Hydrophobicity Score  Graphein   \n",
       "932   FPocket           Hydrophobicity Score  Graphein   \n",
       "894   FPocket           Hydrophobicity Score  Graphein   \n",
       "900   FPocket           Hydrophobicity Score  Graphein   \n",
       "1022  FPocket  Amino Acid based volume Score  Graphein   \n",
       "923   FPocket           Hydrophobicity Score  Graphein   \n",
       "901   FPocket           Hydrophobicity Score  Graphein   \n",
       "1214  FPocket                   Charge Score  Graphein   \n",
       "926   FPocket           Hydrophobicity Score  Graphein   \n",
       "1017  FPocket  Amino Acid based volume Score  Graphein   \n",
       "893   FPocket           Hydrophobicity Score  Graphein   \n",
       "917   FPocket           Hydrophobicity Score  Graphein   \n",
       "905   FPocket           Hydrophobicity Score  Graphein   \n",
       "907   FPocket           Hydrophobicity Score  Graphein   \n",
       "888   FPocket           Hydrophobicity Score  Graphein   \n",
       "933   FPocket           Hydrophobicity Score  Graphein   \n",
       "911   FPocket           Hydrophobicity Score  Graphein   \n",
       "902   FPocket           Hydrophobicity Score  Graphein   \n",
       "913   FPocket           Hydrophobicity Score  Graphein   \n",
       "\n",
       "                     feature2  correlation  \n",
       "899            hphob_miyazawa     0.964340  \n",
       "1008                    dim_3     0.961953  \n",
       "892           hphob_manavalan     0.944778  \n",
       "912     transmembranetendency     0.943616  \n",
       "1007                    dim_2     0.941875  \n",
       "889               hphob_sweet     0.939917  \n",
       "918                hphob_rose     0.935967  \n",
       "909            hphob_mobility     0.932711  \n",
       "896            hphob_fauchere     0.922489  \n",
       "873                     dim_4     0.922489  \n",
       "891           hphob_doolittle     0.910350  \n",
       "908               hphob_ph7_5     0.908787  \n",
       "932          totalbeta_strand     0.905873  \n",
       "894               hphob_black     0.902452  \n",
       "900               hphob_argos     0.897789  \n",
       "1022             refractivity     0.896427  \n",
       "923          beta_sheetfasman     0.891402  \n",
       "901             hphob_roseman     0.881857  \n",
       "1214                    dim_5     0.879985  \n",
       "926            beta_sheetroux     0.876520  \n",
       "1017          molecularweight     0.870896  \n",
       "893                 hphob_leo     0.870786  \n",
       "917             hphob_chothia     0.870030  \n",
       "905              hphob_wilson     0.868411  \n",
       "907               hphob_ph3_4     0.861565  \n",
       "888           hphob_eisenberg     0.855867  \n",
       "933   antiparallelbeta_strand     0.851154  \n",
       "911                   hplctfa     0.844937  \n",
       "902             hphob_tanford     0.842770  \n",
       "913                   hplc2_1     0.840071  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrs_spearman = cross_group_correlations(train_data, \"spearman\")\n",
    "corrs_spearman.sort_values(\"correlation\", ascending=False).iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bef9e4d5-4cd9-4bb0-959f-fb2be99c94b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3685474/3580626995.py:20: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = corr_func(series1, series2) # series1[valid], series2[valid]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>feature1</th>\n",
       "      <th>group2</th>\n",
       "      <th>feature2</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25987</th>\n",
       "      <td>FreeSASA</td>\n",
       "      <td>relative-area_total</td>\n",
       "      <td>DSSP</td>\n",
       "      <td>relative ASA</td>\n",
       "      <td>0.995921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24068</th>\n",
       "      <td>DSSP</td>\n",
       "      <td>relative ASA</td>\n",
       "      <td>FreeSASA</td>\n",
       "      <td>relative-area_total</td>\n",
       "      <td>0.995921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27295</th>\n",
       "      <td>Melodia</td>\n",
       "      <td>psi</td>\n",
       "      <td>DSSP</td>\n",
       "      <td>psi</td>\n",
       "      <td>0.976629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24156</th>\n",
       "      <td>DSSP</td>\n",
       "      <td>psi</td>\n",
       "      <td>Melodia</td>\n",
       "      <td>psi</td>\n",
       "      <td>0.976629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_miyazawa</td>\n",
       "      <td>0.973125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_miyazawa</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.973125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24063</th>\n",
       "      <td>DSSP</td>\n",
       "      <td>relative ASA</td>\n",
       "      <td>FreeSASA</td>\n",
       "      <td>area_total</td>\n",
       "      <td>0.970873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25892</th>\n",
       "      <td>FreeSASA</td>\n",
       "      <td>area_total</td>\n",
       "      <td>DSSP</td>\n",
       "      <td>relative ASA</td>\n",
       "      <td>0.970873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_3</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>0.965673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_3</td>\n",
       "      <td>0.965673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>transmembranetendency</td>\n",
       "      <td>0.955661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>transmembranetendency</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.955661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_manavalan</td>\n",
       "      <td>0.955134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_manavalan</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.955134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_sweet</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.952752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_sweet</td>\n",
       "      <td>0.952752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_rose</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.949307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_rose</td>\n",
       "      <td>0.949307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26025</th>\n",
       "      <td>FreeSASA</td>\n",
       "      <td>relative-area_apolar</td>\n",
       "      <td>DSSP</td>\n",
       "      <td>relative ASA</td>\n",
       "      <td>0.948413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24070</th>\n",
       "      <td>DSSP</td>\n",
       "      <td>relative ASA</td>\n",
       "      <td>FreeSASA</td>\n",
       "      <td>relative-area_apolar</td>\n",
       "      <td>0.948413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_mobility</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.947806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_mobility</td>\n",
       "      <td>0.947806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_2</td>\n",
       "      <td>0.946838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_2</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Amino Acid based volume Score</td>\n",
       "      <td>0.946838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_fauchere</td>\n",
       "      <td>0.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_fauchere</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_4</td>\n",
       "      <td>0.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>dim_4</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.940516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_ph7_5</td>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>0.929956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>FPocket</td>\n",
       "      <td>Hydrophobicity Score</td>\n",
       "      <td>Graphein</td>\n",
       "      <td>hphob_ph7_5</td>\n",
       "      <td>0.929956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         group1                       feature1    group2  \\\n",
       "25987  FreeSASA            relative-area_total      DSSP   \n",
       "24068      DSSP                   relative ASA  FreeSASA   \n",
       "27295   Melodia                            psi      DSSP   \n",
       "24156      DSSP                            psi   Melodia   \n",
       "437     FPocket           Hydrophobicity Score  Graphein   \n",
       "4296   Graphein                 hphob_miyazawa   FPocket   \n",
       "24063      DSSP                   relative ASA  FreeSASA   \n",
       "25892  FreeSASA                     area_total      DSSP   \n",
       "3704   Graphein                          dim_3   FPocket   \n",
       "546     FPocket  Amino Acid based volume Score  Graphein   \n",
       "450     FPocket           Hydrophobicity Score  Graphein   \n",
       "4582   Graphein          transmembranetendency   FPocket   \n",
       "430     FPocket           Hydrophobicity Score  Graphein   \n",
       "4142   Graphein                hphob_manavalan   FPocket   \n",
       "4076   Graphein                    hphob_sweet   FPocket   \n",
       "427     FPocket           Hydrophobicity Score  Graphein   \n",
       "4714   Graphein                     hphob_rose   FPocket   \n",
       "456     FPocket           Hydrophobicity Score  Graphein   \n",
       "26025  FreeSASA           relative-area_apolar      DSSP   \n",
       "24070      DSSP                   relative ASA  FreeSASA   \n",
       "4516   Graphein                 hphob_mobility   FPocket   \n",
       "447     FPocket           Hydrophobicity Score  Graphein   \n",
       "545     FPocket  Amino Acid based volume Score  Graphein   \n",
       "3682   Graphein                          dim_2   FPocket   \n",
       "434     FPocket           Hydrophobicity Score  Graphein   \n",
       "4230   Graphein                 hphob_fauchere   FPocket   \n",
       "411     FPocket           Hydrophobicity Score  Graphein   \n",
       "3724   Graphein                          dim_4   FPocket   \n",
       "4494   Graphein                    hphob_ph7_5   FPocket   \n",
       "446     FPocket           Hydrophobicity Score  Graphein   \n",
       "\n",
       "                            feature2  correlation  \n",
       "25987                   relative ASA     0.995921  \n",
       "24068            relative-area_total     0.995921  \n",
       "27295                            psi     0.976629  \n",
       "24156                            psi     0.976629  \n",
       "437                   hphob_miyazawa     0.973125  \n",
       "4296            Hydrophobicity Score     0.973125  \n",
       "24063                     area_total     0.970873  \n",
       "25892                   relative ASA     0.970873  \n",
       "3704   Amino Acid based volume Score     0.965673  \n",
       "546                            dim_3     0.965673  \n",
       "450            transmembranetendency     0.955661  \n",
       "4582            Hydrophobicity Score     0.955661  \n",
       "430                  hphob_manavalan     0.955134  \n",
       "4142            Hydrophobicity Score     0.955134  \n",
       "4076            Hydrophobicity Score     0.952752  \n",
       "427                      hphob_sweet     0.952752  \n",
       "4714            Hydrophobicity Score     0.949307  \n",
       "456                       hphob_rose     0.949307  \n",
       "26025                   relative ASA     0.948413  \n",
       "24070           relative-area_apolar     0.948413  \n",
       "4516            Hydrophobicity Score     0.947806  \n",
       "447                   hphob_mobility     0.947806  \n",
       "545                            dim_2     0.946838  \n",
       "3682   Amino Acid based volume Score     0.946838  \n",
       "434                   hphob_fauchere     0.940516  \n",
       "4230            Hydrophobicity Score     0.940516  \n",
       "411                            dim_4     0.940516  \n",
       "3724            Hydrophobicity Score     0.940516  \n",
       "4494            Hydrophobicity Score     0.929956  \n",
       "446                      hphob_ph7_5     0.929956  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_group_correlations_all(df, method='pearson'):\n",
    "    corr_func = {'pearson': pearsonr, 'spearman': spearmanr}[method]\n",
    "    # cols = df.columns\n",
    "    cols = set( df.columns.get_level_values(0).unique() ) - {\"Pockets\", \"Label\"}\n",
    "    results = []\n",
    "\n",
    "    g1 = \"FPocket\"\n",
    "    # for i, col1 in enumerate(cols):\n",
    "    for g1 in cols:\n",
    "        for g2 in cols:\n",
    "            if g1 == g2:\n",
    "                continue\n",
    "            for f1 in df[g1]:\n",
    "                for f2 in df[g2]:\n",
    "                    series1 = df[(g1, f1)]\n",
    "                    series2 = df[(g2, f2)]\n",
    "                    # valid = series1.notna() & series2.notna()\n",
    "                    # if valid.sum() < 2:\n",
    "                    #     continue\n",
    "                    corr, _ = corr_func(series1, series2) # series1[valid], series2[valid]\n",
    "                    results.append({\n",
    "                        'group1': g1, 'feature1': f1,\n",
    "                        'group2': g2, 'feature2': f2,\n",
    "                        'correlation': corr\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "corrs_all = cross_group_correlations_all(train_data).sort_values(\"correlation\", ascending=False).iloc[:30]\n",
    "corrs_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f72fb7-2ca6-4665-8495-ea63986d8454",
   "metadata": {},
   "source": [
    "## Dataframe processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041a80cc-ad8f-4284-9baf-a510d2983ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    df.index = df[\"Pockets\"][[\"pdb\", \"pocket\"]].apply(lambda x: \"_\".join(x), axis=1)\n",
    "    df = df.drop(columns=[\"Pockets\"], level=0)\n",
    "    df.columns = map(lambda x: \"_\".join(x), df.columns.values)\n",
    "    df.loc[:,'Label_label'] = df['Label_label'].astype(\"category\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50371457-11a5-436e-b211-ec871e2196f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4031648/572353621.py:1: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  train = process_dataframe(train_data.drop(columns=\"FPocket\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label_label</th>\n",
       "      <th>Amino acids_label_comp_id_A</th>\n",
       "      <th>Amino acids_label_comp_id_C</th>\n",
       "      <th>Amino acids_label_comp_id_D</th>\n",
       "      <th>Amino acids_label_comp_id_E</th>\n",
       "      <th>Amino acids_label_comp_id_F</th>\n",
       "      <th>Amino acids_label_comp_id_G</th>\n",
       "      <th>Amino acids_label_comp_id_H</th>\n",
       "      <th>Amino acids_label_comp_id_I</th>\n",
       "      <th>Amino acids_label_comp_id_K</th>\n",
       "      <th>...</th>\n",
       "      <th>HHBlits_M-&gt;M</th>\n",
       "      <th>HHBlits_M-&gt;I</th>\n",
       "      <th>HHBlits_M-&gt;D</th>\n",
       "      <th>HHBlits_I-&gt;M</th>\n",
       "      <th>HHBlits_I-&gt;I</th>\n",
       "      <th>HHBlits_D-&gt;M</th>\n",
       "      <th>HHBlits_D-&gt;D</th>\n",
       "      <th>HHBlits_Neff</th>\n",
       "      <th>HHBlits_Neff_I</th>\n",
       "      <th>HHBlits_Neff_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971722</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>0.176411</td>\n",
       "      <td>0.823637</td>\n",
       "      <td>0.151071</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>9.994750</td>\n",
       "      <td>1.156583</td>\n",
       "      <td>2.557083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974105</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.013511</td>\n",
       "      <td>0.187106</td>\n",
       "      <td>0.812923</td>\n",
       "      <td>0.253341</td>\n",
       "      <td>0.746686</td>\n",
       "      <td>9.824308</td>\n",
       "      <td>1.154846</td>\n",
       "      <td>2.152077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.491783</td>\n",
       "      <td>0.508309</td>\n",
       "      <td>0.206654</td>\n",
       "      <td>0.793426</td>\n",
       "      <td>10.325625</td>\n",
       "      <td>1.141812</td>\n",
       "      <td>1.770438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888404</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>0.274259</td>\n",
       "      <td>0.725673</td>\n",
       "      <td>0.135536</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>10.183727</td>\n",
       "      <td>1.275091</td>\n",
       "      <td>3.316091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957620</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>0.020451</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>0.627444</td>\n",
       "      <td>0.219810</td>\n",
       "      <td>0.780225</td>\n",
       "      <td>10.475976</td>\n",
       "      <td>1.187833</td>\n",
       "      <td>1.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989698</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.505737</td>\n",
       "      <td>0.494248</td>\n",
       "      <td>0.289794</td>\n",
       "      <td>0.710212</td>\n",
       "      <td>12.009636</td>\n",
       "      <td>1.059273</td>\n",
       "      <td>1.135545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912140</td>\n",
       "      <td>0.040368</td>\n",
       "      <td>0.047440</td>\n",
       "      <td>0.444674</td>\n",
       "      <td>0.502660</td>\n",
       "      <td>0.245143</td>\n",
       "      <td>0.754912</td>\n",
       "      <td>8.720632</td>\n",
       "      <td>1.300158</td>\n",
       "      <td>2.198632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954826</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.390967</td>\n",
       "      <td>0.432574</td>\n",
       "      <td>0.214760</td>\n",
       "      <td>0.785243</td>\n",
       "      <td>8.712882</td>\n",
       "      <td>1.092588</td>\n",
       "      <td>1.600118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978512</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.558250</td>\n",
       "      <td>0.298870</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.790977</td>\n",
       "      <td>8.564143</td>\n",
       "      <td>0.943143</td>\n",
       "      <td>2.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938259</td>\n",
       "      <td>0.039010</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.460395</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>0.635664</td>\n",
       "      <td>9.773813</td>\n",
       "      <td>1.138000</td>\n",
       "      <td>1.508125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4112 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Label_label  Amino acids_label_comp_id_A  \\\n",
       "6ta3_pocket16            0                     0.000000   \n",
       "6ta3_pocket18            0                     0.000000   \n",
       "6ta3_pocket11            0                     0.062500   \n",
       "6ta3_pocket15            0                     0.000000   \n",
       "6ta3_pocket7             0                     0.095238   \n",
       "...                    ...                          ...   \n",
       "3d2p_pocket5             0                     0.090909   \n",
       "3d2p_pocket8             0                     0.157895   \n",
       "3d2p_pocket3             0                     0.058824   \n",
       "3d2p_pocket12            0                     0.000000   \n",
       "3d2p_pocket6             0                     0.187500   \n",
       "\n",
       "               Amino acids_label_comp_id_C  Amino acids_label_comp_id_D  \\\n",
       "6ta3_pocket16                     0.000000                     0.000000   \n",
       "6ta3_pocket18                     0.076923                     0.076923   \n",
       "6ta3_pocket11                     0.187500                     0.000000   \n",
       "6ta3_pocket15                     0.000000                     0.000000   \n",
       "6ta3_pocket7                      0.071429                     0.023810   \n",
       "...                                    ...                          ...   \n",
       "3d2p_pocket5                      0.090909                     0.090909   \n",
       "3d2p_pocket8                      0.052632                     0.052632   \n",
       "3d2p_pocket3                      0.000000                     0.000000   \n",
       "3d2p_pocket12                     0.000000                     0.000000   \n",
       "3d2p_pocket6                      0.000000                     0.125000   \n",
       "\n",
       "               Amino acids_label_comp_id_E  Amino acids_label_comp_id_F  \\\n",
       "6ta3_pocket16                     0.083333                     0.083333   \n",
       "6ta3_pocket18                     0.000000                     0.000000   \n",
       "6ta3_pocket11                     0.000000                     0.062500   \n",
       "6ta3_pocket15                     0.090909                     0.090909   \n",
       "6ta3_pocket7                      0.000000                     0.071429   \n",
       "...                                    ...                          ...   \n",
       "3d2p_pocket5                      0.000000                     0.000000   \n",
       "3d2p_pocket8                      0.052632                     0.000000   \n",
       "3d2p_pocket3                      0.058824                     0.058824   \n",
       "3d2p_pocket12                     0.142857                     0.000000   \n",
       "3d2p_pocket6                      0.000000                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_G  Amino acids_label_comp_id_H  \\\n",
       "6ta3_pocket16                     0.000000                     0.000000   \n",
       "6ta3_pocket18                     0.000000                     0.000000   \n",
       "6ta3_pocket11                     0.000000                     0.000000   \n",
       "6ta3_pocket15                     0.000000                     0.000000   \n",
       "6ta3_pocket7                      0.023810                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "3d2p_pocket5                      0.090909                     0.090909   \n",
       "3d2p_pocket8                      0.105263                     0.000000   \n",
       "3d2p_pocket3                      0.176471                     0.117647   \n",
       "3d2p_pocket12                     0.142857                     0.000000   \n",
       "3d2p_pocket6                      0.187500                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_I  Amino acids_label_comp_id_K  ...  \\\n",
       "6ta3_pocket16                     0.166667                     0.083333  ...   \n",
       "6ta3_pocket18                     0.076923                     0.076923  ...   \n",
       "6ta3_pocket11                     0.000000                     0.062500  ...   \n",
       "6ta3_pocket15                     0.090909                     0.000000  ...   \n",
       "6ta3_pocket7                      0.142857                     0.071429  ...   \n",
       "...                                    ...                          ...  ...   \n",
       "3d2p_pocket5                      0.181818                     0.000000  ...   \n",
       "3d2p_pocket8                      0.000000                     0.000000  ...   \n",
       "3d2p_pocket3                      0.000000                     0.000000  ...   \n",
       "3d2p_pocket12                     0.000000                     0.142857  ...   \n",
       "3d2p_pocket6                      0.000000                     0.062500  ...   \n",
       "\n",
       "               HHBlits_M->M  HHBlits_M->I  HHBlits_M->D  HHBlits_I->M  \\\n",
       "6ta3_pocket16      0.971722      0.011249      0.017076      0.176411   \n",
       "6ta3_pocket18      0.974105      0.012294      0.013511      0.187106   \n",
       "6ta3_pocket11      0.968439      0.015852      0.015674      0.491783   \n",
       "6ta3_pocket15      0.888404      0.030708      0.080922      0.274259   \n",
       "6ta3_pocket7       0.957620      0.021926      0.020451      0.348753   \n",
       "...                     ...           ...           ...           ...   \n",
       "3d2p_pocket5       0.989698      0.006181      0.004065      0.505737   \n",
       "3d2p_pocket8       0.912140      0.040368      0.047440      0.444674   \n",
       "3d2p_pocket3       0.954826      0.028198      0.016987      0.390967   \n",
       "3d2p_pocket12      0.978512      0.008635      0.012686      0.558250   \n",
       "3d2p_pocket6       0.938259      0.039010      0.022726      0.460395   \n",
       "\n",
       "               HHBlits_I->I  HHBlits_D->M  HHBlits_D->D  HHBlits_Neff  \\\n",
       "6ta3_pocket16      0.823637      0.151071      0.848930      9.994750   \n",
       "6ta3_pocket18      0.812923      0.253341      0.746686      9.824308   \n",
       "6ta3_pocket11      0.508309      0.206654      0.793426     10.325625   \n",
       "6ta3_pocket15      0.725673      0.135536      0.864436     10.183727   \n",
       "6ta3_pocket7       0.627444      0.219810      0.780225     10.475976   \n",
       "...                     ...           ...           ...           ...   \n",
       "3d2p_pocket5       0.494248      0.289794      0.710212     12.009636   \n",
       "3d2p_pocket8       0.502660      0.245143      0.754912      8.720632   \n",
       "3d2p_pocket3       0.432574      0.214760      0.785243      8.712882   \n",
       "3d2p_pocket12      0.298870      0.209000      0.790977      8.564143   \n",
       "3d2p_pocket6       0.352079      0.364345      0.635664      9.773813   \n",
       "\n",
       "               HHBlits_Neff_I  HHBlits_Neff_D  \n",
       "6ta3_pocket16        1.156583        2.557083  \n",
       "6ta3_pocket18        1.154846        2.152077  \n",
       "6ta3_pocket11        1.141812        1.770438  \n",
       "6ta3_pocket15        1.275091        3.316091  \n",
       "6ta3_pocket7         1.187833        1.982500  \n",
       "...                       ...             ...  \n",
       "3d2p_pocket5         1.059273        1.135545  \n",
       "3d2p_pocket8         1.300158        2.198632  \n",
       "3d2p_pocket3         1.092588        1.600118  \n",
       "3d2p_pocket12        0.943143        2.130000  \n",
       "3d2p_pocket6         1.138000        1.508125  \n",
       "\n",
       "[4112 rows x 167 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = process_dataframe(train_data.drop(columns=\"FPocket\"))\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efed703-ac2b-40aa-8be9-cab00d22673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4031648/761373645.py:1: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  test = process_dataframe(test_data.drop(columns=\"FPocket\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label_label</th>\n",
       "      <th>Amino acids_label_comp_id_A</th>\n",
       "      <th>Amino acids_label_comp_id_C</th>\n",
       "      <th>Amino acids_label_comp_id_D</th>\n",
       "      <th>Amino acids_label_comp_id_E</th>\n",
       "      <th>Amino acids_label_comp_id_F</th>\n",
       "      <th>Amino acids_label_comp_id_G</th>\n",
       "      <th>Amino acids_label_comp_id_H</th>\n",
       "      <th>Amino acids_label_comp_id_I</th>\n",
       "      <th>Amino acids_label_comp_id_K</th>\n",
       "      <th>...</th>\n",
       "      <th>HHBlits_M-&gt;M</th>\n",
       "      <th>HHBlits_M-&gt;I</th>\n",
       "      <th>HHBlits_M-&gt;D</th>\n",
       "      <th>HHBlits_I-&gt;M</th>\n",
       "      <th>HHBlits_I-&gt;I</th>\n",
       "      <th>HHBlits_D-&gt;M</th>\n",
       "      <th>HHBlits_D-&gt;D</th>\n",
       "      <th>HHBlits_Neff</th>\n",
       "      <th>HHBlits_Neff_I</th>\n",
       "      <th>HHBlits_Neff_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5kwj_pocket1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943829</td>\n",
       "      <td>0.019534</td>\n",
       "      <td>0.036645</td>\n",
       "      <td>0.374693</td>\n",
       "      <td>0.538332</td>\n",
       "      <td>0.188763</td>\n",
       "      <td>0.811216</td>\n",
       "      <td>12.796000</td>\n",
       "      <td>1.211870</td>\n",
       "      <td>4.778174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5kwj_pocket2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981054</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>0.325240</td>\n",
       "      <td>0.674772</td>\n",
       "      <td>0.140677</td>\n",
       "      <td>0.859313</td>\n",
       "      <td>13.194263</td>\n",
       "      <td>1.298421</td>\n",
       "      <td>3.082842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5kwj_pocket4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982741</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.008821</td>\n",
       "      <td>0.355841</td>\n",
       "      <td>0.585323</td>\n",
       "      <td>0.447145</td>\n",
       "      <td>0.552787</td>\n",
       "      <td>12.890706</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>3.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5kwj_pocket5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939657</td>\n",
       "      <td>0.013509</td>\n",
       "      <td>0.046819</td>\n",
       "      <td>0.290462</td>\n",
       "      <td>0.709510</td>\n",
       "      <td>0.161189</td>\n",
       "      <td>0.838811</td>\n",
       "      <td>13.071500</td>\n",
       "      <td>1.273100</td>\n",
       "      <td>5.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5kwj_pocket3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945818</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.346038</td>\n",
       "      <td>0.654003</td>\n",
       "      <td>0.080462</td>\n",
       "      <td>0.919533</td>\n",
       "      <td>13.144091</td>\n",
       "      <td>1.502727</td>\n",
       "      <td>4.729273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4op0_pocket10</th>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965050</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.476154</td>\n",
       "      <td>0.523834</td>\n",
       "      <td>0.371946</td>\n",
       "      <td>0.627957</td>\n",
       "      <td>8.875000</td>\n",
       "      <td>1.278600</td>\n",
       "      <td>1.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4op0_pocket5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937658</td>\n",
       "      <td>0.029391</td>\n",
       "      <td>0.033036</td>\n",
       "      <td>0.435269</td>\n",
       "      <td>0.564699</td>\n",
       "      <td>0.380508</td>\n",
       "      <td>0.619474</td>\n",
       "      <td>8.783900</td>\n",
       "      <td>1.317900</td>\n",
       "      <td>1.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4op0_pocket8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.036707</td>\n",
       "      <td>0.076581</td>\n",
       "      <td>0.679256</td>\n",
       "      <td>0.320693</td>\n",
       "      <td>0.594901</td>\n",
       "      <td>0.405162</td>\n",
       "      <td>8.536545</td>\n",
       "      <td>1.264727</td>\n",
       "      <td>1.994273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4op0_pocket3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977747</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.013359</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.390611</td>\n",
       "      <td>0.434866</td>\n",
       "      <td>0.565146</td>\n",
       "      <td>8.767200</td>\n",
       "      <td>0.862850</td>\n",
       "      <td>1.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4op0_pocket6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950238</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>0.032596</td>\n",
       "      <td>0.595731</td>\n",
       "      <td>0.345453</td>\n",
       "      <td>0.552496</td>\n",
       "      <td>0.329852</td>\n",
       "      <td>8.337294</td>\n",
       "      <td>1.154706</td>\n",
       "      <td>1.500765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Label_label  Amino acids_label_comp_id_A  \\\n",
       "5kwj_pocket1             1                     0.021739   \n",
       "5kwj_pocket2             0                     0.000000   \n",
       "5kwj_pocket4             0                     0.058824   \n",
       "5kwj_pocket5             0                     0.000000   \n",
       "5kwj_pocket3             0                     0.090909   \n",
       "...                    ...                          ...   \n",
       "4op0_pocket10            0                     0.200000   \n",
       "4op0_pocket5             0                     0.000000   \n",
       "4op0_pocket8             0                     0.000000   \n",
       "4op0_pocket3             0                     0.050000   \n",
       "4op0_pocket6             0                     0.117647   \n",
       "\n",
       "               Amino acids_label_comp_id_C  Amino acids_label_comp_id_D  \\\n",
       "5kwj_pocket1                      0.021739                     0.043478   \n",
       "5kwj_pocket2                      0.000000                     0.000000   \n",
       "5kwj_pocket4                      0.000000                     0.117647   \n",
       "5kwj_pocket5                      0.000000                     0.000000   \n",
       "5kwj_pocket3                      0.000000                     0.090909   \n",
       "...                                    ...                          ...   \n",
       "4op0_pocket10                     0.000000                     0.000000   \n",
       "4op0_pocket5                      0.000000                     0.100000   \n",
       "4op0_pocket8                      0.000000                     0.000000   \n",
       "4op0_pocket3                      0.000000                     0.100000   \n",
       "4op0_pocket6                      0.000000                     0.117647   \n",
       "\n",
       "               Amino acids_label_comp_id_E  Amino acids_label_comp_id_F  \\\n",
       "5kwj_pocket1                      0.021739                     0.130435   \n",
       "5kwj_pocket2                      0.052632                     0.105263   \n",
       "5kwj_pocket4                      0.000000                     0.000000   \n",
       "5kwj_pocket5                      0.100000                     0.000000   \n",
       "5kwj_pocket3                      0.090909                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "4op0_pocket10                     0.000000                     0.000000   \n",
       "4op0_pocket5                      0.100000                     0.000000   \n",
       "4op0_pocket8                      0.000000                     0.000000   \n",
       "4op0_pocket3                      0.050000                     0.000000   \n",
       "4op0_pocket6                      0.000000                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_G  Amino acids_label_comp_id_H  \\\n",
       "5kwj_pocket1                      0.108696                     0.021739   \n",
       "5kwj_pocket2                      0.052632                     0.000000   \n",
       "5kwj_pocket4                      0.117647                     0.000000   \n",
       "5kwj_pocket5                      0.100000                     0.000000   \n",
       "5kwj_pocket3                      0.000000                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "4op0_pocket10                     0.100000                     0.000000   \n",
       "4op0_pocket5                      0.300000                     0.000000   \n",
       "4op0_pocket8                      0.090909                     0.000000   \n",
       "4op0_pocket3                      0.050000                     0.050000   \n",
       "4op0_pocket6                      0.000000                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_I  Amino acids_label_comp_id_K  ...  \\\n",
       "5kwj_pocket1                      0.043478                     0.000000  ...   \n",
       "5kwj_pocket2                      0.105263                     0.000000  ...   \n",
       "5kwj_pocket4                      0.058824                     0.058824  ...   \n",
       "5kwj_pocket5                      0.000000                     0.000000  ...   \n",
       "5kwj_pocket3                      0.000000                     0.090909  ...   \n",
       "...                                    ...                          ...  ...   \n",
       "4op0_pocket10                     0.100000                     0.000000  ...   \n",
       "4op0_pocket5                      0.200000                     0.000000  ...   \n",
       "4op0_pocket8                      0.090909                     0.000000  ...   \n",
       "4op0_pocket3                      0.000000                     0.100000  ...   \n",
       "4op0_pocket6                      0.000000                     0.000000  ...   \n",
       "\n",
       "               HHBlits_M->M  HHBlits_M->I  HHBlits_M->D  HHBlits_I->M  \\\n",
       "5kwj_pocket1       0.943829      0.019534      0.036645      0.374693   \n",
       "5kwj_pocket2       0.981054      0.011601      0.007315      0.325240   \n",
       "5kwj_pocket4       0.982741      0.008372      0.008821      0.355841   \n",
       "5kwj_pocket5       0.939657      0.013509      0.046819      0.290462   \n",
       "5kwj_pocket3       0.945818      0.024418      0.029730      0.346038   \n",
       "...                     ...           ...           ...           ...   \n",
       "4op0_pocket10      0.965050      0.021347      0.013608      0.476154   \n",
       "4op0_pocket5       0.937658      0.029391      0.033036      0.435269   \n",
       "4op0_pocket8       0.886737      0.036707      0.076581      0.679256   \n",
       "4op0_pocket3       0.977747      0.008794      0.013359      0.359375   \n",
       "4op0_pocket6       0.950238      0.017191      0.032596      0.595731   \n",
       "\n",
       "               HHBlits_I->I  HHBlits_D->M  HHBlits_D->D  HHBlits_Neff  \\\n",
       "5kwj_pocket1       0.538332      0.188763      0.811216     12.796000   \n",
       "5kwj_pocket2       0.674772      0.140677      0.859313     13.194263   \n",
       "5kwj_pocket4       0.585323      0.447145      0.552787     12.890706   \n",
       "5kwj_pocket5       0.709510      0.161189      0.838811     13.071500   \n",
       "5kwj_pocket3       0.654003      0.080462      0.919533     13.144091   \n",
       "...                     ...           ...           ...           ...   \n",
       "4op0_pocket10      0.523834      0.371946      0.627957      8.875000   \n",
       "4op0_pocket5       0.564699      0.380508      0.619474      8.783900   \n",
       "4op0_pocket8       0.320693      0.594901      0.405162      8.536545   \n",
       "4op0_pocket3       0.390611      0.434866      0.565146      8.767200   \n",
       "4op0_pocket6       0.345453      0.552496      0.329852      8.337294   \n",
       "\n",
       "               HHBlits_Neff_I  HHBlits_Neff_D  \n",
       "5kwj_pocket1         1.211870        4.778174  \n",
       "5kwj_pocket2         1.298421        3.082842  \n",
       "5kwj_pocket4         1.117647        3.610000  \n",
       "5kwj_pocket5         1.273100        5.003400  \n",
       "5kwj_pocket3         1.502727        4.729273  \n",
       "...                       ...             ...  \n",
       "4op0_pocket10        1.278600        1.569400  \n",
       "4op0_pocket5         1.317900        1.763600  \n",
       "4op0_pocket8         1.264727        1.994273  \n",
       "4op0_pocket3         0.862850        1.343000  \n",
       "4op0_pocket6         1.154706        1.500765  \n",
       "\n",
       "[1236 rows x 167 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = process_dataframe(test_data.drop(columns=\"FPocket\"))\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145ba01-749d-4a97-9abe-37b0a553ea91",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55639c0-f74a-4ac4-91b9-1bec52ff3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8307aca4-a31c-423c-b48d-453a1df8cc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label_label</th>\n",
       "      <th>Amino acids_label_comp_id_A</th>\n",
       "      <th>Amino acids_label_comp_id_C</th>\n",
       "      <th>Amino acids_label_comp_id_D</th>\n",
       "      <th>Amino acids_label_comp_id_E</th>\n",
       "      <th>Amino acids_label_comp_id_F</th>\n",
       "      <th>Amino acids_label_comp_id_G</th>\n",
       "      <th>Amino acids_label_comp_id_H</th>\n",
       "      <th>Amino acids_label_comp_id_I</th>\n",
       "      <th>Amino acids_label_comp_id_K</th>\n",
       "      <th>...</th>\n",
       "      <th>HHBlits_M-&gt;M</th>\n",
       "      <th>HHBlits_M-&gt;I</th>\n",
       "      <th>HHBlits_M-&gt;D</th>\n",
       "      <th>HHBlits_I-&gt;M</th>\n",
       "      <th>HHBlits_I-&gt;I</th>\n",
       "      <th>HHBlits_D-&gt;M</th>\n",
       "      <th>HHBlits_D-&gt;D</th>\n",
       "      <th>HHBlits_Neff</th>\n",
       "      <th>HHBlits_Neff_I</th>\n",
       "      <th>HHBlits_Neff_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971722</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>0.176411</td>\n",
       "      <td>0.823637</td>\n",
       "      <td>0.151071</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>9.994750</td>\n",
       "      <td>1.156583</td>\n",
       "      <td>2.557083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974105</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.013511</td>\n",
       "      <td>0.187106</td>\n",
       "      <td>0.812923</td>\n",
       "      <td>0.253341</td>\n",
       "      <td>0.746686</td>\n",
       "      <td>9.824308</td>\n",
       "      <td>1.154846</td>\n",
       "      <td>2.152077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968439</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.491783</td>\n",
       "      <td>0.508309</td>\n",
       "      <td>0.206654</td>\n",
       "      <td>0.793426</td>\n",
       "      <td>10.325625</td>\n",
       "      <td>1.141812</td>\n",
       "      <td>1.770438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888404</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>0.274259</td>\n",
       "      <td>0.725673</td>\n",
       "      <td>0.135536</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>10.183727</td>\n",
       "      <td>1.275091</td>\n",
       "      <td>3.316091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ta3_pocket7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957620</td>\n",
       "      <td>0.021926</td>\n",
       "      <td>0.020451</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>0.627444</td>\n",
       "      <td>0.219810</td>\n",
       "      <td>0.780225</td>\n",
       "      <td>10.475976</td>\n",
       "      <td>1.187833</td>\n",
       "      <td>1.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989698</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.505737</td>\n",
       "      <td>0.494248</td>\n",
       "      <td>0.289794</td>\n",
       "      <td>0.710212</td>\n",
       "      <td>12.009636</td>\n",
       "      <td>1.059273</td>\n",
       "      <td>1.135545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912140</td>\n",
       "      <td>0.040368</td>\n",
       "      <td>0.047440</td>\n",
       "      <td>0.444674</td>\n",
       "      <td>0.502660</td>\n",
       "      <td>0.245143</td>\n",
       "      <td>0.754912</td>\n",
       "      <td>8.720632</td>\n",
       "      <td>1.300158</td>\n",
       "      <td>2.198632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954826</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.390967</td>\n",
       "      <td>0.432574</td>\n",
       "      <td>0.214760</td>\n",
       "      <td>0.785243</td>\n",
       "      <td>8.712882</td>\n",
       "      <td>1.092588</td>\n",
       "      <td>1.600118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978512</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.558250</td>\n",
       "      <td>0.298870</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.790977</td>\n",
       "      <td>8.564143</td>\n",
       "      <td>0.943143</td>\n",
       "      <td>2.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d2p_pocket6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938259</td>\n",
       "      <td>0.039010</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0.460395</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>0.635664</td>\n",
       "      <td>9.773813</td>\n",
       "      <td>1.138000</td>\n",
       "      <td>1.508125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4112 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Label_label  Amino acids_label_comp_id_A  \\\n",
       "6ta3_pocket16            0                     0.000000   \n",
       "6ta3_pocket18            0                     0.000000   \n",
       "6ta3_pocket11            0                     0.062500   \n",
       "6ta3_pocket15            0                     0.000000   \n",
       "6ta3_pocket7             0                     0.095238   \n",
       "...                    ...                          ...   \n",
       "3d2p_pocket5             0                     0.090909   \n",
       "3d2p_pocket8             0                     0.157895   \n",
       "3d2p_pocket3             0                     0.058824   \n",
       "3d2p_pocket12            0                     0.000000   \n",
       "3d2p_pocket6             0                     0.187500   \n",
       "\n",
       "               Amino acids_label_comp_id_C  Amino acids_label_comp_id_D  \\\n",
       "6ta3_pocket16                     0.000000                     0.000000   \n",
       "6ta3_pocket18                     0.076923                     0.076923   \n",
       "6ta3_pocket11                     0.187500                     0.000000   \n",
       "6ta3_pocket15                     0.000000                     0.000000   \n",
       "6ta3_pocket7                      0.071429                     0.023810   \n",
       "...                                    ...                          ...   \n",
       "3d2p_pocket5                      0.090909                     0.090909   \n",
       "3d2p_pocket8                      0.052632                     0.052632   \n",
       "3d2p_pocket3                      0.000000                     0.000000   \n",
       "3d2p_pocket12                     0.000000                     0.000000   \n",
       "3d2p_pocket6                      0.000000                     0.125000   \n",
       "\n",
       "               Amino acids_label_comp_id_E  Amino acids_label_comp_id_F  \\\n",
       "6ta3_pocket16                     0.083333                     0.083333   \n",
       "6ta3_pocket18                     0.000000                     0.000000   \n",
       "6ta3_pocket11                     0.000000                     0.062500   \n",
       "6ta3_pocket15                     0.090909                     0.090909   \n",
       "6ta3_pocket7                      0.000000                     0.071429   \n",
       "...                                    ...                          ...   \n",
       "3d2p_pocket5                      0.000000                     0.000000   \n",
       "3d2p_pocket8                      0.052632                     0.000000   \n",
       "3d2p_pocket3                      0.058824                     0.058824   \n",
       "3d2p_pocket12                     0.142857                     0.000000   \n",
       "3d2p_pocket6                      0.000000                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_G  Amino acids_label_comp_id_H  \\\n",
       "6ta3_pocket16                     0.000000                     0.000000   \n",
       "6ta3_pocket18                     0.000000                     0.000000   \n",
       "6ta3_pocket11                     0.000000                     0.000000   \n",
       "6ta3_pocket15                     0.000000                     0.000000   \n",
       "6ta3_pocket7                      0.023810                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "3d2p_pocket5                      0.090909                     0.090909   \n",
       "3d2p_pocket8                      0.105263                     0.000000   \n",
       "3d2p_pocket3                      0.176471                     0.117647   \n",
       "3d2p_pocket12                     0.142857                     0.000000   \n",
       "3d2p_pocket6                      0.187500                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_I  Amino acids_label_comp_id_K  ...  \\\n",
       "6ta3_pocket16                     0.166667                     0.083333  ...   \n",
       "6ta3_pocket18                     0.076923                     0.076923  ...   \n",
       "6ta3_pocket11                     0.000000                     0.062500  ...   \n",
       "6ta3_pocket15                     0.090909                     0.000000  ...   \n",
       "6ta3_pocket7                      0.142857                     0.071429  ...   \n",
       "...                                    ...                          ...  ...   \n",
       "3d2p_pocket5                      0.181818                     0.000000  ...   \n",
       "3d2p_pocket8                      0.000000                     0.000000  ...   \n",
       "3d2p_pocket3                      0.000000                     0.000000  ...   \n",
       "3d2p_pocket12                     0.000000                     0.142857  ...   \n",
       "3d2p_pocket6                      0.000000                     0.062500  ...   \n",
       "\n",
       "               HHBlits_M->M  HHBlits_M->I  HHBlits_M->D  HHBlits_I->M  \\\n",
       "6ta3_pocket16      0.971722      0.011249      0.017076      0.176411   \n",
       "6ta3_pocket18      0.974105      0.012294      0.013511      0.187106   \n",
       "6ta3_pocket11      0.968439      0.015852      0.015674      0.491783   \n",
       "6ta3_pocket15      0.888404      0.030708      0.080922      0.274259   \n",
       "6ta3_pocket7       0.957620      0.021926      0.020451      0.348753   \n",
       "...                     ...           ...           ...           ...   \n",
       "3d2p_pocket5       0.989698      0.006181      0.004065      0.505737   \n",
       "3d2p_pocket8       0.912140      0.040368      0.047440      0.444674   \n",
       "3d2p_pocket3       0.954826      0.028198      0.016987      0.390967   \n",
       "3d2p_pocket12      0.978512      0.008635      0.012686      0.558250   \n",
       "3d2p_pocket6       0.938259      0.039010      0.022726      0.460395   \n",
       "\n",
       "               HHBlits_I->I  HHBlits_D->M  HHBlits_D->D  HHBlits_Neff  \\\n",
       "6ta3_pocket16      0.823637      0.151071      0.848930      9.994750   \n",
       "6ta3_pocket18      0.812923      0.253341      0.746686      9.824308   \n",
       "6ta3_pocket11      0.508309      0.206654      0.793426     10.325625   \n",
       "6ta3_pocket15      0.725673      0.135536      0.864436     10.183727   \n",
       "6ta3_pocket7       0.627444      0.219810      0.780225     10.475976   \n",
       "...                     ...           ...           ...           ...   \n",
       "3d2p_pocket5       0.494248      0.289794      0.710212     12.009636   \n",
       "3d2p_pocket8       0.502660      0.245143      0.754912      8.720632   \n",
       "3d2p_pocket3       0.432574      0.214760      0.785243      8.712882   \n",
       "3d2p_pocket12      0.298870      0.209000      0.790977      8.564143   \n",
       "3d2p_pocket6       0.352079      0.364345      0.635664      9.773813   \n",
       "\n",
       "               HHBlits_Neff_I  HHBlits_Neff_D  \n",
       "6ta3_pocket16        1.156583        2.557083  \n",
       "6ta3_pocket18        1.154846        2.152077  \n",
       "6ta3_pocket11        1.141812        1.770438  \n",
       "6ta3_pocket15        1.275091        3.316091  \n",
       "6ta3_pocket7         1.187833        1.982500  \n",
       "...                       ...             ...  \n",
       "3d2p_pocket5         1.059273        1.135545  \n",
       "3d2p_pocket8         1.300158        2.198632  \n",
       "3d2p_pocket3         1.092588        1.600118  \n",
       "3d2p_pocket12        0.943143        2.130000  \n",
       "3d2p_pocket6         1.138000        1.508125  \n",
       "\n",
       "[4112 rows x 167 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset = TabularDataset(train)\n",
    "\n",
    "traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6875f7db-ab9d-4319-8ef8-bc5f1eba6e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset.columns[ traindataset.isna().any() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07f1fd2-a622-44b6-b9e3-a408eb1a319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = TabularDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ca8160-fb27-44bf-8011-52bb81b2c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bcc575e-0cdf-454b-ac00-8a3ff0aafbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.core.metrics import make_scorer\n",
    "\n",
    "mcc = make_scorer(\n",
    "    name='mcc',\n",
    "    score_func=matthews_corrcoef,\n",
    "    optimum=1,\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c251f2d6-42d9-4c93-a1ef-2feb1e58e18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TabularPredictor in module autogluon.tabular.predictor.predictor:\n",
      "\n",
      "class TabularPredictor(autogluon.tabular.predictor._deprecated_methods.TabularPredictorDeprecatedMixin)\n",
      " |  TabularPredictor(label: 'str', problem_type: 'str' = None, eval_metric: 'str | Scorer' = None, path: 'str' = None, verbosity: 'int' = 2, log_to_file: 'bool' = False, log_file_path: 'str' = 'auto', sample_weight: 'str' = None, weight_evaluation: 'bool' = False, groups: 'str' = None, positive_class: 'int | str | None' = None, **kwargs)\n",
      " |  \n",
      " |  AutoGluon TabularPredictor predicts values in a column of a tabular dataset (classification or regression).\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  label : str\n",
      " |      Name of the column that contains the target variable to predict.\n",
      " |  problem_type : str, default = None\n",
      " |      Type of prediction problem, i.e. is this a binary/multiclass classification or regression problem (options: 'binary', 'multiclass', 'regression', 'quantile').\n",
      " |      If `problem_type = None`, the prediction problem type is inferred based on the label-values in provided dataset.\n",
      " |  eval_metric : str or Scorer, default = None\n",
      " |      Metric by which predictions will be ultimately evaluated on test data.\n",
      " |      AutoGluon tunes factors such as hyperparameters, early-stopping, ensemble-weights, etc. in order to improve this metric on validation data.\n",
      " |  \n",
      " |      If `eval_metric = None`, it is automatically chosen based on `problem_type`.\n",
      " |      Defaults to 'accuracy' for binary and multiclass classification, 'root_mean_squared_error' for regression, and 'pinball_loss' for quantile.\n",
      " |  \n",
      " |      Otherwise, options for classification:\n",
      " |          ['accuracy', 'balanced_accuracy', 'log_loss', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted',\n",
      " |          'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_macro', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_macro', 'roc_auc_ovr_micro',\n",
      " |          'roc_auc_ovr_weighted', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted',\n",
      " |          'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'mcc', 'pac_score']\n",
      " |      Options for regression:\n",
      " |          ['root_mean_squared_error', 'mean_squared_error', 'mean_absolute_error', 'median_absolute_error', 'mean_absolute_percentage_error', 'r2', 'symmetric_mean_absolute_percentage_error']\n",
      " |      For more information on these options, see `sklearn.metrics`: https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
      " |      For metric source code, see `autogluon.core.metrics`.\n",
      " |  \n",
      " |      You can also pass your own evaluation function here as long as it follows formatting of the functions defined in folder `autogluon.core.metrics`.\n",
      " |      For detailed instructions on creating and using a custom metric, refer to https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-custom-metric.html\n",
      " |  path : Union[str, pathlib.Path], default = None\n",
      " |      Path to directory where models and intermediate outputs should be saved.\n",
      " |      If unspecified, a time-stamped folder called \"AutogluonModels/ag-[TIMESTAMP]\" will be created in the working directory to store all models.\n",
      " |      Note: To call `fit()` twice and save all results of each fit, you must specify different `path` locations or don't specify `path` at all.\n",
      " |      Otherwise files from first `fit()` will be overwritten by second `fit()`.\n",
      " |  verbosity : int, default = 2\n",
      " |      Verbosity levels range from 0 to 4 and control how much information is printed.\n",
      " |      Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n",
      " |      If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`,\n",
      " |      where `L` ranges from 0 to 50 (Note: higher values of `L` correspond to fewer print statements, opposite of verbosity levels).\n",
      " |      Verbosity levels:\n",
      " |          0: Only log exceptions\n",
      " |          1: Only log warnings + exceptions\n",
      " |          2: Standard logging\n",
      " |          3: Verbose logging (ex: log validation score every 50 iterations)\n",
      " |          4: Maximally verbose logging (ex: log validation score every iteration)\n",
      " |  log_to_file: bool, default = False\n",
      " |      Whether to save the logs into a file for later reference\n",
      " |  log_file_path: str, default = \"auto\"\n",
      " |      File path to save the logs.\n",
      " |      If auto, logs will be saved under `predictor_path/logs/predictor_log.txt`.\n",
      " |      Will be ignored if `log_to_file` is set to False\n",
      " |  sample_weight : str, default = None\n",
      " |      If specified, this column-name indicates which column of the data should be treated as sample weights. This column will NOT be considered as a predictive feature.\n",
      " |      Sample weights should be non-negative (and cannot be nan), with larger values indicating which rows are more important than others.\n",
      " |      If you want your usage of sample weights to match results obtained outside of this Predictor, then ensure sample weights for your training (or tuning) data sum to the number of rows in the training (or tuning) data.\n",
      " |      You may also specify two special strings: 'auto_weight' (automatically choose a weighting strategy based on the data) or 'balance_weight' (equally weight classes in classification, no effect in regression). If specifying your own sample_weight column, make sure its name does not match these special strings.\n",
      " |  weight_evaluation : bool, default = False\n",
      " |      Only considered when `sample_weight` column is not None. Determines whether sample weights should be taken into account when computing evaluation metrics on validation/test data.\n",
      " |      If True, then weighted metrics will be reported based on the sample weights provided in the specified `sample_weight` (in which case `sample_weight` column must also be present in test data).\n",
      " |      In this case, the 'best' model used by default for prediction will also be decided based on a weighted version of evaluation metric.\n",
      " |      Note: we do not recommend specifying `weight_evaluation` when `sample_weight` is 'auto_weight' or 'balance_weight', instead specify appropriate `eval_metric`.\n",
      " |  groups : str, default = None\n",
      " |      [Experimental] If specified, AutoGluon will use the column named the value of groups in `train_data` during `.fit` as the data splitting indices for the purposes of bagging.\n",
      " |      This column will not be used as a feature during model training.\n",
      " |      This parameter is ignored if bagging is not enabled. To instead specify a custom validation set with bagging disabled, specify `tuning_data` in `.fit`.\n",
      " |      The data will be split via `sklearn.model_selection.LeaveOneGroupOut`.\n",
      " |      Use this option to control the exact split indices AutoGluon uses.\n",
      " |      It is not recommended to use this option unless it is required for very specific situations.\n",
      " |      Bugs may arise from edge cases if the provided groups are not valid to properly train models, such as if not all classes are present during training in multiclass classification. It is up to the user to sanitize their groups.\n",
      " |  \n",
      " |      As an example, if you want your data folds to preserve adjacent rows in the table without shuffling, then for 3 fold bagging with 6 rows of data, the groups column values should be [0, 0, 1, 1, 2, 2].\n",
      " |  positive_class : str or int, default = None\n",
      " |      Used to determine the positive class in binary classification.\n",
      " |      This is used for certain metrics such as 'f1' which produce different scores depending on which class is considered the positive class.\n",
      " |      If not set, will be inferred as the second element of the existing unique classes after sorting them.\n",
      " |          If classes are [0, 1], then 1 will be selected as the positive class.\n",
      " |          If classes are ['def', 'abc'], then 'def' will be selected as the positive class.\n",
      " |          If classes are [True, False], then True will be selected as the positive class.\n",
      " |  **kwargs :\n",
      " |      learner_type : AbstractLearner, default = DefaultLearner\n",
      " |          A class which inherits from `AbstractLearner`. This dictates the inner logic of predictor.\n",
      " |          If you don't know what this is, keep it as the default.\n",
      " |      learner_kwargs : dict, default = None\n",
      " |          Kwargs to send to the learner. Options include:\n",
      " |  \n",
      " |          ignored_columns : list, default = None\n",
      " |              Banned subset of column names that predictor may not use as predictive features (e.g. unique identifier to a row or user-ID).\n",
      " |              These columns are ignored during `fit()`.\n",
      " |          label_count_threshold : int, default = 10\n",
      " |              For multi-class classification problems, this is the minimum number of times a label must appear in dataset in order to be considered an output class.\n",
      " |              AutoGluon will ignore any classes whose labels do not appear at least this many times in the dataset (i.e. will never predict them).\n",
      " |          cache_data : bool, default = True\n",
      " |              When enabled, the training and validation data are saved to disk for future reuse.\n",
      " |              Enables advanced functionality in predictor such as `fit_extra()` and feature importance calculation on the original data.\n",
      " |          trainer_type : AbstractTrainer, default = AutoTrainer\n",
      " |              A class inheriting from `AbstractTrainer` that controls training/ensembling of many models.\n",
      " |              If you don't know what this is, keep it as the default.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TabularPredictor\n",
      " |      autogluon.tabular.predictor._deprecated_methods.TabularPredictorDeprecatedMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, label: 'str', problem_type: 'str' = None, eval_metric: 'str | Scorer' = None, path: 'str' = None, verbosity: 'int' = 2, log_to_file: 'bool' = False, log_file_path: 'str' = 'auto', sample_weight: 'str' = None, weight_evaluation: 'bool' = False, groups: 'str' = None, positive_class: 'int | str | None' = None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  calibrate_decision_threshold(self, data: 'pd.DataFrame | str | None' = None, metric: 'str | Scorer | None' = None, model: 'str' = 'best', decision_thresholds: 'int | List[float]' = 25, secondary_decision_thresholds: 'int | None' = 19, subsample_size: 'int | None' = 1000000, verbose: 'bool' = True) -> 'float'\n",
      " |      Calibrate the decision threshold in binary classification to optimize a given metric.\n",
      " |      You can pass the output of this method as input to `predictor.set_decision_threshold` to update the predictor.\n",
      " |      Will raise an AssertionError if `predictor.problem_type != 'binary'`.\n",
      " |      \n",
      " |      Note that while calibrating the decision threshold can help to improve a given metric,\n",
      " |      other metrics may end up having worse scores.\n",
      " |      For example, calibrating on `balanced_accuracy` will often harm `accuracy`.\n",
      " |      Users should keep this in mind while leveraging decision threshold calibration.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : pd.DataFrame or str, optional\n",
      " |          The data to use for calibration. Must contain the label column.\n",
      " |          We recommend to keep this value as None unless you are an advanced user and understand the implications.\n",
      " |          If None, will use internal data such as the holdout validation data or out-of-fold predictions.\n",
      " |      metric : autogluon.core.metrics.Scorer or str, default = None\n",
      " |          The metric to optimize during calibration.\n",
      " |          If None, uses `predictor.eval_metric`.\n",
      " |      model : str, default = 'best'\n",
      " |          The model to use prediction probabilities of when calibrating the threshold.\n",
      " |          If 'best', will use `predictor.model_best`.\n",
      " |      decision_thresholds : int | List[float], default = 25\n",
      " |          The number of decision thresholds on either side of `0.5` to search.\n",
      " |          The default of 25 will result in 51 searched thresholds: [0.00, 0.02, 0.04, ..., 0.48, 0.50, 0.52, ..., 0.96, 0.98, 1.00]\n",
      " |          Alternatively, a list of decision thresholds can be passed and only the thresholds in the list will be searched.\n",
      " |      secondary_decision_thresholds : int | None, default = 19\n",
      " |          The number of secondary decision thresholds to check on either side of the threshold identified in the first phase.\n",
      " |          Skipped if None.\n",
      " |          For example, if decision_thresholds=50 and 0.14 was identified as the optimal threshold, while secondary_decision_threshold=9,\n",
      " |              Then the following additional thresholds are checked:\n",
      " |                  [0.131, 0.132, 0.133, 0.134, 0.135, 0.136, 0.137, 0.138, 0.139, 0.141, 0.142, 0.143, 0.144, 0.145, 0.146, 0.147, 0.148, 0.149]\n",
      " |      subsample_size : int | None, default = 1000000\n",
      " |          When `subsample_size` is not None and `data` contains more rows than `subsample_size`, samples to `subsample_size` rows to speed up calibration.\n",
      " |          Usually it is not necessary to use more than 1 million rows for calibration.\n",
      " |      verbose : bool, default = True\n",
      " |          If True, will log information about the calibration process.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Decision Threshold: A float between 0 and 1 defining the decision boundary for predictions that\n",
      " |      maximizes the `metric` score on the `data` for the `model`.\n",
      " |  \n",
      " |  clone(self, path: 'str', *, return_clone: 'bool' = False, dirs_exist_ok: 'bool' = False) -> \"str | 'TabularPredictor'\"\n",
      " |      Clone the predictor and all of its artifacts to a new location on local disk.\n",
      " |      This is ideal for use-cases where saving a snapshot of the predictor is desired before performing\n",
      " |      more advanced operations (such as fit_extra and refit_full).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Directory path the cloned predictor will be saved to.\n",
      " |      return_clone : bool, default = False\n",
      " |          If True, returns the loaded cloned TabularPredictor object.\n",
      " |          If False, returns the local path to the cloned TabularPredictor object.\n",
      " |      dirs_exist_ok : bool, default = False\n",
      " |          If True, will clone the predictor even if the path directory already exists, potentially overwriting unrelated files.\n",
      " |          If False, will raise an exception if the path directory already exists and avoid performing the copy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If return_clone == True, returns the loaded cloned TabularPredictor object.\n",
      " |      If return_clone == False, returns the local path to the cloned TabularPredictor object.\n",
      " |  \n",
      " |  clone_for_deployment(self, path: 'str', *, model: 'str' = 'best', return_clone: 'bool' = False, dirs_exist_ok: 'bool' = False) -> \"str | 'TabularPredictor'\"\n",
      " |      Clone the predictor and all of its artifacts to a new location on local disk,\n",
      " |      then delete the clones artifacts unnecessary during prediction.\n",
      " |      This is ideal for use-cases where saving a snapshot of the predictor is desired before performing\n",
      " |      more advanced operations (such as fit_extra and refit_full).\n",
      " |      \n",
      " |      Note that the clone can no longer fit new models,\n",
      " |      and most functionality except for predict and predict_proba will no longer work.\n",
      " |      \n",
      " |      Identical to performing the following operations in order:\n",
      " |      \n",
      " |      predictor_clone = predictor.clone(path=path, return_clone=True, dirs_exist_ok=dirs_exist_ok)\n",
      " |      predictor_clone.delete_models(models_to_keep=model, dry_run=False)\n",
      " |      predictor_clone.set_model_best(model=model, save_trainer=True)\n",
      " |      predictor_clone.save_space()\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Directory path the cloned predictor will be saved to.\n",
      " |      model : str, default = 'best'\n",
      " |          The model to use in the optimized predictor clone.\n",
      " |          All other unrelated models will be deleted to save disk space.\n",
      " |          Refer to the `models_to_keep` argument of `predictor.delete_models` for available options.\n",
      " |          Internally calls `predictor_clone.delete_models(models_to_keep=model, dry_run=False)`\n",
      " |      return_clone : bool, default = False\n",
      " |          If True, returns the loaded cloned TabularPredictor object.\n",
      " |          If False, returns the local path to the cloned TabularPredictor object.\n",
      " |      dirs_exist_ok : bool, default = False\n",
      " |          If True, will clone the predictor even if the path directory already exists, potentially overwriting unrelated files.\n",
      " |          If False, will raise an exception if the path directory already exists and avoids performing the copy.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If return_clone == True, returns the loaded cloned TabularPredictor object.\n",
      " |      If return_clone == False, returns the local path to the cloned TabularPredictor object.\n",
      " |  \n",
      " |  compile(self, models='best', with_ancestors=True, compiler_configs='auto')\n",
      " |      Compile models for accelerated prediction.\n",
      " |      This can be helpful to reduce prediction latency and improve throughput.\n",
      " |      \n",
      " |      Note that this is currently an experimental feature, the supported compilers can be ['native', 'onnx'].\n",
      " |      \n",
      " |      In order to compile with a specific compiler, that compiler must be installed in the Python environment.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      models : list of str or str, default = 'best'\n",
      " |          Model names of models to compile.\n",
      " |          If 'best' then the model with the highest validation score is compiled (this is the model used for prediction by default).\n",
      " |          If 'all' then all models are compiled.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |      with_ancestors : bool, default = True\n",
      " |          If True, all ancestor models of the provided models will also be compiled.\n",
      " |      compiler_configs : dict or str, default = \"auto\"\n",
      " |          If \"auto\", defaults to the following:\n",
      " |              compiler_configs = {\n",
      " |                  \"RF\": {\"compiler\": \"onnx\"},\n",
      " |                  \"XT\": {\"compiler\": \"onnx\"},\n",
      " |                  \"NN_TORCH\": {\"compiler\": \"onnx\"},\n",
      " |              }\n",
      " |          Otherwise, specify a compiler_configs dictionary manually. Keys can be exact model names or model types.\n",
      " |          Exact model names take priority over types if both are valid for a model.\n",
      " |          Types can be either the true type such as RandomForestModel or the shorthand \"RF\".\n",
      " |          The dictionary key logic for types is identical to the logic in the hyperparameters argument of `predictor.fit`\n",
      " |      \n",
      " |          Example values within the configs:\n",
      " |              compiler : str, default = None\n",
      " |                  The compiler that is used for model compilation.\n",
      " |              batch_size : int, default = None\n",
      " |                  The batch size that is optimized for model prediction.\n",
      " |                  By default, the batch size is None. This means the compiler would try to leverage dynamic shape for prediction.\n",
      " |                  Using batch_size=1 would be more suitable for online prediction, which expects a result from one data point.\n",
      " |                  However, it can be slow for batch processing, because of the overhead of multiple kernel execution.\n",
      " |                  Increasing batch size to a number that is larger than 1 would help increase the prediction throughput.\n",
      " |                  This comes with an expense of utilizing larger memory for prediction.\n",
      " |  \n",
      " |  delete_models(self, models_to_keep=None, models_to_delete=None, allow_delete_cascade=False, delete_from_disk=True, dry_run=True)\n",
      " |      Deletes models from `predictor`.\n",
      " |      This can be helpful to minimize memory usage and disk usage, particularly for model deployment.\n",
      " |      This will remove all references to the models in `predictor`.\n",
      " |          For example, removed models will not appear in `predictor.leaderboard()`.\n",
      " |      WARNING: If `delete_from_disk=True`, this will DELETE ALL FILES in the deleted model directories, regardless if they were created by AutoGluon or not.\n",
      " |          DO NOT STORE FILES INSIDE OF THE MODEL DIRECTORY THAT ARE UNRELATED TO AUTOGLUON.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      models_to_keep : str or list, default = None\n",
      " |          Name of model or models to not delete.\n",
      " |          All models that are not specified and are also not required as a dependency of any model in `models_to_keep` will be deleted.\n",
      " |          Specify `models_to_keep='best'` to keep only the best model and its model dependencies.\n",
      " |          `models_to_delete` must be None if `models_to_keep` is set.\n",
      " |          To see the list of possible model names, use: `predictor.model_names()` or `predictor.leaderboard()`.\n",
      " |      models_to_delete : str or list, default = None\n",
      " |          Name of model or models to delete.\n",
      " |          All models that are not specified but depend on a model in `models_to_delete` will also be deleted.\n",
      " |          `models_to_keep` must be None if `models_to_delete` is set.\n",
      " |      allow_delete_cascade : bool, default = False\n",
      " |          If `False`, if unspecified dependent models of models in `models_to_delete` exist an exception will be raised instead of deletion occurring.\n",
      " |              An example of a dependent model is m1 if m2 is a stacker model and takes predictions from m1 as inputs. In this case, m1 would be a dependent model of m2.\n",
      " |          If `True`, all dependent models of models in `models_to_delete` will be deleted.\n",
      " |          Has no effect if `models_to_delete=None`.\n",
      " |      delete_from_disk : bool, default = True\n",
      " |          If `True`, deletes the models from disk if they were persisted.\n",
      " |          WARNING: This deletes the entire directory for the deleted models, and ALL FILES located there.\n",
      " |              It is highly recommended to first run with `dry_run=True` to understand which directories will be deleted.\n",
      " |      dry_run : bool, default = True\n",
      " |          If `True`, then deletions don't occur, and logging statements are printed describing what would have occurred.\n",
      " |          Set `dry_run=False` to perform the deletions.\n",
      " |  \n",
      " |  disk_usage(self) -> 'int'\n",
      " |      Returns the combined size of all files under the `predictor.path` directory in bytes.\n",
      " |  \n",
      " |  disk_usage_per_file(self, *, sort_by: 'str' = 'size', include_path_in_name: 'bool' = False) -> 'pd.Series'\n",
      " |      Returns the size of each file under the `predictor.path` directory in bytes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sort_by : str, default = \"size\"\n",
      " |          If None, output files will be ordered based on order of search in os.walk(path).\n",
      " |          If \"size\", output files will be ordered in descending order of file size.\n",
      " |          If \"name\", output files will be ordered by name in ascending alphabetical order.\n",
      " |      include_path_in_name : bool, default = False\n",
      " |          If True, includes the full path of the file including the input `path` as part of the index in the output pd.Series.\n",
      " |          If False, removes the `path` prefix of the file path in the index of the output pd.Series.\n",
      " |      \n",
      " |          For example, for a file located at `foo/bar/model.pkl`, with path='foo/'\n",
      " |              If True, index will be `foo/bar/model.pkl`\n",
      " |              If False, index will be `bar/model.pkl`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pd.Series with index file path and value file size in bytes.\n",
      " |  \n",
      " |  distill(self, train_data: 'pd.DataFrame | str' = None, tuning_data: 'pd.DataFrame | str' = None, augmentation_data: 'pd.DataFrame' = None, time_limit: 'float' = None, hyperparameters: 'dict | str' = None, holdout_frac: 'float' = None, teacher_preds: 'str' = 'soft', augment_method: 'str' = 'spunge', augment_args: 'dict' = {'size_factor': 5, 'max_size': 100000}, models_name_suffix: 'str' = None, verbosity: 'int' = None)\n",
      " |      [EXPERIMENTAL]\n",
      " |      Distill AutoGluon's most accurate ensemble-predictor into single models which are simpler/faster and require less memory/compute.\n",
      " |      Distillation can produce a model that is more accurate than the same model fit directly on the original training data.\n",
      " |      After calling `distill()`, there will be more models available in this Predictor, which can be evaluated using `predictor.leaderboard(test_data)` and deployed with: `predictor.predict(test_data, model=MODEL_NAME)`.\n",
      " |      This will raise an exception if `cache_data=False` was previously set in `fit()`.\n",
      " |      \n",
      " |      NOTE: Until catboost v0.24 is released, `distill()` with CatBoost students in multiclass classification requires you to first install catboost-dev: `pip install catboost-dev`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_data : str or :class:`pd.DataFrame`, default = None\n",
      " |          Same as `train_data` argument of `fit()`.\n",
      " |          If None, the same training data will be loaded from `fit()` call used to produce this Predictor.\n",
      " |      tuning_data : str or :class:`pd.DataFrame`, default = None\n",
      " |          Same as `tuning_data` argument of `fit()`.\n",
      " |          If `tuning_data = None` and `train_data = None`: the same training/validation splits will be loaded from `fit()` call used to produce this Predictor,\n",
      " |          unless bagging/stacking was previously used in which case a new training/validation split is performed.\n",
      " |      augmentation_data : :class:`pd.DataFrame`, default = None\n",
      " |          An optional extra dataset of unlabeled rows that can be used for augmenting the dataset used to fit student models during distillation (ignored if None).\n",
      " |      time_limit : int, default = None\n",
      " |          Approximately how long (in seconds) the distillation process should run for.\n",
      " |          If None, no time-constraint will be enforced allowing the distilled models to fully train.\n",
      " |      hyperparameters : dict or str, default = None\n",
      " |          Specifies which models to use as students and what hyperparameter-values to use for them.\n",
      " |          Same as `hyperparameters` argument of `fit()`.\n",
      " |          If = None, then student models will use the same hyperparameters from `fit()` used to produce this Predictor.\n",
      " |          Note: distillation is currently only supported for ['GBM','NN_TORCH','RF','CAT'] student models, other models and their hyperparameters are ignored here.\n",
      " |      holdout_frac : float\n",
      " |          Same as `holdout_frac` argument of :meth:`TabularPredictor.fit`.\n",
      " |      teacher_preds : str, default = 'soft'\n",
      " |          What form of teacher predictions to distill from (teacher refers to the most accurate AutoGluon ensemble-predictor).\n",
      " |          If None, we only train with original labels (no data augmentation).\n",
      " |          If 'hard', labels are hard teacher predictions given by: `teacher.predict()`\n",
      " |          If 'soft', labels are soft teacher predictions given by: `teacher.predict_proba()`\n",
      " |          Note: 'hard' and 'soft' are equivalent for regression problems.\n",
      " |          If `augment_method` is not None, teacher predictions are only used to label augmented data (training data keeps original labels).\n",
      " |          To apply label-smoothing: `teacher_preds='onehot'` will use original training data labels converted to one-hot vectors for multiclass problems (no data augmentation).\n",
      " |      augment_method : str, default='spunge'\n",
      " |          Specifies method to use for generating augmented data for distilling student models.\n",
      " |          Options include:\n",
      " |              None : no data augmentation performed.\n",
      " |              'munge' : The MUNGE algorithm (https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf).\n",
      " |              'spunge' : A simpler, more efficient variant of the MUNGE algorithm.\n",
      " |      augment_args : dict, default = {'size_factor':5, 'max_size': int(1e5)}\n",
      " |          Contains the following kwargs that control the chosen `augment_method` (these are ignored if `augment_method=None`):\n",
      " |              'num_augmented_samples': int, number of augmented datapoints used during distillation. Overrides 'size_factor', 'max_size' if specified.\n",
      " |              'max_size': float, the maximum number of augmented datapoints to add (ignored if 'num_augmented_samples' specified).\n",
      " |              'size_factor': float, if n = training data sample-size, we add int(n * size_factor) augmented datapoints, up to 'max_size'.\n",
      " |              Larger values in `augment_args` will slow down the runtime of distill(), and may produce worse results if provided time_limit are too small.\n",
      " |              You can also pass in kwargs for the `spunge_augment`, `munge_augment` functions in `autogluon.tabular.augmentation.distill_utils`.\n",
      " |      models_name_suffix : str, default = None\n",
      " |          Optional suffix that can be appended at the end of all distilled student models' names.\n",
      " |          Note: all distilled models will contain '_DSTL' substring in their name by default.\n",
      " |      verbosity : int, default = None\n",
      " |          Controls amount of printed output during distillation (4 = highest, 0 = lowest).\n",
      " |          Same as `verbosity` parameter of :class:`TabularPredictor`.\n",
      " |          If None, the same `verbosity` used in previous fit is employed again.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of names (str) corresponding to the distilled models.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from autogluon.tabular import TabularDataset, TabularPredictor\n",
      " |      >>> train_data = TabularDataset('train.csv')\n",
      " |      >>> predictor = TabularPredictor(label='class').fit(train_data, auto_stack=True)\n",
      " |      >>> distilled_model_names = predictor.distill()\n",
      " |      >>> test_data = TabularDataset('test.csv')\n",
      " |      >>> ldr = predictor.leaderboard(test_data)\n",
      " |      >>> model_to_deploy = distilled_model_names[0]\n",
      " |      >>> predictor.predict(test_data, model=model_to_deploy)\n",
      " |  \n",
      " |  evaluate(self, data: 'pd.DataFrame | str', model: 'str' = None, decision_threshold: 'float' = None, display: 'bool' = False, auxiliary_metrics: 'bool' = True, detailed_report: 'bool' = False, **kwargs) -> 'dict'\n",
      " |      Report the predictive performance evaluated over a given dataset.\n",
      " |      This is basically a shortcut for: `pred_proba = predict_proba(data); evaluate_predictions(data[label], pred_proba)`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : str or :class:`pd.DataFrame`\n",
      " |          This dataset must also contain the `label` with the same column-name as previously specified.\n",
      " |          If str is passed, `data` will be loaded using the str value as the file path.\n",
      " |          If `self.sample_weight` is set and `self.weight_evaluation==True`, then a column with the sample weight name is checked and used for weighted metric evaluation if it exists.\n",
      " |      model : str (optional)\n",
      " |          The name of the model to get prediction probabilities from. Defaults to None, which uses the highest scoring model on the validation set.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |      decision_threshold : float, default = None\n",
      " |          The decision threshold to use when converting prediction probabilities to predictions.\n",
      " |          This will impact the scores of metrics such as `f1` and `accuracy`.\n",
      " |          If None, defaults to `predictor.decision_threshold`. Ignored unless `problem_type='binary'`.\n",
      " |          Refer to the `predictor.decision_threshold` docstring for more information.\n",
      " |      display : bool, default = False\n",
      " |          If True, performance results are printed.\n",
      " |      auxiliary_metrics: bool, default = True\n",
      " |          Should we compute other (`problem_type` specific) metrics in addition to the default metric?\n",
      " |      detailed_report : bool, default = False\n",
      " |          Should we computed more detailed versions of the `auxiliary_metrics`? (requires `auxiliary_metrics = True`)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Returns dict where keys = metrics, values = performance along each metric. To get the `eval_metric` score, do `output[predictor.eval_metric.name]`\n",
      " |      NOTE: Metrics scores always show in higher is better form.\n",
      " |      This means that metrics such as log_loss and root_mean_squared_error will have their signs FLIPPED, and values will be negative.\n",
      " |  \n",
      " |  evaluate_predictions(self, y_true, y_pred, sample_weight=None, decision_threshold=None, display: 'bool' = False, auxiliary_metrics=True, detailed_report=False, **kwargs) -> 'dict'\n",
      " |      Evaluate the provided prediction probabilities against ground truth labels.\n",
      " |      Evaluation is based on the `eval_metric` previously specified in init, or default metrics if none was specified.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y_true : :class:`np.array` or :class:`pd.Series`\n",
      " |          The ordered collection of ground-truth labels.\n",
      " |      y_pred : :class:`pd.Series` or :class:`pd.DataFrame`\n",
      " |          The ordered collection of prediction probabilities or predictions.\n",
      " |          Obtainable via the output of `predictor.predict_proba`.\n",
      " |          Caution: For certain types of `eval_metric` (such as 'roc_auc'), `y_pred` must be predicted-probabilities rather than predicted labels.\n",
      " |      sample_weight : :class:`pd.Series`, default = None\n",
      " |          Sample weight for each row of data. If None, uniform sample weights are used.\n",
      " |      decision_threshold : float, default = None\n",
      " |          The decision threshold to use when converting prediction probabilities to predictions.\n",
      " |          This will impact the scores of metrics such as `f1` and `accuracy`.\n",
      " |          If None, defaults to `predictor.decision_threshold`. Ignored unless `problem_type='binary'`.\n",
      " |          Refer to the `predictor.decision_threshold` docstring for more information.\n",
      " |      display : bool, default = False\n",
      " |          If True, performance results are printed.\n",
      " |      auxiliary_metrics: bool, default = True\n",
      " |          Should we compute other (`problem_type` specific) metrics in addition to the default metric?\n",
      " |      detailed_report : bool, default = False\n",
      " |          Should we computed more detailed versions of the `auxiliary_metrics`? (requires `auxiliary_metrics = True`)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Returns dict where keys = metrics, values = performance along each metric.\n",
      " |      NOTE: Metrics scores always show in higher is better form.\n",
      " |      This means that metrics such as log_loss and root_mean_squared_error will have their signs FLIPPED, and values will be negative.\n",
      " |  \n",
      " |  feature_importance(self, data=None, model: 'str' = None, features: 'list' = None, feature_stage: 'str' = 'original', subsample_size: 'int' = 5000, time_limit: 'float' = None, num_shuffle_sets: 'int' = None, include_confidence_band: 'bool' = True, confidence_level: 'float' = 0.99, silent: 'bool' = False)\n",
      " |      Calculates feature importance scores for the given model via permutation importance. Refer to https://explained.ai/rf-importance/ for an explanation of permutation importance.\n",
      " |      A feature's importance score represents the performance drop that results when the model makes predictions on a perturbed copy of the data where this feature's values have been randomly shuffled across rows.\n",
      " |      A feature score of 0.01 would indicate that the predictive performance dropped by 0.01 when the feature was randomly shuffled.\n",
      " |      The higher the score a feature has, the more important it is to the model's performance.\n",
      " |      If a feature has a negative score, this means that the feature is likely harmful to the final model, and a model trained with the feature removed would be expected to achieve a better predictive performance.\n",
      " |      Note that calculating feature importance can be a very computationally expensive process, particularly if the model uses hundreds or thousands of features. In many cases, this can take longer than the original model training.\n",
      " |      To estimate how long `feature_importance(model, data, features)` will take, it is roughly the time taken by `predict_proba(data, model)` multiplied by the number of features.\n",
      " |      \n",
      " |      Note: For highly accurate importance and p_value estimates, it is recommended to set `subsample_size` to at least 5000 if possible and `num_shuffle_sets` to at least 10.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : str or :class:`pd.DataFrame` (optional)\n",
      " |          This data must also contain the label-column with the same column-name as specified during `fit()`.\n",
      " |          If specified, then the data is used to calculate the feature importance scores.\n",
      " |          If str is passed, `data` will be loaded using the str value as the file path.\n",
      " |          If not specified, the original data used during `fit()` will be used if `cache_data=True`. Otherwise, an exception will be raised.\n",
      " |          Do not pass the training data through this argument, as the feature importance scores calculated will be biased due to overfitting.\n",
      " |              More accurate feature importances will be obtained from new data that was held-out during `fit()`.\n",
      " |      model : str, default = None\n",
      " |          Model to get feature importances for, if None the best model is chosen.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`\n",
      " |      features : list, default = None\n",
      " |          List of str feature names that feature importances are calculated for and returned, specify None to get all feature importances.\n",
      " |          If you only want to compute feature importances for some of the features, you can pass their names in as a list of str.\n",
      " |          Valid feature names change depending on the `feature_stage`.\n",
      " |              To get the list of feature names for `feature_stage='original'`, call `predictor.feature_metadata_in.get_features()`.\n",
      " |              To get the list of feature names for `feature_stage='transformed'`, call `list(predictor.transform_features().columns)`.\n",
      " |              To get the list of feature names for `feature_stage=`transformed_model`, call `list(predictor.transform_features(model={model_name}).columns)`.\n",
      " |          [Advanced] Can also contain tuples as elements of (feature_name, feature_list) form.\n",
      " |              feature_name can be any string so long as it is unique with all other feature names / features in the list.\n",
      " |              feature_list can be any list of valid features in the data.\n",
      " |              This will compute importance of the combination of features in feature_list, naming the set of features in the returned DataFrame feature_name.\n",
      " |              This importance will differ from adding the individual importances of each feature in feature_list, and will be more accurate to the overall group importance.\n",
      " |              Example: ['featA', 'featB', 'featC', ('featBC', ['featB', 'featC'])]\n",
      " |              In this example, the importance of 'featBC' will be calculated by jointly permuting 'featB' and 'featC' together as if they were a single two-dimensional feature.\n",
      " |      feature_stage : str, default = 'original'\n",
      " |          What stage of feature-processing should importances be computed for.\n",
      " |          Options:\n",
      " |              'original':\n",
      " |                  Compute importances of the original features.\n",
      " |                  Warning: `data` must be specified with this option, otherwise an exception will be raised.\n",
      " |              'transformed':\n",
      " |                  Compute importances of the post-internal-transformation features (after automated feature engineering). These features may be missing some original features, or add new features entirely.\n",
      " |                  An example of new features would be ngram features generated from a text column.\n",
      " |                  Warning: For bagged models, feature importance calculation is not yet supported with this option when `data=None`. Doing so will raise an exception.\n",
      " |              'transformed_model':\n",
      " |                  Compute importances of the post-model-transformation features. These features are the internal features used by the requested model. They may differ greatly from the original features.\n",
      " |                  If the model is a stack ensemble, this will include stack ensemble features such as the prediction probability features of the stack ensemble's base (ancestor) models.\n",
      " |      subsample_size : int, default = 5000\n",
      " |          The number of rows to sample from `data` when computing feature importance.\n",
      " |          If `subsample_size=None` or `data` contains fewer than `subsample_size` rows, all rows will be used during computation.\n",
      " |          Larger values increase the accuracy of the feature importance scores.\n",
      " |          Runtime linearly scales with `subsample_size`.\n",
      " |      time_limit : float, default = None\n",
      " |          Time in seconds to limit the calculation of feature importance.\n",
      " |          If None, feature importance will calculate without early stopping.\n",
      " |          A minimum of 1 full shuffle set will always be evaluated. If a shuffle set evaluation takes longer than `time_limit`, the method will take the length of a shuffle set evaluation to return regardless of the `time_limit`.\n",
      " |      num_shuffle_sets : int, default = None\n",
      " |          The number of different permutation shuffles of the data that are evaluated.\n",
      " |          Larger values will increase the quality of the importance evaluation.\n",
      " |          It is generally recommended to increase `subsample_size` before increasing `num_shuffle_sets`.\n",
      " |          Defaults to 5 if `time_limit` is None or 10 if `time_limit` is specified.\n",
      " |          Runtime linearly scales with `num_shuffle_sets`.\n",
      " |      include_confidence_band: bool, default = True\n",
      " |          If True, returned DataFrame will include two additional columns specifying confidence interval for the true underlying importance value of each feature.\n",
      " |          Increasing `subsample_size` and `num_shuffle_sets` will tighten the confidence interval.\n",
      " |      confidence_level: float, default = 0.99\n",
      " |          This argument is only considered when `include_confidence_band` is True, and can be used to specify the confidence level used for constructing confidence intervals.\n",
      " |          For example, if `confidence_level` is set to 0.99, then the returned DataFrame will include columns 'p99_high' and 'p99_low' which indicates that the true feature importance will be between 'p99_high' and 'p99_low' 99% of the time (99% confidence interval).\n",
      " |          More generally, if `confidence_level` = 0.XX, then the columns containing the XX% confidence interval will be named 'pXX_high' and 'pXX_low'.\n",
      " |      silent : bool, default = False\n",
      " |          Whether to suppress logging output.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pd.DataFrame` of feature importance scores with 6 columns:\n",
      " |          index: The feature name.\n",
      " |          'importance': The estimated feature importance score.\n",
      " |          'stddev': The standard deviation of the feature importance score. If NaN, then not enough num_shuffle_sets were used to calculate a variance.\n",
      " |          'p_value': P-value for a statistical t-test of the null hypothesis: importance = 0, vs the (one-sided) alternative: importance > 0.\n",
      " |              Features with low p-value appear confidently useful to the predictor, while the other features may be useless to the predictor (or even harmful to include in its training data).\n",
      " |              A p-value of 0.01 indicates that there is a 1% chance that the feature is useless or harmful, and a 99% chance that the feature is useful.\n",
      " |              A p-value of 0.99 indicates that there is a 99% chance that the feature is useless or harmful, and a 1% chance that the feature is useful.\n",
      " |          'n': The number of shuffles performed to estimate importance score (corresponds to sample-size used to determine confidence interval for true score).\n",
      " |          'pXX_high': Upper end of XX% confidence interval for true feature importance score (where XX=99 by default).\n",
      " |          'pXX_low': Lower end of XX% confidence interval for true feature importance score.\n",
      " |  \n",
      " |  features(self, feature_stage: 'str' = 'original') -> 'list'\n",
      " |      Returns a list of feature names dependent on the value of feature_stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature_stage : str, default = 'original'\n",
      " |          If 'original', returns the list of features specified in the original training data. This feature set is required in input data when making predictions.\n",
      " |          If 'transformed', returns the list of features after pre-processing by the feature generator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Returns a list of feature names\n",
      " |  \n",
      " |  fit(self, train_data: 'pd.DataFrame | str', tuning_data: 'pd.DataFrame | str' = None, time_limit: 'float' = None, presets: 'List[str] | str' = None, hyperparameters: 'dict | str' = None, feature_metadata='infer', infer_limit: 'float' = None, infer_limit_batch_size: 'int' = None, fit_weighted_ensemble: 'bool' = True, fit_full_last_level_weighted_ensemble: 'bool' = True, full_weighted_ensemble_additionally: 'bool' = False, dynamic_stacking: 'bool | str' = False, calibrate_decision_threshold: 'bool | str' = 'auto', num_cpus: 'int | str' = 'auto', num_gpus: 'int | str' = 'auto', fit_strategy: \"Literal['sequential', 'parallel']\" = 'sequential', memory_limit: 'float | str' = 'auto', callbacks: 'List[AbstractCallback]' = None, **kwargs) -> \"'TabularPredictor'\"\n",
      " |      Fit models to predict a column of a data table (label) based on the other columns (features).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_data : :class:`pd.DataFrame` or str\n",
      " |          Table of the training data as a pandas DataFrame.\n",
      " |          If str is passed, `train_data` will be loaded using the str value as the file path.\n",
      " |      tuning_data : :class:`pd.DataFrame` or str, optional\n",
      " |          Another dataset containing validation data reserved for tuning processes such as early stopping and hyperparameter tuning.\n",
      " |          This dataset should be in the same format as `train_data`.\n",
      " |          If str is passed, `tuning_data` will be loaded using the str value as the file path.\n",
      " |          Note: final model returned may be fit on `tuning_data` as well as `train_data`. Do not provide your evaluation test data here!\n",
      " |          In particular, when `num_bag_folds` > 0 or `num_stack_levels` > 0, models will be trained on both `tuning_data` and `train_data`.\n",
      " |          If `tuning_data = None`, `fit()` will automatically hold out some random validation examples from `train_data`.\n",
      " |      time_limit : int, default = None\n",
      " |          Approximately how long `fit()` should run for (wallclock time in seconds).\n",
      " |          If not specified, `fit()` will run until all models have completed training, but will not repeatedly bag models unless `num_bag_sets` is specified.\n",
      " |      presets : list or str or dict, default = ['medium_quality']\n",
      " |          List of preset configurations for various arguments in `fit()`. Can significantly impact predictive accuracy, memory-footprint, and inference latency of trained models, and various other properties of the returned `predictor`.\n",
      " |          It is recommended to specify presets and avoid specifying most other `fit()` arguments or model hyperparameters prior to becoming familiar with AutoGluon.\n",
      " |          As an example, to get the most accurate overall predictor (regardless of its efficiency), set `presets='best_quality'`.\n",
      " |          To get good quality with minimal disk usage, set `presets=['good_quality', 'optimize_for_deployment']`\n",
      " |          Any user-specified arguments in `fit()` will override the values used by presets.\n",
      " |          If specifying a list of presets, later presets will override earlier presets if they alter the same argument.\n",
      " |          For precise definitions of the provided presets, see file: `autogluon/tabular/configs/presets_configs.py`.\n",
      " |          Users can specify custom presets by passing in a dictionary of argument values as an element to the list.\n",
      " |      \n",
      " |          Available Presets: ['best_quality', 'high_quality', 'good_quality', 'medium_quality', 'experimental_quality', 'optimize_for_deployment', 'interpretable', 'ignore_text']\n",
      " |      \n",
      " |          It is recommended to only use one `quality` based preset in a given call to `fit()` as they alter many of the same arguments and are not compatible with each-other.\n",
      " |      \n",
      " |          In-depth Preset Info:\n",
      " |              best_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'zeroshot'}\n",
      " |                  Best predictive accuracy with little consideration to inference time or disk usage. Achieve even better results by specifying a large time_limit value.\n",
      " |                  Recommended for applications that benefit from the best possible model accuracy.\n",
      " |      \n",
      " |              high_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'zeroshot', 'refit_full': True, 'set_best_to_refit_full': True, 'save_bag_folds': False}\n",
      " |                  High predictive accuracy with fast inference. ~8x faster inference and ~8x lower disk usage than `best_quality`.\n",
      " |                  Recommended for applications that require reasonable inference speed and/or model size.\n",
      " |      \n",
      " |              good_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'light', 'refit_full': True, 'set_best_to_refit_full': True, 'save_bag_folds': False}\n",
      " |                  Good predictive accuracy with very fast inference. ~4x faster inference and ~4x lower disk usage than `high_quality`.\n",
      " |                  Recommended for applications that require fast inference speed.\n",
      " |      \n",
      " |              medium_quality={'auto_stack': False}\n",
      " |                  Medium predictive accuracy with very fast inference and very fast training time. ~20x faster training than `good_quality`.\n",
      " |                  This is the default preset in AutoGluon, but should generally only be used for quick prototyping, as `good_quality` results in significantly better predictive accuracy and faster inference time.\n",
      " |      \n",
      " |              experimental_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'experimental', 'fit_strategy': 'parallel', 'num_gpus': 0}\n",
      " |                  This preset acts as a testing ground for cutting edge features and models which could later be added to the `best_quality` preset in future releases.\n",
      " |                  Recommended when `best_quality` was already being used and the user wants to push performance even further.\n",
      " |      \n",
      " |              optimize_for_deployment={'keep_only_best': True, 'save_space': True}\n",
      " |                  Optimizes result immediately for deployment by deleting unused models and removing training artifacts.\n",
      " |                  Often can reduce disk usage by ~2-4x with no negatives to model accuracy or inference speed.\n",
      " |                  This will disable numerous advanced functionality, but has no impact on inference.\n",
      " |                  This will make certain functionality less informative, such as `predictor.leaderboard()` and `predictor.fit_summary()`.\n",
      " |                      Because unused models will be deleted under this preset, methods like `predictor.leaderboard()` and `predictor.fit_summary()` will no longer show the full set of models that were trained during `fit()`.\n",
      " |                  Recommended for applications where the inner details of AutoGluon's training is not important and there is no intention of manually choosing between the final models.\n",
      " |                  This preset pairs well with the other presets such as `good_quality` to make a very compact final model.\n",
      " |                  Identical to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` and `predictor.save_space()` directly after `fit()`.\n",
      " |      \n",
      " |              interpretable={'auto_stack': False, 'hyperparameters': 'interpretable'}\n",
      " |                  Fits only interpretable rule-based models from the imodels package.\n",
      " |                  Trades off predictive accuracy for conciseness.\n",
      " |      \n",
      " |              ignore_text={'_feature_generator_kwargs': {'enable_text_ngram_features': False, 'enable_text_special_features': False, 'enable_raw_text_features': False}}\n",
      " |                  Disables automated feature generation when text features are detected.\n",
      " |                  This is useful to determine how beneficial text features are to the end result, as well as to ensure features are not mistaken for text when they are not.\n",
      " |                  Ignored if `feature_generator` was also specified.\n",
      " |      \n",
      " |      hyperparameters : str or dict, default = 'default'\n",
      " |          Determines the hyperparameters used by the models.\n",
      " |          If `str` is passed, will use a preset hyperparameter configuration.\n",
      " |              Valid `str` options: ['default', 'zeroshot', 'light', 'very_light', 'toy', 'multimodal']\n",
      " |                  'default': Default AutoGluon hyperparameters intended to get strong accuracy with reasonable disk usage and inference time. Used in the 'medium_quality' preset.\n",
      " |                  'zeroshot': A powerful model portfolio learned from TabRepo's ensemble simulation on 200 datasets. Contains ~100 models and is used in 'best_quality' and 'high_quality' presets.\n",
      " |                  'light': Results in smaller models. Generally will make inference speed much faster and disk usage much lower, but with worse accuracy. Used in the 'good_quality' preset.\n",
      " |                  'very_light': Results in much smaller models. Behaves similarly to 'light', but in many cases with over 10x less disk usage and a further reduction in accuracy.\n",
      " |                  'toy': Results in extremely small models. Only use this when prototyping, as the model quality will be severely reduced.\n",
      " |                  'multimodal': [EXPERIMENTAL] Trains a multimodal transformer model alongside tabular models. Requires that some text columns appear in the data and GPU.\n",
      " |                      When combined with 'best_quality' `presets` option, this can achieve extremely strong results in multimodal data tables that contain columns with text in addition to numeric/categorical columns.\n",
      " |              Reference `autogluon/tabular/configs/hyperparameter_configs.py` for information on the hyperparameters associated with each preset.\n",
      " |          Keys are strings that indicate which model types to train.\n",
      " |              Stable model options include:\n",
      " |                  'GBM' (LightGBM)\n",
      " |                  'CAT' (CatBoost)\n",
      " |                  'XGB' (XGBoost)\n",
      " |                  'RF' (random forest)\n",
      " |                  'XT' (extremely randomized trees)\n",
      " |                  'KNN' (k-nearest neighbors)\n",
      " |                  'LR' (linear regression)\n",
      " |                  'NN_TORCH' (neural network implemented in Pytorch)\n",
      " |                  'FASTAI' (neural network with FastAI backend)\n",
      " |                  'AG_AUTOMM' (`MultimodalPredictor` from `autogluon.multimodal`. Supports Tabular, Text, and Image modalities. GPU is required.)\n",
      " |              Experimental model options include:\n",
      " |                  'FT_TRANSFORMER' (Tabular Transformer, GPU is recommended. Does not scale well to >100 features.)\n",
      " |                  'FASTTEXT' (FastText. Note: Has not been tested for a long time.)\n",
      " |                  'TABPFN' (TabPFN. Does not scale well to >100 features or >1000 rows, and does not support regression. Extremely slow inference speed.)\n",
      " |                  'VW' (VowpalWabbit. Note: Has not been tested for a long time.)\n",
      " |                  'AG_TEXT_NN' (Multimodal Text+Tabular model, GPU is required. Recommended to instead use its successor, 'AG_AUTOMM'.)\n",
      " |                  'AG_IMAGE_NN' (Image model, GPU is required. Recommended to instead use its successor, 'AG_AUTOMM'.)\n",
      " |              If a certain key is missing from hyperparameters, then `fit()` will not train any models of that type. Omitting a model key from hyperparameters is equivalent to including this model key in `excluded_model_types`.\n",
      " |              For example, set `hyperparameters = { 'NN_TORCH':{...} }` if say you only want to train (PyTorch) neural networks and no other types of models.\n",
      " |          Values = dict of hyperparameter settings for each model type, or list of dicts.\n",
      " |              Each hyperparameter can either be a single fixed value or a search space containing many possible values.\n",
      " |              Unspecified hyperparameters will be set to default values (or default search spaces if `hyperparameter_tune_kwargs='auto'`).\n",
      " |              Caution: Any provided search spaces will error if `hyperparameter_tune_kwargs=None` (Default).\n",
      " |              To train multiple models of a given type, set the value to a list of hyperparameter dictionaries.\n",
      " |                  For example, `hyperparameters = {'RF': [{'criterion': 'gini'}, {'criterion': 'entropy'}]}` will result in 2 random forest models being trained with separate hyperparameters.\n",
      " |          Advanced functionality: Bring your own model / Custom model support\n",
      " |              AutoGluon fully supports custom models. For a detailed tutorial on creating and using custom models with AutoGluon, refer to https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-custom-model.html\n",
      " |          Advanced functionality: Custom stack levels\n",
      " |              By default, AutoGluon re-uses the same models and model hyperparameters at each level during stack ensembling.\n",
      " |              To customize this behaviour, create a hyperparameters dictionary separately for each stack level, and then add them as values to a new dictionary, with keys equal to the stack level.\n",
      " |                  Example: `hyperparameters = {1: {'RF': rf_params1}, 2: {'CAT': [cat_params1, cat_params2], 'NN_TORCH': {}}}`\n",
      " |                  This will result in a stack ensemble that has one custom random forest in level 1 followed by two CatBoost models with custom hyperparameters and a default neural network in level 2, for a total of 4 models.\n",
      " |              If a level is not specified in `hyperparameters`, it will default to using the highest specified level to train models. This can also be explicitly controlled by adding a 'default' key.\n",
      " |      \n",
      " |          Default:\n",
      " |              hyperparameters = {\n",
      " |                  'NN_TORCH': {},\n",
      " |                  'GBM': [\n",
      " |                      {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
      " |                      {},\n",
      " |                      {\n",
      " |                          \"learning_rate\": 0.03,\n",
      " |                          \"num_leaves\": 128,\n",
      " |                          \"feature_fraction\": 0.9,\n",
      " |                          \"min_data_in_leaf\": 3,\n",
      " |                          \"ag_args\": {\"name_suffix\": \"Large\", \"priority\": 0, \"hyperparameter_tune_kwargs\": None},\n",
      " |                      },\n",
      " |                  ],\n",
      " |                  'CAT': {},\n",
      " |                  'XGB': {},\n",
      " |                  'FASTAI': {},\n",
      " |                  'RF': [\n",
      " |                      {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
      " |                      {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
      " |                      {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
      " |                  ],\n",
      " |                  'XT': [\n",
      " |                      {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
      " |                      {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
      " |                      {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
      " |                  ],\n",
      " |                  'KNN': [\n",
      " |                      {'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}},\n",
      " |                      {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}},\n",
      " |                  ],\n",
      " |              }\n",
      " |      \n",
      " |          Details regarding the hyperparameters you can specify for each model are provided in the following files:\n",
      " |              NN: `autogluon.tabular.models.tabular_nn.hyperparameters.parameters`\n",
      " |                  Note: certain hyperparameter settings may cause these neural networks to train much slower.\n",
      " |              GBM: `autogluon.tabular.models.lgb.hyperparameters.parameters`\n",
      " |                   See also the lightGBM docs: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
      " |              CAT: `autogluon.tabular.models.catboost.hyperparameters.parameters`\n",
      " |                   See also the CatBoost docs: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
      " |              XGB: `autogluon.tabular.models.xgboost.hyperparameters.parameters`\n",
      " |                   See also the XGBoost docs: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
      " |              FASTAI: `autogluon.tabular.models.fastainn.hyperparameters.parameters`\n",
      " |                   See also the FastAI docs: https://docs.fast.ai/tabular.learner.html\n",
      " |              RF: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
      " |                  Note: Hyperparameter tuning is disabled for this model.\n",
      " |              XT: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
      " |                  Note: Hyperparameter tuning is disabled for this model.\n",
      " |              KNN: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
      " |                  Note: Hyperparameter tuning is disabled for this model.\n",
      " |              LR: `autogluon.tabular.models.lr.hyperparameters.parameters`\n",
      " |                  Note: Hyperparameter tuning is disabled for this model.\n",
      " |                  Note: 'penalty' parameter can be used for regression to specify regularization method: 'L1' and 'L2' values are supported.\n",
      " |              Advanced functionality: Custom AutoGluon model arguments\n",
      " |                  These arguments are optional and can be specified in any model's hyperparameters.\n",
      " |                      Example: `hyperparameters = {'RF': {..., 'ag_args': {'name_suffix': 'CustomModelSuffix', 'disable_in_hpo': True}}`\n",
      " |                  ag_args: Dictionary of customization options related to meta properties of the model such as its name, the order it is trained, the problem types it is valid for, and the type of HPO it utilizes.\n",
      " |                      Valid keys:\n",
      " |                          name: (str) The name of the model. This overrides AutoGluon's naming logic and all other name arguments if present.\n",
      " |                          name_main: (str) The main name of the model. Example: 'RandomForest'.\n",
      " |                          name_prefix: (str) Add a custom prefix to the model name. Unused by default.\n",
      " |                          name_suffix: (str) Add a custom suffix to the model name. Unused by default.\n",
      " |                          priority: (int) Determines the order in which the model is trained. Larger values result in the model being trained earlier. Default values range from 100 (KNN) to 0 (custom), dictated by model type. If you want this model to be trained first, set priority = 999.\n",
      " |                          problem_types: (list) List of valid problem types for the model. `problem_types=['binary']` will result in the model only being trained if `problem_type` is 'binary'.\n",
      " |                          disable_in_hpo: (bool) If True, the model will only be trained if `hyperparameter_tune_kwargs=None`.\n",
      " |                          valid_stacker: (bool) If False, the model will not be trained as a level 2 or higher stacker model.\n",
      " |                          valid_base: (bool) If False, the model will not be trained as a level 1 (base) model.\n",
      " |                          hyperparameter_tune_kwargs: (dict) Refer to :meth:`TabularPredictor.fit` hyperparameter_tune_kwargs argument. If specified here, will override global HPO settings for this model.\n",
      " |                      Reference the default hyperparameters for example usage of these options.\n",
      " |                  ag_args_fit: Dictionary of model fit customization options related to how and with what constraints the model is trained. These parameters affect stacker fold models, but not stacker models themselves.\n",
      " |                      Clarification: `time_limit` is the internal time in seconds given to a particular model to train, which is dictated in part by the `time_limit` argument given during `predictor.fit()` but is not the same.\n",
      " |                      Valid keys:\n",
      " |                          stopping_metric: (str or :class:`autogluon.core.metrics.Scorer`, default=None) The metric to use for early stopping of the model. If None, model will decide.\n",
      " |                          max_memory_usage_ratio: (float, default=1.0) The ratio of memory usage relative to the default to allow before early stopping or killing the model. Values greater than 1.0 will be increasingly prone to out-of-memory errors.\n",
      " |                          max_time_limit_ratio: (float, default=1.0) The ratio of the provided time_limit to use during model `fit()`. If `time_limit=10` and `max_time_limit_ratio=0.3`, time_limit would be changed to 3. Does not alter max_time_limit or min_time_limit values.\n",
      " |                          max_time_limit: (float, default=None) Maximum amount of time to allow this model to train for (in sec). If the provided time_limit is greater than this value, it will be replaced by max_time_limit.\n",
      " |                          min_time_limit: (float, default=0) Allow this model to train for at least this long (in sec), regardless of the time limit it would otherwise be granted.\n",
      " |                              If `min_time_limit >= max_time_limit`, time_limit will be set to min_time_limit.\n",
      " |                              If `min_time_limit=None`, time_limit will be set to None and the model will have no training time restriction.\n",
      " |                          num_cpus : (int or str, default='auto')\n",
      " |                              How many CPUs to use during model fit.\n",
      " |                              If 'auto', model will decide.\n",
      " |                          num_gpus : (int or str, default='auto')\n",
      " |                              How many GPUs to use during model fit.\n",
      " |                              If 'auto', model will decide. Some models can use GPUs but don't by default due to differences in model quality.\n",
      " |                              Set to 0 to disable usage of GPUs.\n",
      " |                  ag_args_ensemble: Dictionary of hyperparameters shared by all models that control how they are ensembled, if bag mode is enabled.\n",
      " |                      Valid keys:\n",
      " |                          use_orig_features: [True, False, \"never\"], default True\n",
      " |                              Whether a stack model will use the original features along with the stack features to train (akin to skip-connections).\n",
      " |                              If True, will use the original data features.\n",
      " |                              If False, will discard the original data features and only use stack features, except when no stack features exist (such as in layer 1).\n",
      " |                              If \"never\", will always discard the original data features. Will be skipped in layer 1.\n",
      " |                          valid_stacker : bool, default True\n",
      " |                              If True, will be marked as valid to include as a stacker model.\n",
      " |                              If False, will only be fit as a base model (layer 1) and will not be fit in stack layers (layer 2+).\n",
      " |                          max_base_models : int, default 0\n",
      " |                              Maximum number of base models whose predictions form the features input to this stacker model.\n",
      " |                              If more than `max_base_models` base models are available, only the top `max_base_models` models with highest validation score are used.\n",
      " |                              If 0, the logic is skipped.\n",
      " |                          max_base_models_per_type : int | str, default \"auto\"\n",
      " |                              Similar to `max_base_models`. If more than `max_base_models_per_type` of any particular model type are available,\n",
      " |                              only the top `max_base_models_per_type` of that type are used. This occurs before the `max_base_models` filter.\n",
      " |                              If \"auto\", the value will be adaptively set based on the number of training samples.\n",
      " |                                  More samples will lead to larger values, starting at 1 with <1000 samples, increasing up to 12 at >=50000 samples.\n",
      " |                              If 0, the logic is skipped.\n",
      " |                          num_folds: (int, default=None) If specified, the number of folds to fit in the bagged model.\n",
      " |                              If specified, overrides any other value used to determine the number of folds such as predictor.fit `num_bag_folds` argument.\n",
      " |                          max_sets: (int, default=None) If specified, the maximum sets to fit in the bagged model.\n",
      " |                              The lesser of `max_sets` and the predictor.fit `num_bag_sets` argument will be used for the given model.\n",
      " |                              Useful if a particular model is expensive relative to others and you want to avoid repeated bagging of the expensive model while still repeated bagging the cheaper models.\n",
      " |                          save_bag_folds: (bool, default=True)\n",
      " |                              If True, bagged models will save their fold models (the models from each individual fold of bagging). This is required to use bagged models for prediction.\n",
      " |                              If False, bagged models will not save their fold models. This means that bagged models will not be valid models during inference.\n",
      " |                                  This should only be set to False when planning to call `predictor.refit_full()` or when `refit_full` is set and `set_best_to_refit_full=True`.\n",
      " |                                  Particularly useful if disk usage is a concern. By not saving the fold models, bagged models will use only very small amounts of disk space during training.\n",
      " |                                  In many training runs, this will reduce peak disk usage by >10x.\n",
      " |                          fold_fitting_strategy: (AbstractFoldFittingStrategy default=auto) Whether to fit folds in parallel or in sequential order.\n",
      " |                              If parallel_local, folds will be trained in parallel with evenly distributed computing resources. This could bring 2-4x speedup compared to SequentialLocalFoldFittingStrategy, but could consume much more memory.\n",
      " |                              If sequential_local, folds will be trained in sequential.\n",
      " |                              If auto, strategy will be determined by OS and whether ray is installed or not. MacOS support for parallel_local is unstable, and may crash if enabled.\n",
      " |                          num_folds_parallel: (int or str, default='auto') Number of folds to be trained in parallel if using ParallelLocalFoldFittingStrategy. Consider lowering this value if you encounter either out of memory issue or CUDA out of memory issue(when trained on gpu).\n",
      " |                              if 'auto', will try to train all folds in parallel.\n",
      " |      \n",
      " |      feature_metadata : :class:`autogluon.tabular.FeatureMetadata` or str, default = 'infer'\n",
      " |          The feature metadata used in various inner logic in feature preprocessing.\n",
      " |          If 'infer', will automatically construct a FeatureMetadata object based on the properties of `train_data`.\n",
      " |          In this case, `train_data` is input into :meth:`autogluon.tabular.FeatureMetadata.from_df` to infer `feature_metadata`.\n",
      " |          If 'infer' incorrectly assumes the dtypes of features, consider explicitly specifying `feature_metadata`.\n",
      " |      infer_limit : float, default = None\n",
      " |          The inference time limit in seconds per row to adhere to during fit.\n",
      " |          If infer_limit=0.05 and infer_limit_batch_size=1000, AutoGluon will avoid training models that take longer than 50 ms/row to predict when given a batch of 1000 rows to predict (must predict 1000 rows in no more than 50 seconds).\n",
      " |          If bagging is enabled, the inference time limit will be respected based on estimated inference speed of `_FULL` models after refit_full is called, NOT on the inference speed of the bagged ensembles.\n",
      " |          The inference times calculated for models are assuming `predictor.persist('all')` is called after fit.\n",
      " |          If None, no limit is enforced.\n",
      " |          If it is impossible to satisfy the constraint, an exception will be raised.\n",
      " |      infer_limit_batch_size : int, default = None\n",
      " |          The batch size to use when predicting in bulk to estimate per-row inference time.\n",
      " |          Must be an integer greater than 0.\n",
      " |          If None and `infer_limit` is specified, will default to 10000.\n",
      " |          It is recommended to set to 10000 unless you must satisfy an online-inference scenario.\n",
      " |          Small values, especially `infer_limit_batch_size=1`, will result in much larger per-row inference times and should be avoided if possible.\n",
      " |          Refer to `infer_limit` for more details on how this is used.\n",
      " |          If specified when `infer_limit=None`, the inference time will be logged during training but will not be limited.\n",
      " |      fit_weighted_ensemble : bool, default = True\n",
      " |          If True, a WeightedEnsembleModel will be fit in each stack layer.\n",
      " |          A weighted ensemble will often be stronger than an individual model while being very fast to train.\n",
      " |          It is recommended to keep this value set to True to maximize predictive quality.\n",
      " |      fit_full_last_level_weighted_ensemble : bool, default = True\n",
      " |          If True, the WeightedEnsembleModel of the last stacking level will be fit with all (successful) models from all previous layers as base models.\n",
      " |          If stacking is disabled, settings this to True or False makes no difference because the WeightedEnsembleModel L2 always uses all models from L1.\n",
      " |          It is recommended to keep this value set to True to maximize predictive quality.\n",
      " |      full_weighted_ensemble_additionally : bool, default = False\n",
      " |          If True, AutoGluon will fit two WeightedEnsembleModels after training all stacking levels. Setting this to True, simulates calling\n",
      " |          `fit_weighted_ensemble()` after calling `fit()`. Has no affect if `fit_full_last_level_weighted_ensemble` is False and does not fit an additional\n",
      " |          WeightedEnsembleModel if stacking is disabled.\n",
      " |      dynamic_stacking: bool | str, default = False\n",
      " |          If True and `num_stack_levels` > 0, AutoGluon will dynamically determine whether to use stacking or not by first validating AutoGluon's stacking\n",
      " |          behavior. This is done to avoid so-called stacked overfitting that can make traditional multi-layer stacking, as used in AutoGluon, fail drastically\n",
      " |          and produce unreliable validation scores.\n",
      " |          It is recommended to keep this value set to True or \"auto\" when using stacking,\n",
      " |          as long as it is unknown whether the data is affected by stacked overfitting.\n",
      " |          If it is known that the data is unaffected by stacked overfitting, then setting this value to False is expected to maximize predictive quality.\n",
      " |          If enabled, by default, AutoGluon performs dynamic stacking by spending 25% of the provided time limit for detection and all remaining\n",
      " |          time for fitting AutoGluon. This can be adjusted by specifying `ds_args` with different parameters to `fit()`.\n",
      " |          If \"auto\", will be set to `not use_bag_holdout`.\n",
      " |          See the documentation of `ds_args` for more information.\n",
      " |      calibrate_decision_threshold : bool | str, default = \"auto\"\n",
      " |          If True, will automatically calibrate the decision threshold at the end of fit for calls to `.predict` based on the evaluation metric.\n",
      " |          If \"auto\", will be set to True if `eval_metric.needs_class=True` and `problem_type=\"binary\"`.\n",
      " |          By default, the decision threshold is `0.5`, however for some metrics such as `f1` and `balanced_accuracy`,\n",
      " |          scores can be significantly improved by choosing a threshold other than `0.5`.\n",
      " |          Only valid for `problem_type='binary'`. Ignored for all other problem types.\n",
      " |      num_cpus: int | str, default = \"auto\"\n",
      " |          The total amount of cpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of cpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      num_gpus: int | str, default = \"auto\"\n",
      " |          The total amount of gpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of gpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      fit_strategy: Literal[\"sequential\", \"parallel\"], default = \"sequential\"\n",
      " |          The strategy used to fit models.\n",
      " |          If \"sequential\", models will be fit sequentially. This is the most stable option with the most readable logging.\n",
      " |          If \"parallel\", models will be fit in parallel with ray, splitting available compute between them.\n",
      " |              Note: \"parallel\" is experimental and may run into issues. It was first added in version 1.2.0.\n",
      " |          For machines with 16 or more CPU cores, it is likely that \"parallel\" will be faster than \"sequential\".\n",
      " |      \n",
      " |          .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      memory_limit: float | str, default = \"auto\"\n",
      " |          The total amount of memory in GB you want AutoGluon predictor to use. \"auto\" means AutoGluon will use all available memory on the system\n",
      " |          (that is detectable by psutil).\n",
      " |          Note that this is only a soft limit! AutoGluon uses this limit to skip training models that are expected to require too much memory or stop\n",
      " |          training a model that would exceed the memory limit. AutoGluon does not guarantee the enforcement of this limit (yet). Nevertheless, we expect\n",
      " |          AutoGluon to abide by the limit in most cases or, at most, go over the limit by a small margin.\n",
      " |          For most virtualized systems (e.g., in the cloud) and local usage on a server or laptop, \"auto\" is ideal for this parameter. We recommend manually\n",
      " |          setting the memory limit (and any other resources) on systems with shared resources that are controlled by the operating system (e.g., SLURM and\n",
      " |          cgroups). Otherwise, AutoGluon might wrongly assume more resources are available for fitting a model than the operating system allows,\n",
      " |          which can result in model training failing or being very inefficient.\n",
      " |      callbacks : List[AbstractCallback], default = None\n",
      " |          :::{warning}\n",
      " |          Callbacks are an experimental feature and may change in future releases without warning.\n",
      " |          Callback support is preliminary and targeted towards developers.\n",
      " |          :::\n",
      " |          A list of callback objects inheriting from `autogluon.core.callbacks.AbstractCallback`.\n",
      " |          These objects will be called before and after each model fit within trainer.\n",
      " |          They have the ability to skip models or early stop the training process.\n",
      " |          They can also theoretically change the entire logical flow of the trainer code by interacting with the passed `trainer` object.\n",
      " |          For more details, refer to `AbstractCallback` source code.\n",
      " |          If None, no callback objects will be used.\n",
      " |      \n",
      " |          [Note] Callback objects can be mutated in-place by the fit call if they are stateful.\n",
      " |          Ensure that you avoid re-using a mutated callback object between multiple fit calls.\n",
      " |      \n",
      " |          [Note] Callback objects are deleted from trainer at the end of the fit call. They will not impact operations such as `refit_full` or `fit_extra`.\n",
      " |      **kwargs :\n",
      " |          auto_stack : bool, default = False\n",
      " |              Whether AutoGluon should automatically utilize bagging and multi-layer stack ensembling to boost predictive accuracy.\n",
      " |              Set this = True if you are willing to tolerate longer training times in order to maximize predictive accuracy!\n",
      " |              Automatically sets `num_bag_folds` and `num_stack_levels` arguments based on dataset properties.\n",
      " |              Note: Setting `num_bag_folds` and `num_stack_levels` arguments will override `auto_stack`.\n",
      " |              Note: This can increase training time (and inference time) by up to 20x, but can greatly improve predictive performance.\n",
      " |          num_bag_folds : int, default = None\n",
      " |              Number of folds used for bagging of models. When `num_bag_folds = k`, training time is roughly increased by a factor of `k` (set = 0 to disable bagging).\n",
      " |              Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance.\n",
      " |              Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting.\n",
      " |              `num_bag_folds = 1` is an invalid value, and will raise a ValueError.\n",
      " |              Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting.\n",
      " |              To further improve predictions, avoid increasing `num_bag_folds` much beyond 10 and instead increase `num_bag_sets`.\n",
      " |          num_bag_sets : int, default = None\n",
      " |              Number of repeats of kfold bagging to perform (values must be >= 1). Total number of models trained during bagging = `num_bag_folds * num_bag_sets`.\n",
      " |              Defaults to 1 when unspecified. Value is ignored if `num_bag_folds<=2`.\n",
      " |              Values greater than 1 will result in superior predictive performance, especially on smaller problems and with stacking enabled (reduces overall variance).\n",
      " |              Be warned: This will drastically increase overall runtime, and if using a time limit, can very commonly lead to worse performance.\n",
      " |              It is recommended to increase this value only as a last resort, as it is the least computationally efficient method to improve performance.\n",
      " |          num_stack_levels : int, default = None\n",
      " |              Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of `num_stack_levels+1` (set = 0 to disable stack ensembling).\n",
      " |              Disabled by default (0), but we recommend `num_stack_levels=1` to maximize predictive performance.\n",
      " |              To prevent overfitting, `num_bag_folds >= 2` must also be set or else a ValueError will be raised.\n",
      " |          delay_bag_sets : bool, default = False\n",
      " |              Controls when repeats of kfold bagging are executed in AutoGluon when under a time limit.\n",
      " |              We suggest sticking to `False` to avoid overfitting.\n",
      " |                  If True, AutoGluon delays repeating kfold bagging until after evaluating all models\n",
      " |                      from `hyperparameters`, if there is enough time. This allows AutoGluon to explore\n",
      " |                      more hyperparameters to obtain a better final performance but it may lead to\n",
      " |                      more overfitting.\n",
      " |                  If False, AutoGluon repeats kfold bagging immediately after evaluating each model.\n",
      " |                      Thus, AutoGluon might evaluate fewer models with less overfitting.\n",
      " |          holdout_frac : float, default = None\n",
      " |              Fraction of train_data to holdout as tuning data for optimizing hyperparameters (ignored unless `tuning_data = None`, ignored if `num_bag_folds != 0` unless `use_bag_holdout == True`).\n",
      " |              Default value (if None) is selected based on the number of rows in the training data. Default values range from 0.2 at 2,500 rows to 0.01 at 250,000 rows.\n",
      " |              Default value is doubled if `hyperparameter_tune_kwargs` is set, up to a maximum of 0.2.\n",
      " |              Disabled if `num_bag_folds >= 2` unless `use_bag_holdout == True`.\n",
      " |          use_bag_holdout : bool | str, default = False\n",
      " |              If True, a `holdout_frac` portion of the data is held-out from model bagging.\n",
      " |              This held-out data is only used to score models and determine weighted ensemble weights.\n",
      " |              Enable this if there is a large gap between score_val and score_test in stack models.\n",
      " |              Note: If `tuning_data` was specified, `tuning_data` is used as the holdout data.\n",
      " |              Disabled if not bagging.\n",
      " |              If \"auto\", will be set to True if the training data has >= 1000000 rows, else it will be set to False.\n",
      " |          hyperparameter_tune_kwargs : str or dict, default = None\n",
      " |              Hyperparameter tuning strategy and kwargs (for example, how many HPO trials to run).\n",
      " |              If None, then hyperparameter tuning will not be performed.\n",
      " |              You can either choose to provide a preset\n",
      " |                  Valid preset values:\n",
      " |                      'auto': Performs HPO via bayesian optimization search on NN_TORCH and FASTAI models, and random search on other models using local scheduler.\n",
      " |                      'random': Performs HPO via random search using local scheduler.\n",
      " |              Or provide a dict to specify searchers and schedulers\n",
      " |                  Valid keys:\n",
      " |                      'num_trials': How many HPO trials to run\n",
      " |                      'scheduler': Which scheduler to use\n",
      " |                          Valid values:\n",
      " |                              'local': Local scheduler that schedule trials FIFO\n",
      " |                      'searcher': Which searching algorithm to use\n",
      " |                          'local_random': Uses the 'random' searcher\n",
      " |                          'random': Perform random search\n",
      " |                          'auto': Perform bayesian optimization search on NN_TORCH and FASTAI models. Perform random search on other models.\n",
      " |                  The 'scheduler' and 'searcher' key are required when providing a dict.\n",
      " |                  An example of a valid dict:\n",
      " |                      hyperparameter_tune_kwargs = {\n",
      " |                          'num_trials': 5,\n",
      " |                          'scheduler' : 'local',\n",
      " |                          'searcher': 'auto',\n",
      " |                      }\n",
      " |          feature_prune_kwargs: dict, default = None\n",
      " |              Performs layer-wise feature pruning via recursive feature elimination with permutation feature importance.\n",
      " |              This fits all models in a stack layer once, discovers a pruned set of features, fits all models in the stack layer\n",
      " |              again with the pruned set of features, and updates input feature lists for models whose validation score improved.\n",
      " |              If None, do not perform feature pruning. If empty dictionary, perform feature pruning with default configurations.\n",
      " |              For valid dictionary keys, refer to :class:`autogluon.core.utils.feature_selection.FeatureSelector` and\n",
      " |              `autogluon.core.trainer.abstract_trainer.AbstractTrainer._proxy_model_feature_prune` documentation.\n",
      " |              To force all models to work with the pruned set of features, set force_prune=True in the dictionary.\n",
      " |          ag_args : dict, default = None\n",
      " |              Keyword arguments to pass to all models (i.e. common hyperparameters shared by all AutoGluon models).\n",
      " |              See the `ag_args` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
      " |              Identical to specifying `ag_args` parameter for all models in `hyperparameters`.\n",
      " |              If a key in `ag_args` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
      " |          ag_args_fit : dict, default = None\n",
      " |              Keyword arguments to pass to all models.\n",
      " |              See the `ag_args_fit` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
      " |              Identical to specifying `ag_args_fit` parameter for all models in `hyperparameters`.\n",
      " |              If a key in `ag_args_fit` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
      " |          ag_args_ensemble : dict, default = None\n",
      " |              Keyword arguments to pass to all models.\n",
      " |              See the `ag_args_ensemble` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
      " |              Identical to specifying `ag_args_ensemble` parameter for all models in `hyperparameters`.\n",
      " |              If a key in `ag_args_ensemble` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
      " |          ds_args : dict, see below for default\n",
      " |              Keyword arguments for dynamic stacking, only used if `dynamic_stacking=True`. These keyword arguments control the behavior of dynamic stacking\n",
      " |              and determine how AutoGluon tries to detect stacked overfitting. To detect stacked overfitting, AutoGluon will fit itself (so called sub-fits)\n",
      " |              on a subset (for holdout) or multiple subsets (for repeated cross-validation) and use the predictions of AutoGluon on the validation data to\n",
      " |              detect stacked overfitting. The sub-fits stop and stacking will be disabled if any sub-fit shows stacked overfitting.\n",
      " |              Allowed keys and values are:\n",
      " |                  `detection_time_frac` : float in (0,1), default = 1/4\n",
      " |                      Determines how much of the original training time is used for detecting stacked overfitting.\n",
      " |                      When using (repeated) cross-validation, each sub-fit will be fit for `1/n_splits * detection_time_frac * time_limit`.\n",
      " |                      If no time limit is given to AutoGluon, this parameter is ignored and AutoGluon is fit without a time limit in the sub-fit.\n",
      " |                  `validation_procedure`: str, default = 'holdout'\n",
      " |                      Determines the validation procedure used to detect stacked overfitting. Can be either `cv` or `holdout`.\n",
      " |                          If `validation_procedure='holdout'` and `holdout_data` is not specified (default), then `holdout_frac` determines the holdout data.\n",
      " |                          If `validation_procedure='holdout'` and `holdout_data` is specified, then the provided `holdout_data` is used for validation.\n",
      " |                          If `validation_procedure='cv'`, `n_folds` and `n_repeats` determine the kind cross-validation procedure.\n",
      " |                  `holdout_frac` : float in (0,1), default = 1/9\n",
      " |                      Determines how much of the original training data is used for the holdout data during holdout validation.\n",
      " |                      Ignored if `holdout_data` is not None.\n",
      " |                  `n_folds` : int in [2, +inf), default = 2\n",
      " |                      Number of folds to use for cross-validation.\n",
      " |                  `n_repeats` : int [1, +inf), default = 1\n",
      " |                      Number of repeats to use for repeated cross-validation. If set to 1, performs 1-repeated cross-validation which is equivalent to\n",
      " |                      cross-validation without repeats.\n",
      " |                  `memory_safe_fits` : bool, default = True\n",
      " |                      If True, AutoGluon runs each sub-fit in a ray-based subprocess to avoid memory leakage that exist due to Python's lackluster\n",
      " |                      garbage collector.\n",
      " |                  `clean_up_fits` : bool, default = True\n",
      " |                      If True, AutoGluon will remove all saved information from sub-fits from disk.\n",
      " |                      If False, the sub-fits are kept on disk and `self._sub_fits` will store paths to the sub-fits, which can be loaded just like any other\n",
      " |                      predictor from disk using `TabularPredictor.load()`.\n",
      " |                  `enable_ray_logging` : bool, default = True\n",
      " |                      If True, will log the dynamic stacking sub-fit when ray is used (`memory_safe_fits=True`).\n",
      " |                      Note that because of how ray works, this may cause extra unwanted logging in the main fit process after dynamic stacking completes.\n",
      " |                  `enable_callbacks` : bool, default = False\n",
      " |                      If True, will perform a deepcopy on the specified user callbacks and enable them during the DyStack call.\n",
      " |                      If False, will not include callbacks in the DyStack call.\n",
      " |                  `holdout_data`: str or :class:`pd.DataFrame`, default = None\n",
      " |                      Another dataset containing validation data reserved for detecting stacked overfitting. This dataset should be in the same format as\n",
      " |                      `train_data`. If str is passed, `holdout_data` will be loaded using the str value as the file path.\n",
      " |                      If `holdout_data` is not None, the sub-fit is fit on all of `train_data` and the full fit is fit on all of `train_data` and\n",
      " |                      `holdout_data` combined.\n",
      " |          included_model_types : list, default = None\n",
      " |              To only include listed model types for training during `fit()`.\n",
      " |              Models that are listed in `included_model_types` but not in `hyperparameters` will be ignored.\n",
      " |              Reference `hyperparameters` documentation for what models correspond to each value.\n",
      " |              Useful when only a subset of model needs to be trained and the `hyperparameters` dictionary is difficult or time-consuming.\n",
      " |                  Example: To include both 'GBM' and 'FASTAI' models, specify `included_model_types=['GBM', 'FASTAI']`.\n",
      " |          excluded_model_types : list, default = None\n",
      " |              Banned subset of model types to avoid training during `fit()`, even if present in `hyperparameters`.\n",
      " |              Reference `hyperparameters` documentation for what models correspond to each value.\n",
      " |              Useful when a particular model type such as 'KNN' or 'custom' is not desired but altering the `hyperparameters` dictionary is difficult or time-consuming.\n",
      " |                  Example: To exclude both 'KNN' and 'custom' models, specify `excluded_model_types=['KNN', 'custom']`.\n",
      " |          refit_full : bool or str, default = False\n",
      " |              Whether to retrain all models on all of the data (training + validation) after the normal training procedure.\n",
      " |              This is equivalent to calling `predictor.refit_full(model=refit_full)` after fit.\n",
      " |              If `refit_full=True`, it will be treated as `refit_full='all'`.\n",
      " |              If `refit_full=False`, refitting will not occur.\n",
      " |              Valid str values:\n",
      " |                  `all`: refits all models.\n",
      " |                  `best`: refits only the best model (and its ancestors if it is a stacker model).\n",
      " |                  `{model_name}`: refits only the specified model (and its ancestors if it is a stacker model).\n",
      " |              For bagged models:\n",
      " |                  Reduces a model's inference time by collapsing bagged ensembles into a single model fit on all of the training data.\n",
      " |                  This process will typically result in a slight accuracy reduction and a large inference speedup.\n",
      " |                  The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.\n",
      " |                      The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bag_folds`) and n is the number of finished repeats (`num_bag_sets`) in the bagged ensemble.\n",
      " |                  The runtime is generally 10% or less of the original fit runtime.\n",
      " |                      The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.\n",
      " |              For non-bagged models:\n",
      " |                  Optimizes a model's accuracy by retraining on 100% of the data without using a validation set.\n",
      " |                  Will typically result in a slight accuracy increase and no change to inference time.\n",
      " |                  The runtime will be approximately equal to the original fit runtime.\n",
      " |              This process does not alter the original models, but instead adds additional models.\n",
      " |              If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.\n",
      " |              Models produced by this process will not have validation scores, as they use all of the data for training.\n",
      " |                  Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(test_data)`.\n",
      " |                  If the user does not have additional test data, they should reference the original model's score for an estimate of the performance of the refit_full model.\n",
      " |                      Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.\n",
      " |              The time taken by this process is not enforced by `time_limit`.\n",
      " |          save_bag_folds : bool, default = True\n",
      " |              If True, will save the bagged fold models to disk.\n",
      " |              If False, will not save the bagged fold models, only keeping their metadata and out-of-fold predictions.\n",
      " |                  Note: The bagged models will not be available for prediction, only use this if you intend to call `refit_full`.\n",
      " |                  The purpose of setting it to False is that it greatly decreases the peak disk usage of the predictor during the fit call when bagging.\n",
      " |                  Note that this makes refit_full slightly more likely to crash in scenarios where the dataset is large relative to available system memory.\n",
      " |                  This is because by default, refit_full will fall back to cloning the first fold of the bagged model in case it lacks memory to refit.\n",
      " |                  However, if `save_bag_folds=False`, this fallback isn't possible, as there is not fold model to clone because it wasn't saved.\n",
      " |                  In this scenario, refit will raise an exception for `save_bag_folds=False`, but will succeed if `save_bag_folds=True`.\n",
      " |              Final disk usage of predictor will be identical regardless of the setting after `predictor.delete_models(models_to_keep=\"best\", dry_run=False)` is called post-fit.\n",
      " |          set_best_to_refit_full : bool, default = False\n",
      " |              If True, will change the default model that Predictor uses for prediction when model is not specified to the refit_full version of the model that exhibited the highest validation score.\n",
      " |              Only valid if `refit_full` is set.\n",
      " |          keep_only_best : bool, default = False\n",
      " |              If True, only the best model and its ancestor models are saved in the outputted `predictor`. All other models are deleted.\n",
      " |                  If you only care about deploying the most accurate predictor with the smallest file-size and no longer need any of the other trained models or functionality beyond prediction on new data, then set: `keep_only_best=True`, `save_space=True`.\n",
      " |                  This is equivalent to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` directly after `fit()`.\n",
      " |              If used with `refit_full` and `set_best_to_refit_full`, the best model will be the refit_full model, and the original bagged best model will be deleted.\n",
      " |                  `refit_full` will be automatically set to 'best' in this case to avoid training models which will be later deleted.\n",
      " |          save_space : bool, default = False\n",
      " |              If True, reduces the memory and disk size of predictor by deleting auxiliary model files that aren't needed for prediction on new data.\n",
      " |                  This is equivalent to calling `predictor.save_space()` directly after `fit()`.\n",
      " |              This has NO impact on inference accuracy.\n",
      " |              It is recommended if the only goal is to use the trained model for prediction.\n",
      " |              Certain advanced functionality may no longer be available if `save_space=True`. Refer to `predictor.save_space()` documentation for more details.\n",
      " |          feature_generator : :class:`autogluon.features.generators.AbstractFeatureGenerator`, default = :class:`autogluon.features.generators.AutoMLPipelineFeatureGenerator`\n",
      " |              The feature generator used by AutoGluon to process the input data to the form sent to the models. This often includes automated feature generation and data cleaning.\n",
      " |              It is generally recommended to keep the default feature generator unless handling an advanced use-case.\n",
      " |              To control aspects of the default feature generation process, you can pass in an :class:`AutoMLPipelineFeatureGenerator` object constructed using some of these kwargs:\n",
      " |                  enable_numeric_features : bool, default True\n",
      " |                      Whether to keep features of 'int' and 'float' raw types.\n",
      " |                      These features are passed without alteration to the models.\n",
      " |                      Appends IdentityFeatureGenerator(infer_features_in_args=dict(valid_raw_types=['int', 'float']))) to the generator group.\n",
      " |                  enable_categorical_features : bool, default True\n",
      " |                      Whether to keep features of 'object' and 'category' raw types.\n",
      " |                      These features are processed into memory optimized 'category' features.\n",
      " |                      Appends CategoryFeatureGenerator() to the generator group.\n",
      " |                  enable_datetime_features : bool, default True\n",
      " |                      Whether to keep features of 'datetime' raw type and 'object' features identified as 'datetime_as_object' features.\n",
      " |                      These features will be converted to 'int' features representing milliseconds since epoch.\n",
      " |                      Appends DatetimeFeatureGenerator() to the generator group.\n",
      " |                  enable_text_special_features : bool, default True\n",
      " |                      Whether to use 'object' features identified as 'text' features to generate 'text_special' features such as word count, capital letter ratio, and symbol counts.\n",
      " |                      Appends TextSpecialFeatureGenerator() to the generator group.\n",
      " |                  enable_text_ngram_features : bool, default True\n",
      " |                      Whether to use 'object' features identified as 'text' features to generate 'text_ngram' features.\n",
      " |                      Appends TextNgramFeatureGenerator(vectorizer=vectorizer) to the generator group.\n",
      " |                  enable_raw_text_features : bool, default False\n",
      " |                      Whether to keep the raw text features.\n",
      " |                      Appends IdentityFeatureGenerator(infer_features_in_args=dict(required_special_types=['text'])) to the generator group.\n",
      " |                  vectorizer : CountVectorizer, default CountVectorizer(min_df=30, ngram_range=(1, 3), max_features=10000, dtype=np.uint8)\n",
      " |                      sklearn CountVectorizer object to use in TextNgramFeatureGenerator.\n",
      " |                      Only used if `enable_text_ngram_features=True`.\n",
      " |          unlabeled_data : pd.DataFrame, default = None\n",
      " |              [Experimental Parameter]\n",
      " |              Collection of data without labels that we can use to pretrain on. This is the same schema as train_data, except\n",
      " |              without the labels. Currently, unlabeled_data is only used for pretraining a TabTransformer model.\n",
      " |              If you do not specify 'TRANSF' with unlabeled_data, then no pretraining will occur and unlabeled_data will be ignored!\n",
      " |              After the pretraining step, we will finetune using the TabTransformer model as well. If TabTransformer is ensembled\n",
      " |              with other models, like in typical AutoGluon fashion, then the output of this \"pretrain/finetune\" will be ensembled\n",
      " |              with other models, which will not used the unlabeled_data. The \"pretrain/finetune flow\" is also known as semi-supervised learning.\n",
      " |              The typical use case for unlabeled_data is to add signal to your model where you may not have sufficient training\n",
      " |              data. e.g. 500 hand-labeled samples (perhaps a hard human task), whole data set (unlabeled) is thousands/millions.\n",
      " |              However, this isn't the only use case. Given enough unlabeled data(millions of rows), you may see improvements\n",
      " |              to any amount of labeled data.\n",
      " |          verbosity : int\n",
      " |              If specified, overrides the existing `predictor.verbosity` value.\n",
      " |          raise_on_no_models_fitted: bool, default = True\n",
      " |              If True, will raise a RuntimeError if no models were successfully fit during `fit()`.\n",
      " |          calibrate: bool or str, default = 'auto'\n",
      " |              Note: It is recommended to use ['auto', False] as the values and avoid True.\n",
      " |              If 'auto' will automatically set to True if the problem_type and eval_metric are suitable for calibration.\n",
      " |              If True and the problem_type is classification, temperature scaling will be used to calibrate the Predictor's estimated class probabilities\n",
      " |              (which may improve metrics like log_loss) and will train a scalar parameter on the validation set.\n",
      " |              If True and the problem_type is quantile regression, conformalization will be used to calibrate the Predictor's estimated quantiles\n",
      " |              (which may improve the prediction interval coverage, and bagging could further improve it) and will compute a set of scalar parameters on the validation set.\n",
      " |          test_data : str or :class:`pd.DataFrame`, default = None\n",
      " |              Table of the test data.\n",
      " |              If str is passed, `test_data` will be loaded using the str value as the file path.\n",
      " |              NOTE: This test_data is NEVER SEEN by the model during training and, if specified, is only used for logging purposes (i.e. for learning curve generation).\n",
      " |              This test_data should be treated the same way test data is used in predictor.leaderboard.\n",
      " |          learning_curves : bool or dict, default = None\n",
      " |              If bool and is True, default learning curve hyperparameter ag_args will be initialized for each of the models included in the ensemble.\n",
      " |                  By default, learning curves will include eval_metric scores specified in fit call arguments.\n",
      " |                  This can be overwritten as shown below.\n",
      " |              If dict, user can pass learning_curves parameters to be initialized as ag_args in the following format:\n",
      " |                  learning_curves = {\n",
      " |                      \"metrics\": str or list(str) or Scorer or list(Scorer):\n",
      " |                          autogluon metric scorer(s) to be calculated at each iteration, represented as Scorer object(s) or scorer name(s) (str)\n",
      " |                      \"use_error\": bool : whether to use error or score format for metrics listed above\n",
      " |                  }\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`TabularPredictor` object. Returns self.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from autogluon.tabular import TabularDataset, TabularPredictor\n",
      " |      >>> train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
      " |      >>> label = 'class'\n",
      " |      >>> predictor = TabularPredictor(label=label).fit(train_data)\n",
      " |      >>> test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
      " |      >>> leaderboard = predictor.leaderboard(test_data)\n",
      " |      >>> y_test = test_data[label]\n",
      " |      >>> test_data = test_data.drop(columns=[label])\n",
      " |      >>> y_pred = predictor.predict(test_data)\n",
      " |      >>> perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred)\n",
      " |      \n",
      " |      To maximize predictive performance, use the following:\n",
      " |      \n",
      " |      >>> eval_metric = 'roc_auc'  # set this to the metric you ultimately care about\n",
      " |      >>> time_limit = 3600  # set as long as you are willing to wait (in sec)\n",
      " |      >>> predictor = TabularPredictor(label=label, eval_metric=eval_metric).fit(train_data, presets=['best_quality'], time_limit=time_limit)\n",
      " |  \n",
      " |  fit_extra(self, hyperparameters: 'str | Dict[str, Any]', time_limit: 'float' = None, base_model_names: 'List[str]' = None, fit_weighted_ensemble: 'bool' = True, fit_full_last_level_weighted_ensemble: 'bool' = True, full_weighted_ensemble_additionally: 'bool' = False, num_cpus: 'str | int' = 'auto', num_gpus: 'str | int' = 'auto', fit_strategy: \"Literal['auto', 'sequential', 'parallel']\" = 'auto', memory_limit: 'float | str' = 'auto', **kwargs) -> \"'TabularPredictor'\"\n",
      " |      Fits additional models after the original :meth:`TabularPredictor.fit` call.\n",
      " |      The original train_data and tuning_data will be used to train the models.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hyperparameters : str or dict\n",
      " |          Refer to argument documentation in :meth:`TabularPredictor.fit`.\n",
      " |          If `base_model_names` is specified and hyperparameters is using the level-based key notation,\n",
      " |          the key of the level which directly uses the base models should be 1. The level in the hyperparameters\n",
      " |          dictionary is relative, not absolute.\n",
      " |      time_limit : int, default = None\n",
      " |          Refer to argument documentation in :meth:`TabularPredictor.fit`.\n",
      " |      base_model_names : List[str], default = None\n",
      " |          The names of the models to use as base models for this fit call.\n",
      " |          Base models will provide their out-of-fold predictions as additional features to the models in `hyperparameters`.\n",
      " |          If specified, all models trained will be stack ensembles.\n",
      " |          If None, models will be trained as if they were specified in :meth:`TabularPredictor.fit`, without depending on existing models.\n",
      " |          Only valid if bagging is enabled.\n",
      " |      fit_weighted_ensemble : bool, default = True\n",
      " |          If True, a WeightedEnsembleModel will be fit in each stack layer.\n",
      " |          A weighted ensemble will often be stronger than an individual model while being very fast to train.\n",
      " |          It is recommended to keep this value set to True to maximize predictive quality.\n",
      " |      fit_full_last_level_weighted_ensemble : bool, default = True\n",
      " |          If True, the WeightedEnsembleModel of the last stacking level will be fit with all (successful) models from all previous layers as base models.\n",
      " |          If stacking is disabled, settings this to True or False makes no difference because the WeightedEnsembleModel L2 always uses all models from L1.\n",
      " |          It is recommended to keep this value set to True to maximize predictive quality.\n",
      " |      full_weighted_ensemble_additionally : bool, default = False\n",
      " |          If True, AutoGluon will fit two WeightedEnsembleModels after training all stacking levels. Setting this to True, simulates calling\n",
      " |          `fit_weighted_ensemble()` after calling `fit()`. Has no affect if `fit_full_last_level_weighted_ensemble` is False and does not fit an additional\n",
      " |          WeightedEnsembleModel if stacking is disabled.\n",
      " |      num_cpus: int, default = \"auto\"\n",
      " |          The total amount of cpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of cpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      num_gpus: int, default = \"auto\"\n",
      " |          The total amount of gpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of gpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      fit_strategy: Literal[\"auto\", \"sequential\", \"parallel\"], default = \"auto\"\n",
      " |          The strategy used to fit models.\n",
      " |          If \"auto\", uses the same fit_strategy as used in the original :meth:`TabularPredictor.fit` call.\n",
      " |          If \"sequential\", models will be fit sequentially. This is the most stable option with the most readable logging.\n",
      " |          If \"parallel\", models will be fit in parallel with ray, splitting available compute between them.\n",
      " |              Note: \"parallel\" is experimental and may run into issues. It was first added in version 1.2.0.\n",
      " |          For machines with 16 or more CPU cores, it is likely that \"parallel\" will be faster than \"sequential\".\n",
      " |      \n",
      " |          .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      memory_limit: float | str, default = \"auto\"\n",
      " |          The total amount of memory in GB you want AutoGluon predictor to use. \"auto\" means AutoGluon will use all available memory on the system\n",
      " |          (that is detectable by psutil).\n",
      " |          Note that this is only a soft limit! AutoGluon uses this limit to skip training models that are expected to require too much memory or stop\n",
      " |          training a model that would exceed the memory limit. AutoGluon does not guarantee the enforcement of this limit (yet). Nevertheless, we expect\n",
      " |          AutoGluon to abide by the limit in most cases or, at most, go over the limit by a small margin.\n",
      " |          For most virtualized systems (e.g., in the cloud) and local usage on a server or laptop, \"auto\" is ideal for this parameter. We recommend manually\n",
      " |          setting the memory limit (and any other resources) on systems with shared resources that are controlled by the operating system (e.g., SLURM and\n",
      " |          cgroups). Otherwise, AutoGluon might wrongly assume more resources are available for fitting a model than the operating system allows,\n",
      " |          which can result in model training failing or being very inefficient.\n",
      " |      **kwargs :\n",
      " |          Refer to kwargs documentation in :meth:`TabularPredictor.fit`.\n",
      " |          Note that the following kwargs are not available in `fit_extra` as they cannot be changed from their values set in `fit()`:\n",
      " |              [`holdout_frac`, `num_bag_folds`, `auto_stack`, `feature_generator`, `unlabeled_data`]\n",
      " |          Moreover, `dynamic_stacking` is also not available in `fit_extra` as the detection of stacked overfitting is only supported at the first fit time.\n",
      " |          pseudo_data : pd.DataFrame, default = None\n",
      " |              Data that has been self labeled by Autogluon model and will be incorporated into training during 'fit_extra'\n",
      " |  \n",
      " |  fit_pseudolabel(self, pseudo_data: 'pd.DataFrame', max_iter: 'int' = 3, return_pred_prob: 'bool' = False, use_ensemble: 'bool' = False, fit_ensemble: 'bool' = False, fit_ensemble_every_iter: 'bool' = False, **kwargs)\n",
      " |      [Advanced] Uses additional data (`pseudo_data`) to try to achieve better model quality.\n",
      " |      Pseudo data can come either with or without the `label` column.\n",
      " |      \n",
      " |      If `pseudo_data` is labeled, then models will be refit using the `pseudo_data` as additional training data.\n",
      " |      If bagging, each fold of the bagged ensemble will use all the `pseudo_data` as additional training data.\n",
      " |      `pseudo_data` will never be used for validation/scoring.\n",
      " |      \n",
      " |      If the data is unlabeled, such as providing the batched test data without ground truth available, then transductive learning is leveraged.\n",
      " |      In transductive learning, the existing predictor will predict on `pseudo_data`\n",
      " |      to identify the most confident rows (For example all rows with predictive probability above 95%).\n",
      " |      These rows will then be pseudo-labelled, given the label of the most confident class.\n",
      " |      The pseudo-labelled rows will then be used as additional training data when fitting the models.\n",
      " |      Then, if `max_iter > 1`, this process can repeat itself, using the new models to predict on the unused `pseudo_data` rows\n",
      " |      to see if any new rows should be used in the next iteration as training data.\n",
      " |      We recommend specifying `return_pred_prob=True` if the data is unlabeled to get the correct prediction probabilities on the `pseudo_data`,\n",
      " |      rather than calling `predictor.predict_proba(pseudo_data)`.\n",
      " |      \n",
      " |      For example:\n",
      " |          Original fit: 10000 `train_data` rows with 10-fold bagging\n",
      " |              Bagged fold models will use 9000 `train_data` rows for training, and 1000 for validation.\n",
      " |          `fit_pseudolabel` is called with 5000 row labelled `pseudo_data`.\n",
      " |              Bagged fold models are then fit again with `_PSEUDO` suffix.\n",
      " |              10000 train_data rows with 10-fold bagging + 5000 `pseudo_data` rows.\n",
      " |              Bagged fold models will use 9000 `train_data` rows + 5000 `pseudo_data` rows = 14000 rows for training, and 1000 for validation.\n",
      " |                  Note: The same validation rows will be used as was done in the original fit, so that validation scores are directly comparable.\n",
      " |          Alternatively, `fit_pseduolabel` is called with 5000 rows unlabelled `pseudo_data`.\n",
      " |              Predictor predicts on the `pseudo_data`, finds 965 rows with confident predictions.\n",
      " |              Set the ground truth of those 965 rows as the most confident prediction.\n",
      " |              Bagged fold models are then fit with `_PSEUDO` suffix.\n",
      " |              10000 train_data rows with 10-fold bagging + 965 labelled `pseudo_data` rows.\n",
      " |              Bagged fold models will use 9000 `train_data` rows + 965 `pseudo_data` rows = 9965 rows for training, and 1000 for validation.\n",
      " |                  Note: The same validation rows will be used as was done in the original fit, so that validation scores are directly comparable.\n",
      " |              Repeat the process using the new pseudo-labelled predictor on the remaining `pseudo_data`.\n",
      " |              In the example, lets assume 188 new `pseudo_data` rows have confident predictions.\n",
      " |              Now the total labelled `pseudo_data` rows is 965 + 188 = 1153.\n",
      " |              Then repeat the process, up to `max_iter` times: ex 10000 train_data rows with 10-fold bagging + 1153 `pseudo_data` rows.\n",
      " |              Early stopping will trigger if validation score improvement is not observed.\n",
      " |      \n",
      " |      Note: pseudo_data is only used for L1 models. Support for L2+ models is not yet implemented. L2+ models will only use the original train_data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pseudo_data : :class:`pd.DataFrame`\n",
      " |          Extra data to incorporate into training. Pre-labeled test data allowed. If no labels\n",
      " |          then pseudo-labeling algorithm will predict and filter out which rows to incorporate into training\n",
      " |      max_iter: int, default = 3\n",
      " |          Maximum iterations of pseudo-labeling allowed\n",
      " |      return_pred_prob: bool, default = False\n",
      " |          Returns held-out predictive probabilities from pseudo-labeling. If test_data is labeled then\n",
      " |          returns model's predictive probabilities.\n",
      " |      use_ensemble: bool, default = False\n",
      " |          If True will use ensemble pseudo labeling algorithm. If False will just use best model\n",
      " |          for pseudo labeling algorithm.\n",
      " |      fit_ensemble: bool, default = False\n",
      " |          If True with fit weighted ensemble model using combination of best models.\n",
      " |          Fitting weighted ensemble will be done after fitting has\n",
      " |          been completed unless otherwise specified. If False will not fit weighted ensemble\n",
      " |          over models trained with pseudo labeling and models trained without it.\n",
      " |      fit_ensemble_every_iter: bool, default = False\n",
      " |          If True fits weighted ensemble model for every iteration of pseudo labeling algorithm. If False\n",
      " |          and fit_ensemble is True will fit after all pseudo labeling training is done.\n",
      " |      **kwargs:\n",
      " |          If predictor is not already fit, then kwargs are for the functions 'fit' and 'fit_extra':\n",
      " |          Refer to parameters documentation in :meth:`TabularPredictor.fit`.\n",
      " |          Refer to parameters documentation in :meth:`TabularPredictor.fit_extra`.\n",
      " |          If predictor is fit kwargs are for 'fit_extra':\n",
      " |          Refer to parameters documentation in :meth:`TabularPredictor.fit_extra`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : TabularPredictor\n",
      " |          Returns self, which is a Python class of TabularPredictor\n",
      " |  \n",
      " |  fit_summary(self, verbosity: 'int' = 3, show_plot: 'bool' = False) -> 'dict'\n",
      " |      Output summary of information about models produced during `fit()`.\n",
      " |      May create various generated summary plots and store them in folder: `predictor.path`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      verbosity : int, default = 3\n",
      " |          Controls how detailed of a summary to output.\n",
      " |          Set <= 0 for no output printing, 1 to print just high-level summary,\n",
      " |          2 to print summary and create plots, >= 3 to print all information produced during `fit()`.\n",
      " |      show_plot : bool, default = False\n",
      " |          If True, shows the model summary plot in browser when verbosity > 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dict containing various detailed information. We do not recommend directly printing this dict as it may be very large.\n",
      " |  \n",
      " |  fit_weighted_ensemble(self, base_models: 'list' = None, name_suffix: 'str' = 'Best', expand_pareto_frontier: 'bool' = False, time_limit: 'float' = None, refit_full: 'bool' = False, num_cpus: 'int | str' = 'auto', num_gpus: 'int | str' = 'auto')\n",
      " |      Fits new weighted ensemble models to combine predictions of previously-trained models.\n",
      " |      `cache_data` must have been set to `True` during the original training to enable this functionality.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base_models: list, default = None\n",
      " |          List of model names the weighted ensemble can consider as candidates.\n",
      " |          If None, all previously trained models are considered except for weighted ensemble models.\n",
      " |          As an example, to train a weighted ensemble that can only have weights assigned to the models 'model_a' and 'model_b', set `base_models=['model_a', 'model_b']`\n",
      " |      name_suffix: str, default = 'Best'\n",
      " |          Name suffix to add to the name of the newly fitted ensemble model.\n",
      " |      expand_pareto_frontier: bool, default = False\n",
      " |          If True, will train N-1 weighted ensemble models instead of 1, where `N=len(base_models)`.\n",
      " |          The final model trained when True is equivalent to the model trained when False.\n",
      " |          These weighted ensemble models will attempt to expand the pareto frontier.\n",
      " |          This will create many different weighted ensembles which have different accuracy/memory/inference-speed trade-offs.\n",
      " |          This is particularly useful when inference speed is an important consideration.\n",
      " |      time_limit: float, default = None\n",
      " |          Time in seconds each weighted ensemble model is allowed to train for. If `expand_pareto_frontier=True`, the `time_limit` value is applied to each model.\n",
      " |          If None, the ensemble models train without time restriction.\n",
      " |      refit_full : bool, default = False\n",
      " |          If True, will apply refit_full to all weighted ensembles created during this call.\n",
      " |          Identical to calling `predictor.refit_full(model=predictor.fit_weighted_ensemble(...))`\n",
      " |      num_cpus: int | str, default = \"auto\"\n",
      " |          The total amount of cpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of cpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      num_gpus: int | str, default = \"auto\"\n",
      " |          The total amount of gpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of gpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of newly trained weighted ensemble model names.\n",
      " |      If an exception is encountered while training an ensemble model, that model's name will be absent from the list.\n",
      " |  \n",
      " |  info(self)\n",
      " |      [EXPERIMENTAL] Returns a dictionary of `predictor` metadata.\n",
      " |      Warning: This functionality is currently in preview mode.\n",
      " |          The metadata information returned may change in structure in future versions without warning.\n",
      " |          The definitions of various metadata values are not yet documented.\n",
      " |          The output of this function should not be used for programmatic decisions.\n",
      " |      Contains information such as row count, column count, model training time, validation scores, hyperparameters, and much more.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dictionary of `predictor` metadata.\n",
      " |  \n",
      " |  leaderboard(self, data: 'pd.DataFrame | str | None' = None, extra_info: 'bool' = False, extra_metrics: 'list | None' = None, decision_threshold: 'float | None' = None, score_format: 'str' = 'score', only_pareto_frontier: 'bool' = False, skip_score: 'bool' = False, refit_full: 'bool | None' = None, set_refit_score_to_parent: 'bool' = False, display: 'bool' = False, **kwargs) -> 'pd.DataFrame'\n",
      " |      Output summary of information about models produced during `fit()` as a :class:`pd.DataFrame`.\n",
      " |      Includes information on test and validation scores for all models, model training times, inference times, and stack levels.\n",
      " |      Output DataFrame columns include:\n",
      " |          'model': The name of the model.\n",
      " |      \n",
      " |          'score_val': The validation score of the model on the 'eval_metric'.\n",
      " |              NOTE: Metrics scores always show in higher is better form.\n",
      " |              This means that metrics such as log_loss and root_mean_squared_error will have their signs FLIPPED, and values will be negative.\n",
      " |              This is necessary to avoid the user needing to know the metric to understand if higher is better when looking at leaderboard.\n",
      " |          'eval_metric': The evaluation metric name used to calculate the scores.\n",
      " |              This should be identical to `predictor.eval_metric.name`.\n",
      " |          'pred_time_val': The inference time required to compute predictions on the validation data end-to-end.\n",
      " |              Equivalent to the sum of all 'pred_time_val_marginal' values for the model and all of its base models.\n",
      " |          'fit_time': The fit time required to train the model end-to-end (Including base models if the model is a stack ensemble).\n",
      " |              Equivalent to the sum of all 'fit_time_marginal' values for the model and all of its base models.\n",
      " |          'pred_time_val_marginal': The inference time required to compute predictions on the validation data (Ignoring inference times for base models).\n",
      " |              Note that this ignores the time required to load the model into memory when bagging is disabled.\n",
      " |          'fit_time_marginal': The fit time required to train the model (Ignoring base models).\n",
      " |          'stack_level': The stack level of the model.\n",
      " |              A model with stack level N can take any set of models with stack level less than N as input, with stack level 1 models having no model inputs.\n",
      " |          'can_infer': If model is able to perform inference on new data. If False, then the model either was not saved, was deleted, or an ancestor of the model cannot infer.\n",
      " |              `can_infer` is often False when `save_bag_folds=False` was specified in initial `fit()`.\n",
      " |          'fit_order': The order in which models were fit. The first model fit has `fit_order=1`, and the Nth model fit has `fit_order=N`. The order corresponds to the first child model fit in the case of bagged ensembles.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : str or :class:`pd.DataFrame` (optional)\n",
      " |          This dataset must also contain the label-column with the same column-name as specified during fit().\n",
      " |          If extra_metrics=None and skip_score=True, then the label column is not required.\n",
      " |          If specified, then the leaderboard returned will contain additional columns 'score_test', 'pred_time_test', and 'pred_time_test_marginal'.\n",
      " |              'score_test': The score of the model on the 'eval_metric' for the data provided.\n",
      " |                  NOTE: Metrics scores always show in higher is better form.\n",
      " |                  This means that metrics such as log_loss and root_mean_squared_error will have their signs FLIPPED, and values will be negative.\n",
      " |                  This is necessary to avoid the user needing to know the metric to understand if higher is better when looking at leaderboard.\n",
      " |              'pred_time_test': The true end-to-end wall-clock inference time of the model for the data provided.\n",
      " |                  Equivalent to the sum of all 'pred_time_test_marginal' values for the model and all of its base models.\n",
      " |              'pred_time_test_marginal': The inference time of the model for the data provided, minus the inference time for the model's base models, if it has any.\n",
      " |                  Note that this ignores the time required to load the model into memory when bagging is disabled.\n",
      " |          If str is passed, `data` will be loaded using the str value as the file path.\n",
      " |      extra_info : bool, default = False\n",
      " |          If `True`, will return extra columns with advanced info.\n",
      " |          This requires additional computation as advanced info data is calculated on demand.\n",
      " |          Additional output columns when `extra_info=True` include:\n",
      " |              'num_features': Number of input features used by the model.\n",
      " |                  Some models may ignore certain features in the preprocessed data.\n",
      " |              'num_models': Number of models that actually make up this \"model\" object.\n",
      " |                  For non-bagged models, this is 1. For bagged models, this is equal to the number of child models (models trained on bagged folds) the bagged ensemble contains.\n",
      " |              'num_models_w_ancestors': Equivalent to the sum of 'num_models' values for the model and its' ancestors (see below).\n",
      " |              'memory_size': The amount of memory in bytes the model requires when persisted in memory. This is not equivalent to the amount of memory the model may use during inference.\n",
      " |                  For bagged models, this is the sum of the 'memory_size' of all child models.\n",
      " |              'memory_size_w_ancestors': Equivalent to the sum of 'memory_size' values for the model and its' ancestors.\n",
      " |                  This is the amount of memory required to avoid loading any models in-between inference calls to get predictions from this model.\n",
      " |                  For online-inference, this is critical. It is important that the machine performing online inference has memory more than twice this value to avoid loading models for every call to inference by persisting models in memory.\n",
      " |              'memory_size_min': The amount of memory in bytes the model minimally requires to perform inference.\n",
      " |                  For non-bagged models, this is equivalent to 'memory_size'.\n",
      " |                  For bagged models, this is equivalent to the largest child model's 'memory_size_min'.\n",
      " |                  To minimize memory usage, child models can be loaded and un-persisted one by one to infer. This is the default behavior if a bagged model was not already persisted in memory prior to inference.\n",
      " |              'memory_size_min_w_ancestors': Equivalent to the max of the 'memory_size_min' values for the model and its' ancestors.\n",
      " |                  This is the minimum required memory to infer with the model by only loading one model at a time, as each of its ancestors will also have to be loaded into memory.\n",
      " |                  For offline-inference where latency is not a concern, this should be used to determine the required memory for a machine if 'memory_size_w_ancestors' is too large.\n",
      " |              'num_ancestors': Number of ancestor models for the given model.\n",
      " |      \n",
      " |              'num_descendants': Number of descendant models for the given model.\n",
      " |      \n",
      " |              'model_type': The type of the given model.\n",
      " |                  If the model is an ensemble type, 'child_model_type' will indicate the inner model type. A stack ensemble of bagged LightGBM models would have 'StackerEnsembleModel' as its model type.\n",
      " |              'child_model_type': The child model type. None if the model is not an ensemble. A stack ensemble of bagged LightGBM models would have 'LGBModel' as its child type.\n",
      " |                  child models are models which are used as a group to generate a given bagged ensemble model's predictions. These are the models trained on each fold of a bagged ensemble.\n",
      " |                  For 10-fold bagging, the bagged ensemble model would have 10 child models.\n",
      " |                  For 10-fold bagging with 3 repeats, the bagged ensemble model would have 30 child models.\n",
      " |                  Note that child models are distinct from ancestors and descendants.\n",
      " |              'hyperparameters': The hyperparameter values specified for the model.\n",
      " |                  All hyperparameters that do not appear in this dict remained at their default values.\n",
      " |              'hyperparameters_fit': The hyperparameters set by the model during fit.\n",
      " |                  This overrides the 'hyperparameters' value for a particular key if present in 'hyperparameters_fit' to determine the fit model's final hyperparameters.\n",
      " |                  This is most commonly set for hyperparameters that indicate model training iterations or epochs, as early stopping can find a different value from what 'hyperparameters' indicated.\n",
      " |                  In these cases, the provided hyperparameter in 'hyperparameters' is used as a maximum for the model, but the model is still able to early stop at a smaller value during training to achieve a better validation score or to satisfy time constraints.\n",
      " |                  For example, if a NN model was given `epochs=500` as a hyperparameter, but found during training that `epochs=60` resulted in optimal validation score, it would use `epoch=60` and `hyperparameters_fit={'epoch': 60}` would be set.\n",
      " |              'ag_args_fit': Special AutoGluon arguments that influence model fit.\n",
      " |                  See the documentation of the `hyperparameters` argument in `TabularPredictor.fit()` for more information.\n",
      " |              'features': List of feature names used by the model.\n",
      " |      \n",
      " |              'child_hyperparameters': Equivalent to 'hyperparameters', but for the model's children.\n",
      " |      \n",
      " |              'child_hyperparameters_fit': Equivalent to 'hyperparameters_fit', but for the model's children.\n",
      " |      \n",
      " |              'child_ag_args_fit': Equivalent to 'ag_args_fit', but for the model's children.\n",
      " |      \n",
      " |              'ancestors': The model's ancestors. Ancestor models are the models which are required to make predictions during the construction of the model's input features.\n",
      " |                  If A is an ancestor of B, then B is a descendant of A.\n",
      " |                  If a model's ancestor is deleted, the model is no longer able to infer on new data, and its 'can_infer' value will be False.\n",
      " |                  A model can only have ancestor models whose 'stack_level' are lower than itself.\n",
      " |                  'stack_level'=1 models have no ancestors.\n",
      " |              'descendants': The model's descendants. Descendant models are the models which require this model to make predictions during the construction of their input features.\n",
      " |                  If A is a descendant of B, then B is an ancestor of A.\n",
      " |                  If this model is deleted, then all descendant models will no longer be able to infer on new data, and their 'can_infer' values will be False.\n",
      " |                  A model can only have descendant models whose 'stack_level' are higher than itself.\n",
      " |      extra_metrics : list, default = None\n",
      " |          A list of metrics to calculate scores for and include in the output DataFrame.\n",
      " |          Only valid when `data` is specified. The scores refer to the scores on `data` (same data as used to calculate the `score_test` column).\n",
      " |          This list can contain any values which would also be valid for `eval_metric` in predictor init.\n",
      " |          For example, `extra_metrics=['accuracy', 'roc_auc', 'log_loss']` would be valid in binary classification.\n",
      " |          This example would return 3 additional columns in the output DataFrame, whose column names match the names of the metrics.\n",
      " |          Passing `extra_metrics=[predictor.eval_metric]` would return an extra column in the name of the eval metric that has identical values to `score_test`.\n",
      " |          This also works with custom metrics. If passing an object instead of a string, the column name will be equal to the `.name` attribute of the object.\n",
      " |          NOTE: Metrics scores always show in higher is better form.\n",
      " |          This means that metrics such as log_loss and root_mean_squared_error will have their signs FLIPPED, and values will be negative.\n",
      " |          This is necessary to avoid the user needing to know the metric to understand if higher is better when looking at leaderboard.\n",
      " |      decision_threshold : float, default = None\n",
      " |          The decision threshold to use when converting prediction probabilities to predictions.\n",
      " |          This will impact the scores of metrics such as `f1` and `accuracy`.\n",
      " |          If None, defaults to `predictor.decision_threshold`. Ignored unless `problem_type='binary'`.\n",
      " |          Refer to the `predictor.decision_threshold` docstring for more information.\n",
      " |          NOTE: `score_val` will not be impacted by this value in v0.8.\n",
      " |              `score_val` will always show the validation scores achieved with a decision threshold of `0.5`.\n",
      " |              Only test scores will be properly updated.\n",
      " |      score_format : {'score', 'error'}\n",
      " |          If \"score\", leaderboard is returned as normal.\n",
      " |          If \"error\", the column \"score_val\" is converted to \"metric_error_val\", and \"score_test\" is converted to \"metric_error_test\".\n",
      " |              \"metric_error\" is calculated by taking `predictor.eval_metric.convert_score_to_error(score)`.\n",
      " |              This will result in errors where 0 is perfect and lower is better.\n",
      " |      only_pareto_frontier : bool, default = False\n",
      " |          If `True`, only return model information of models in the Pareto frontier of the accuracy/latency trade-off (models which achieve the highest score within their end-to-end inference time).\n",
      " |          At minimum this will include the model with the highest score and the model with the lowest inference time.\n",
      " |          This is useful when deciding which model to use during inference if inference time is a consideration.\n",
      " |          Models filtered out by this process would never be optimal choices for a user that only cares about model inference time and score.\n",
      " |      skip_score : bool, default = False\n",
      " |          [Advanced, primarily for developers]\n",
      " |          If `True`, will skip computing `score_test` if `data` is specified. `score_test` will be set to NaN for all models.\n",
      " |          `pred_time_test` and related columns will still be computed.\n",
      " |      refit_full : bool, default = None\n",
      " |          If True, will return only models that have been refit (ex: have `_FULL` in the name).\n",
      " |          If False, will return only models that have not been refit.\n",
      " |          If None, will return all models.\n",
      " |      set_refit_score_to_parent : bool, default = False\n",
      " |          If True, the `score_val` of refit models will be set to the `score_val` of their parent.\n",
      " |          While this does not represent the genuine validation score of the refit model, it is a reasonable proxy.\n",
      " |      display : bool, default = False\n",
      " |          If True, the output DataFrame is printed to stdout.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pd.DataFrame` of model performance summary information.\n",
      " |  \n",
      " |  learning_curves(self) -> 'Tuple[dict, dict]'\n",
      " |      Retrieves learning curves generated during predictor.fit().\n",
      " |      Will not work if the learning_curves flag was not set during training.\n",
      " |      Note that learning curves are only generated for iterative learners with\n",
      " |      learning curve support.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      metadata: dict\n",
      " |          A dictionary containing metadata related to the training process.\n",
      " |      \n",
      " |      model_data: dict\n",
      " |          A dictionary containing the learning curves across all models.\n",
      " |          To see curve_data format, refer to AbstractModel's save_learning_curves() method.\n",
      " |              {\n",
      " |                  \"model\": curve_data,\n",
      " |                  \"model\": curve_data,\n",
      " |                  \"model\": curve_data,\n",
      " |                  \"model\": curve_data,\n",
      " |              }\n",
      " |  \n",
      " |  load_data_internal(self, data='train', return_X=True, return_y=True)\n",
      " |      Loads the internal data representation used during model training.\n",
      " |      Individual AutoGluon models like the neural network may apply additional feature transformations that are not reflected in this method.\n",
      " |      This method only applies universal transforms employed by all AutoGluon models.\n",
      " |      Warning, the internal representation may:\n",
      " |          Have different features compared to the original data.\n",
      " |          Have different row counts compared to the original data.\n",
      " |          Have indices which do not align with the original data.\n",
      " |          Have label values which differ from those in the original data.\n",
      " |      Internal data representations should NOT be combined with the original data, in most cases this is not possible.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : str, default = 'train'\n",
      " |          The data to load.\n",
      " |          Valid values are:\n",
      " |              'train':\n",
      " |                  Load the training data used during model training.\n",
      " |                  This is a transformed and augmented version of the `train_data` passed in `fit()`.\n",
      " |              'val':\n",
      " |                  Load the validation data used during model training.\n",
      " |                  This is a transformed and augmented version of the `tuning_data` passed in `fit()`.\n",
      " |                  If `tuning_data=None` was set in `fit()`, then `tuning_data` is an automatically generated validation set created by splitting `train_data`.\n",
      " |                  Warning: Will raise an exception if called by a bagged predictor, as bagged predictors have no validation data.\n",
      " |      return_X : bool, default = True\n",
      " |          Whether to return the internal data features\n",
      " |          If set to `False`, then the first element in the returned tuple will be None.\n",
      " |      return_y : bool, default = True\n",
      " |          Whether to return the internal data labels\n",
      " |          If set to `False`, then the second element in the returned tuple will be None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Tuple of (:class:`pd.DataFrame`, :class:`pd.Series`) corresponding to the internal data features and internal data labels, respectively.\n",
      " |  \n",
      " |  model_failures(self, verbose: 'bool' = False) -> 'pd.DataFrame'\n",
      " |      [Advanced] Get the model failures that occurred during the fitting of this model, in the form of a pandas DataFrame.\n",
      " |      \n",
      " |      This is useful for in-depth debugging of model failures and identifying bugs.\n",
      " |      \n",
      " |      For more information on model failures, refer to `predictor.info()['model_info_failures']`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      verbose: bool, default = False\n",
      " |          If True, the output DataFrame is printed to stdout.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model_failures_df: pd.DataFrame\n",
      " |          A DataFrame of model failures. Each row corresponds to a model failure, and columns correspond to meta information about that model.\n",
      " |      \n",
      " |          Included Columns:\n",
      " |              \"model\": The name of the model that failed\n",
      " |              \"exc_type\": The class name of the exception raised\n",
      " |              \"total_time\": The total time in seconds taken by the model prior to the exception (lost time due to the failure)\n",
      " |              \"model_type\": The class name of the model\n",
      " |              \"child_model_type\": The child class name of the model\n",
      " |              \"is_initialized\"\n",
      " |              \"is_fit\"\n",
      " |              \"is_valid\"\n",
      " |              \"can_infer\"\n",
      " |              \"num_features\"\n",
      " |              \"num_models\"\n",
      " |              \"memory_size\"\n",
      " |              \"hyperparameters\"\n",
      " |              \"hyperparameters_fit\"\n",
      " |              \"child_hyperparameters\"\n",
      " |              \"child_hyperparameters_fit\"\n",
      " |              \"exc_str\": The string message contained in the raised exception\n",
      " |              \"exc_traceback\": The full traceback message of the exception as a string\n",
      " |              \"exc_order\": The order of the model failure (starting from 1)\n",
      " |  \n",
      " |  model_names(self, stack_name: 'str' = None, level: 'int' = None, can_infer: 'bool' = None, models: 'List[str]' = None, persisted: 'bool' = None) -> 'List[str]'\n",
      " |      Returns the list of model names trained in this `predictor` object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      stack_name: str, default = None\n",
      " |          If specified, returns only models under a given stack name.\n",
      " |      level: int, default = None\n",
      " |          If specified, returns only models at the given stack level.\n",
      " |      can_infer: bool, default = None\n",
      " |          If specified, returns only models that can/cannot infer on new data.\n",
      " |      models: List[str], default = None\n",
      " |          The list of model names to consider.\n",
      " |          If None, considers all models.\n",
      " |      persisted: bool, default = None\n",
      " |          If None: no filtering will occur based on persisted status\n",
      " |          If True: will return only the models that are persisted in memory via `predictor.persist()`\n",
      " |          If False: will return only the models that are not persisted in memory via `predictor.persist()`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of model names\n",
      " |  \n",
      " |  model_refit_map(self, inverse=False) -> 'Dict[str, str]'\n",
      " |      Returns a dictionary of original model name -> refit full model name.\n",
      " |      Empty unless `refit_full=True` was set during fit or `predictor.refit_full()` was called.\n",
      " |      This can be useful when determining the best model based off of `predictor.leaderboard()`, then getting the _FULL version of the model by passing its name as the key to this dictionary.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inverse : bool, default = False\n",
      " |          If True, instead returns a dictionary of refit full model name -> original model name (Swap keys with values)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dictionary of original model name -> refit full model name.\n",
      " |  \n",
      " |  persist(self, models='best', with_ancestors=True, max_memory=0.4) -> 'List[str]'\n",
      " |      Persist models in memory for reduced inference latency. This is particularly important if the models are being used for online-inference where low latency is critical.\n",
      " |      If models are not persisted in memory, they are loaded from disk every time they are asked to make predictions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      models : list of str or str, default = 'best'\n",
      " |          Model names of models to persist.\n",
      " |          If 'best' then the model with the highest validation score is persisted (this is the model used for prediction by default).\n",
      " |          If 'all' then all models are persisted.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |      with_ancestors : bool, default = True\n",
      " |          If True, all ancestor models of the provided models will also be persisted.\n",
      " |          If False, stacker models will not have the models they depend on persisted unless those models were specified in `models`. This will slow down inference as the ancestor models will still need to be loaded from disk for each predict call.\n",
      " |          Only relevant for stacker models.\n",
      " |      max_memory : float, default = 0.4\n",
      " |          Proportion of total available memory to allow for the persisted models to use.\n",
      " |          If the models' summed memory usage requires a larger proportion of memory than max_memory, they are not persisted. In this case, the output will be an empty list.\n",
      " |          If None, then models are persisted regardless of estimated memory usage. This can cause out-of-memory errors.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of persisted model names.\n",
      " |  \n",
      " |  plot_ensemble_model(self, model: 'str' = 'best', *, prune_unused_nodes: 'bool' = True, filename: 'str' = 'ensemble_model.png') -> 'str'\n",
      " |      Output the visualized stack ensemble architecture of a model trained by `fit()`.\n",
      " |      The plot is stored to a file, `ensemble_model.png` in folder `predictor.path` (or by the name specified in `filename`)\n",
      " |      \n",
      " |      This function requires `graphviz` and `pygraphviz` to be installed because this visualization depends on those package.\n",
      " |      Unless this function will raise `ImportError` without being able to generate the visual of the ensemble model.\n",
      " |      \n",
      " |      To install the required package, run the below commands (for Ubuntu linux):\n",
      " |      \n",
      " |      $ sudo apt-get install graphviz graphviz-dev\n",
      " |      $ pip install pygraphviz\n",
      " |      \n",
      " |      For other platforms, refer to https://graphviz.org/ for Graphviz install, and https://pygraphviz.github.io/ for PyGraphviz.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : str, default 'best'\n",
      " |          The model to highlight in golden orange, with all component models highlighted in yellow.\n",
      " |          If 'best', will default to the best model returned from `self.model_best`\n",
      " |      prune_unused_nodes : bool, default True\n",
      " |          If True, only plot the models that are components of the specified `model`.\n",
      " |          If False, will plot all models.\n",
      " |      filename : str, default 'ensemble_model.png'\n",
      " |          The filename to save the plot as. Will be located under the `self.path` folder.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The file name with the full path to the saved graphic on disk.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from autogluon.tabular import TabularDataset, TabularPredictor\n",
      " |      >>> train_data = TabularDataset('train.csv')\n",
      " |      >>> predictor = TabularPredictor(label='class').fit(train_data)\n",
      " |      >>> path_to_png = predictor.plot_ensemble_model()\n",
      " |      >>>\n",
      " |      >>> # To view the plot inside a Jupyter Notebook, use the below code:\n",
      " |      >>> from IPython.display import Image, display\n",
      " |      >>> display(Image(filename=path_to_png))\n",
      " |  \n",
      " |  predict(self, data: 'pd.DataFrame | str', model: 'str | None' = None, as_pandas: 'bool' = True, transform_features: 'bool' = True, *, decision_threshold: 'float | None' = None) -> 'pd.Series | np.ndarray'\n",
      " |      Use trained models to produce predictions of `label` column values for new data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : :class:`pd.DataFrame` or str\n",
      " |          The data to make predictions for. Should contain same column names as training data and follow same format\n",
      " |          (may contain extra columns that won't be used by Predictor, including the label-column itself).\n",
      " |          If str is passed, `data` will be loaded using the str value as the file path.\n",
      " |      model : str (optional)\n",
      " |          The name of the model to get predictions from. Defaults to None, which uses the highest scoring model on the validation set.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`\n",
      " |      as_pandas : bool, default = True\n",
      " |          Whether to return the output as a :class:`pd.Series` (True) or :class:`np.ndarray` (False).\n",
      " |      transform_features : bool, default = True\n",
      " |          If True, preprocesses data before predicting with models.\n",
      " |          If False, skips global feature preprocessing.\n",
      " |              This is useful to save on inference time if you have already called `data = predictor.transform_features(data)`.\n",
      " |      decision_threshold : float, default = None\n",
      " |          The decision threshold used to convert prediction probabilities to predictions.\n",
      " |          Only relevant for binary classification, otherwise ignored.\n",
      " |          If None, defaults to `predictor.decision_threshold`.\n",
      " |          Valid values are in the range [0.0, 1.0]\n",
      " |          You can obtain an optimized `decision_threshold` by first calling `predictor.calibrate_decision_threshold()`.\n",
      " |          Useful to set for metrics such as `balanced_accuracy` and `f1` as `0.5` is often not an optimal threshold.\n",
      " |          Predictions are calculated via the following logic on the positive class: `1 if pred > decision_threshold else 0`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Array of predictions, one corresponding to each row in given dataset. Either :class:`np.ndarray` or :class:`pd.Series` depending on `as_pandas` argument.\n",
      " |  \n",
      " |  predict_from_proba(self, y_pred_proba: 'pd.DataFrame | np.ndarray', decision_threshold: 'float | None' = None) -> 'pd.Series | np.array'\n",
      " |      Given prediction probabilities, convert to predictions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y_pred_proba : :class:`pd.DataFrame` or :class:`np.ndarray`\n",
      " |          The prediction probabilities to convert to predictions.\n",
      " |          Obtainable via the output of `predictor.predict_proba`.\n",
      " |      decision_threshold : float, default = None\n",
      " |          The decision threshold used to convert prediction probabilities to predictions.\n",
      " |          Only relevant for binary classification, otherwise ignored.\n",
      " |          If None, defaults to `predictor.decision_threshold`.\n",
      " |          Valid values are in the range [0.0, 1.0]\n",
      " |          You can obtain an optimized `decision_threshold` by first calling `predictor.calibrate_decision_threshold()`.\n",
      " |          Useful to set for metrics such as `balanced_accuracy` and `f1` as `0.5` is often not an optimal threshold.\n",
      " |          Predictions are calculated via the following logic on the positive class: `1 if pred > decision_threshold else 0`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Array of predictions, one corresponding to each row in given dataset. Either :class:`np.ndarray` or :class:`pd.Series` depending on `y_pred_proba` dtype.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from autogluon.tabular import TabularPredictor\n",
      " |      >>> predictor = TabularPredictor(label='class').fit('train.csv', label='class')\n",
      " |      >>> y_pred_proba = predictor.predict_proba('test.csv')\n",
      " |      >>>\n",
      " |      >>> # y_pred and y_pred_from_proba are identical\n",
      " |      >>> y_pred = predictor.predict('test.csv')\n",
      " |      >>> y_pred_from_proba = predictor.predict_from_proba(y_pred_proba=y_pred_proba)\n",
      " |  \n",
      " |  predict_multi(self, data: 'pd.DataFrame' = None, models: 'List[str]' = None, as_pandas: 'bool' = True, transform_features: 'bool' = True, inverse_transform: 'bool' = True, *, decision_threshold: 'float' = None) -> 'dict[str, pd.Series] | dict[str, np.ndarray]'\n",
      " |      Returns a dictionary of predictions where the key is\n",
      " |      the model name and the value is the model's prediction probabilities on the data.\n",
      " |      \n",
      " |      Equivalent output to:\n",
      " |      ```\n",
      " |      predict_dict = {}\n",
      " |      for m in models:\n",
      " |          predict_dict[m] = predictor.predict(data, model=m)\n",
      " |      ```\n",
      " |      \n",
      " |      Note that this will generally be much faster than calling :meth:`TabularPredictor.predict` separately for each model\n",
      " |      because this method leverages the model dependency graph to avoid redundant computation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DataFrame, default = None\n",
      " |          The data to predict on.\n",
      " |          If None:\n",
      " |              If self.has_val, the validation data is used.\n",
      " |              Else, prediction is skipped and the out-of-fold (OOF) predictions are returned, equivalent to:\n",
      " |              ```\n",
      " |              predict_dict = {}\n",
      " |              for m in models:\n",
      " |                  predict_dict[m] = predictor.predict_oof(model=m)\n",
      " |              ```\n",
      " |      models : List[str], default = None\n",
      " |          The list of models to get predictions for.\n",
      " |          If None, all models that can infer are used.\n",
      " |      as_pandas : bool, default = True\n",
      " |          Whether to return the output of each model as a :class:`pd.Series` (True) or :class:`np.ndarray` (False).\n",
      " |      transform_features : bool, default = True\n",
      " |          If True, preprocesses data before predicting with models.\n",
      " |          If False, skips global feature preprocessing.\n",
      " |              This is useful to save on inference time if you have already called `data = predictor.transform_features(data)`.\n",
      " |      inverse_transform : bool, default = True\n",
      " |          If True, will return predictions in the original format.\n",
      " |          If False (advanced), will return predictions in AutoGluon's internal format.\n",
      " |      decision_threshold : float, default = None\n",
      " |          The decision threshold used to convert prediction probabilities to predictions.\n",
      " |          Only relevant for binary classification, otherwise ignored.\n",
      " |          If None, defaults to `0.5`.\n",
      " |          Valid values are in the range [0.0, 1.0]\n",
      " |          You can obtain an optimized `decision_threshold` by first calling :meth:`TabularPredictor.calibrate_decision_threshold`.\n",
      " |          Useful to set for metrics such as `balanced_accuracy` and `f1` as `0.5` is often not an optimal threshold.\n",
      " |          Predictions are calculated via the following logic on the positive class: `1 if pred > decision_threshold else 0`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict[str, pd.Series] | dict[str, np.ndarray]\n",
      " |          Dictionary with model names as keys and model predictions as values.\n",
      " |  \n",
      " |  predict_oof(self, model: 'str' = None, *, transformed=False, train_data=None, internal_oof=False, decision_threshold=None, can_infer=None) -> 'pd.Series'\n",
      " |      Note: This is advanced functionality not intended for normal usage.\n",
      " |      \n",
      " |      Returns the out-of-fold (OOF) predictions for every row in the training data.\n",
      " |      \n",
      " |      For a similar method, refer to :meth:`TabularPredictor.predict_multi` with `data=None`.\n",
      " |      For more information, refer to `predict_proba_oof()` documentation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : str (optional)\n",
      " |          Refer to `predict_proba_oof()` documentation.\n",
      " |      transformed : bool, default = False\n",
      " |          Refer to `predict_proba_oof()` documentation.\n",
      " |      train_data : pd.DataFrame, default = None\n",
      " |          Refer to `predict_proba_oof()` documentation.\n",
      " |      internal_oof : bool, default = False\n",
      " |          Refer to `predict_proba_oof()` documentation.\n",
      " |      decision_threshold : float, default = None\n",
      " |          Refer to `predict_multi` documentation.\n",
      " |      can_infer : bool, default = None\n",
      " |          Refer to `predict_proba_oof()` documentation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pd.Series` object of the out-of-fold training predictions of the model.\n",
      " |  \n",
      " |  predict_proba(self, data: 'pd.DataFrame | str', model: 'str | None' = None, as_pandas: 'bool' = True, as_multiclass: 'bool' = True, transform_features: 'bool' = True) -> 'pd.DataFrame | pd.Series | np.ndarray'\n",
      " |      Use trained models to produce predicted class probabilities rather than class-labels (if task is classification).\n",
      " |      If `predictor.problem_type` is regression or quantile, this will raise an AssertionError.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : :class:`pd.DataFrame` or str\n",
      " |          The data to make predictions for. Should contain same column names as training dataset and follow same format\n",
      " |          (may contain extra columns that won't be used by Predictor, including the label-column itself).\n",
      " |          If str is passed, `data` will be loaded using the str value as the file path.\n",
      " |      model : str (optional)\n",
      " |          The name of the model to get prediction probabilities from. Defaults to None, which uses the highest scoring model on the validation set.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |      as_pandas : bool, default = True\n",
      " |          Whether to return the output as a pandas object (True) or numpy array (False).\n",
      " |          Pandas object is a DataFrame if this is a multiclass problem or `as_multiclass=True`, otherwise it is a Series.\n",
      " |          If the output is a DataFrame, the column order will be equivalent to `predictor.class_labels`.\n",
      " |      as_multiclass : bool, default = True\n",
      " |          Whether to return binary classification probabilities as if they were for multiclass classification.\n",
      " |              Output will contain two columns, and if `as_pandas=True`, the column names will correspond to the binary class labels.\n",
      " |              The columns will be the same order as `predictor.class_labels`.\n",
      " |          If False, output will contain only 1 column for the positive class (get positive_class name via `predictor.positive_class`).\n",
      " |          Only impacts output for binary classification problems.\n",
      " |      transform_features : bool, default = True\n",
      " |          If True, preprocesses data before predicting with models.\n",
      " |          If False, skips global feature preprocessing.\n",
      " |              This is useful to save on inference time if you have already called `data = predictor.transform_features(data)`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Array of predicted class-probabilities, corresponding to each row in the given data.\n",
      " |      May be a :class:`np.ndarray` or :class:`pd.DataFrame` / :class:`pd.Series` depending on `as_pandas` and `as_multiclass` arguments and the type of prediction problem.\n",
      " |      For binary classification problems, the output contains for each datapoint the predicted probabilities of the negative and positive classes, unless you specify `as_multiclass=False`.\n",
      " |  \n",
      " |  predict_proba_multi(self, data: 'pd.DataFrame' = None, models: 'List[str]' = None, as_pandas: 'bool' = True, as_multiclass: 'bool' = True, transform_features: 'bool' = True, inverse_transform: 'bool' = True) -> 'dict[str, pd.DataFrame] | dict[str, pd.Series] | dict[str, np.ndarray]'\n",
      " |      Returns a dictionary of prediction probabilities where the key is\n",
      " |      the model name and the value is the model's prediction probabilities on the data.\n",
      " |      \n",
      " |      Equivalent output to:\n",
      " |      ```\n",
      " |      predict_proba_dict = {}\n",
      " |      for m in models:\n",
      " |          predict_proba_dict[m] = predictor.predict_proba(data, model=m)\n",
      " |      ```\n",
      " |      \n",
      " |      Note that this will generally be much faster than calling :meth:`TabularPredictor.predict_proba` separately for each model\n",
      " |      because this method leverages the model dependency graph to avoid redundant computation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : str or DataFrame, default = None\n",
      " |          The data to predict on.\n",
      " |          If None:\n",
      " |              If self.has_val, the validation data is used.\n",
      " |              Else, prediction is skipped and the out-of-fold (OOF) prediction probabilities are returned, equivalent to:\n",
      " |              ```\n",
      " |              predict_proba_dict = {}\n",
      " |              for m in models:\n",
      " |                  predict_proba_dict[m] = predictor.predict_proba_oof(model=m)\n",
      " |              ```\n",
      " |      models : List[str], default = None\n",
      " |          The list of models to get predictions for.\n",
      " |          If None, all models that can infer are used.\n",
      " |      as_pandas : bool, default = True\n",
      " |          Whether to return the output of each model as a pandas object (True) or numpy array (False).\n",
      " |          Pandas object is a :class:`pd.DataFrame` if this is a multiclass problem or `as_multiclass=True`, otherwise it is a :class:`pd.Series`.\n",
      " |          If the output is a :class:`pd.DataFrame`, the column order will be equivalent to `predictor.classes_`.\n",
      " |      as_multiclass : bool, default = True\n",
      " |          Whether to return binary classification probabilities as if they were for multiclass classification.\n",
      " |              Output will contain two columns, and if `as_pandas=True`, the column names will correspond to the binary class labels.\n",
      " |              The columns will be the same order as `predictor.class_labels`.\n",
      " |          If False, output will contain only 1 column for the positive class (get positive_class name via `predictor.positive_class`).\n",
      " |          Only impacts output for binary classification problems.\n",
      " |      transform_features : bool, default = True\n",
      " |          If True, preprocesses data before predicting with models.\n",
      " |          If False, skips global feature preprocessing.\n",
      " |              This is useful to save on inference time if you have already called `data = predictor.transform_features(data)`.\n",
      " |      inverse_transform : bool, default = True\n",
      " |          If True, will return prediction probabilities in the original format.\n",
      " |          If False (advanced), will return prediction probabilities in AutoGluon's internal format.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          Dictionary with model names as keys and model prediction probabilities as values.\n",
      " |  \n",
      " |  predict_proba_oof(self, model: 'str' = None, *, transformed=False, as_multiclass=True, train_data=None, internal_oof=False, can_infer=None) -> 'pd.DataFrame | pd.Series'\n",
      " |      Note: This is advanced functionality not intended for normal usage.\n",
      " |      \n",
      " |      Returns the out-of-fold (OOF) predicted class probabilities for every row in the training data.\n",
      " |      OOF prediction probabilities may provide unbiased estimates of generalization accuracy (reflecting how predictions will behave on new data)\n",
      " |      Predictions for each row are only made using models that were fit to a subset of data where this row was held-out.\n",
      " |      \n",
      " |      For a similar method, refer to :meth:`TabularPredictor.predict_proba_multi` with `data=None`.\n",
      " |      \n",
      " |      Warning: This method will raise an exception if called on a model that is not a bagged ensemble. Only bagged models (such a stacker models) can produce OOF predictions.\n",
      " |          This also means that refit_full models and distilled models will raise an exception.\n",
      " |      Warning: If intending to join the output of this method with the original training data, be aware that a rare edge-case issue exists:\n",
      " |          Multiclass problems with rare classes combined with the use of the 'log_loss' eval_metric may have forced AutoGluon to duplicate rows in the training data to satisfy minimum class counts in the data.\n",
      " |          If this has occurred, then the indices and row counts of the returned :class:`pd.Series` in this method may not align with the training data.\n",
      " |          In this case, consider fetching the processed training data using `predictor.load_data_internal()` instead of using the original training data.\n",
      " |          A more benign version of this issue occurs when 'log_loss' wasn't specified as the eval_metric but rare classes were dropped by AutoGluon.\n",
      " |          In this case, not all original training data rows will have an OOF prediction. It is recommended to either drop these rows during the join or to get direct predictions on the missing rows via :meth:`TabularPredictor.predict_proba`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : str (optional)\n",
      " |          The name of the model to get out-of-fold predictions from. Defaults to None, which uses the highest scoring model on the validation set.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`\n",
      " |      transformed : bool, default = False\n",
      " |          Whether the output values should be of the original label representation (False) or the internal label representation (True).\n",
      " |          The internal representation for binary and multiclass classification are integers numbering the k possible classes from 0 to k-1, while the original representation is identical to the label classes provided during fit.\n",
      " |          Generally, most users will want the original representation and keep `transformed=False`.\n",
      " |      as_multiclass : bool, default = True\n",
      " |          Whether to return binary classification probabilities as if they were for multiclass classification.\n",
      " |              Output will contain two columns, and if `transformed=False`, the column names will correspond to the binary class labels.\n",
      " |              The columns will be the same order as `predictor.class_labels`.\n",
      " |          If False, output will contain only 1 column for the positive class (get positive_class name via `predictor.positive_class`).\n",
      " |          Only impacts output for binary classification problems.\n",
      " |      train_data : pd.DataFrame, default = None\n",
      " |          Specify the original `train_data` to ensure that any training rows that were originally dropped internally are properly handled.\n",
      " |          If None, then output will not contain all rows if training rows were dropped internally during fit.\n",
      " |          If `train_data` is specified and `model` is unable to predict and rows were dropped internally, an exception will be raised.\n",
      " |      internal_oof : bool, default = False\n",
      " |          [Advanced Option] Return the internal OOF preds rather than the externally facing OOF preds.\n",
      " |          Internal OOF preds may have more/fewer rows than was provided in train_data, and are incompatible with external data.\n",
      " |          If you don't know what this does, keep it as False.\n",
      " |      can_infer : bool, default = None\n",
      " |          Only used if `model` is not specified.\n",
      " |          This is used to determine if the best model must be one that is able to predict on new data (True).\n",
      " |          If None, the best model does not need to be able to infer on new data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pd.Series` or :class:`pd.DataFrame` object of the out-of-fold training prediction probabilities of the model.\n",
      " |  \n",
      " |  refit_full(self, model: 'str | List[str]' = 'all', set_best_to_refit_full: 'bool' = True, train_data_extra: 'pd.DataFrame' = None, num_cpus: 'int | str' = 'auto', num_gpus: 'int | str' = 'auto', fit_strategy: \"Literal['auto', 'sequential', 'parallel']\" = 'auto', **kwargs) -> 'Dict[str, str]'\n",
      " |      Retrain model on all of the data (training + validation).\n",
      " |      For bagged models:\n",
      " |          Optimizes a model's inference time by collapsing bagged ensembles into a single model fit on all of the training data.\n",
      " |          This process will typically result in a slight accuracy reduction and a large inference speedup.\n",
      " |          The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.\n",
      " |              The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bag_folds`) and n is the number of finished repeats (`num_bag_sets`) in the bagged ensemble.\n",
      " |          The runtime is generally 10% or less of the original fit runtime.\n",
      " |              The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.\n",
      " |      For non-bagged models:\n",
      " |          Optimizes a model's accuracy by retraining on 100% of the data without using a validation set.\n",
      " |          Will typically result in a slight accuracy increase and no change to inference time.\n",
      " |          The runtime will be approximately equal to the original fit runtime.\n",
      " |      This process does not alter the original models, but instead adds additional models.\n",
      " |      If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.\n",
      " |      Models produced by this process will not have validation scores, as they use all of the data for training.\n",
      " |          Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(test_data)`.\n",
      " |          If the user does not have additional test data, they should reference the original model's score for an estimate of the performance of the refit_full model.\n",
      " |              Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.\n",
      " |      `cache_data` must have been set to `True` during the original training to enable this functionality.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : str | List[str], default = 'all'\n",
      " |          Model name of model(s) to refit.\n",
      " |              If 'all' then all models are refitted.\n",
      " |              If 'best' then the model with the highest validation score is refit.\n",
      " |          All ancestor models will also be refit in the case that the selected model is a weighted or stacker ensemble.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |      set_best_to_refit_full : bool | str, default = True\n",
      " |          If True, sets best model to the refit_full version of the prior best model.\n",
      " |          This means the model used when `predictor.predict(data)` is called will be the refit_full version instead of the original version of the model.\n",
      " |          Ignored if `model` is not the best model.\n",
      " |          If str, interprets as a model name and sets best model to the refit_full version of the model `set_best_to_refit_full`.\n",
      " |      train_data_extra : pd.DataFrame, default = None\n",
      " |          If specified, will be used as additional rows of training data when refitting models.\n",
      " |          Requires label column. Will only be used for L1 models.\n",
      " |      num_cpus: int | str, default = \"auto\"\n",
      " |          The total amount of cpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of cpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      num_gpus: int | str, default = \"auto\"\n",
      " |          The total amount of gpus you want AutoGluon predictor to use.\n",
      " |          Auto means AutoGluon will make the decision based on the total number of gpus available and the model requirement for best performance.\n",
      " |          Users generally don't need to set this value\n",
      " |      fit_strategy: Literal[\"auto\", \"sequential\", \"parallel\"], default = \"auto\"\n",
      " |          The strategy used to fit models.\n",
      " |          If \"auto\", uses the same fit_strategy as used in the original :meth:`TabularPredictor.fit` call.\n",
      " |          If \"sequential\", models will be fit sequentially. This is the most stable option with the most readable logging.\n",
      " |          If \"parallel\", models will be fit in parallel with ray, splitting available compute between them.\n",
      " |              Note: \"parallel\" is experimental and may run into issues. It was first added in version 1.2.0.\n",
      " |          For machines with 16 or more CPU cores, it is likely that \"parallel\" will be faster than \"sequential\".\n",
      " |      \n",
      " |          .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      **kwargs\n",
      " |          [Advanced] Developer debugging arguments.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dictionary of original model names -> refit_full model names.\n",
      " |  \n",
      " |  save(self, silent: 'bool' = False)\n",
      " |      Save this Predictor to file in directory specified by this Predictor's `path`.\n",
      " |      Note that :meth:`TabularPredictor.fit` already saves the predictor object automatically\n",
      " |      (we do not recommend modifying the Predictor object yourself as it tracks many trained models).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      silent : bool, default = False\n",
      " |          Whether to save without logging a message.\n",
      " |  \n",
      " |  save_space(self, remove_data=True, remove_fit_stack=True, requires_save=True, reduce_children=False)\n",
      " |      Reduces the memory and disk size of predictor by deleting auxiliary model files that aren't needed for prediction on new data.\n",
      " |      This function has NO impact on inference accuracy.\n",
      " |      It is recommended to invoke this method if the only goal is to use the trained model for prediction.\n",
      " |      However, certain advanced functionality may no longer be available after `save_space()` has been called.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      remove_data : bool, default = True\n",
      " |          Whether to remove cached files of the original training and validation data.\n",
      " |          Only reduces disk usage, it has no impact on memory usage.\n",
      " |          This is especially useful when the original data was large.\n",
      " |          This is equivalent to setting `cache_data=False` during the original `fit()`.\n",
      " |              Will disable all advanced functionality that requires `cache_data=True`.\n",
      " |      remove_fit_stack : bool, default = True\n",
      " |          Whether to remove information required to fit new stacking models and continue fitting bagged models with new folds.\n",
      " |          Only reduces disk usage, it has no impact on memory usage.\n",
      " |          This includes:\n",
      " |              out-of-fold (OOF) predictions\n",
      " |          This is useful for multiclass problems with many classes, as OOF predictions can become very large on disk. (1 GB per model in extreme cases)\n",
      " |          This disables `predictor.refit_full()` for stacker models.\n",
      " |      requires_save : bool, default = True\n",
      " |          Whether to remove information that requires the model to be saved again to disk.\n",
      " |          Typically this only includes flag variables that don't have significant impact on memory or disk usage, but should technically be updated due to the removal of more important information.\n",
      " |              An example is the `is_data_saved` boolean variable in `trainer`, which should be updated to `False` if `remove_data=True` was set.\n",
      " |      reduce_children : bool, default = False\n",
      " |          Whether to apply the reduction rules to bagged ensemble children models. These are the models trained for each fold of the bagged ensemble.\n",
      " |          This should generally be kept as `False` since the most important memory and disk reduction techniques are automatically applied to these models during the original `fit()` call.\n",
      " |  \n",
      " |  set_decision_threshold(self, decision_threshold: 'float')\n",
      " |      Set `predictor.decision_threshold`. Problem type must be 'binary', and the value must be between 0 and 1.\n",
      " |  \n",
      " |  set_model_best(self, model: 'str', save_trainer: 'bool' = False)\n",
      " |      Sets the model to be used by default when calling `predictor.predict(data)`.\n",
      " |      By default, this is the model with the best validation score, but this is not always the case.\n",
      " |      If manually set, this can be overwritten internally if further training occurs, such as through fit_extra, refit_full, or distill.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : str\n",
      " |          Name of model to set to best. If model does not exist or cannot infer, raises an AssertionError.\n",
      " |      save_trainer : bool, default = False\n",
      " |          If True, self._trainer is saved with the new model_best value, such that it is reflected when predictor is loaded in future from disk.\n",
      " |  \n",
      " |  simulation_artifact(self, test_data: 'pd.DataFrame' = None) -> 'dict'\n",
      " |      [Advanced] Computes and returns the necessary information to perform zeroshot HPO simulation.\n",
      " |      For a usage example, refer to https://github.com/autogluon/tabrepo/blob/main/examples/run_quickstart_from_scratch.py\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      test_data: pd.DataFrame, default = None\n",
      " |          The test data to predict with.\n",
      " |          If None, the keys `pred_proba_dict_test` and `y_test` will not be present in the output.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      simulation_dict: dict\n",
      " |          The dictionary of information required for zeroshot HPO simulation.\n",
      " |          Keys are as follows:\n",
      " |              pred_proba_dict_val: Dictionary of model name to prediction probabilities (or predictions if regression) on the validation data\n",
      " |              pred_proba_dict_test: Dictionary of model name to prediction probabilities (or predictions if regression) on the test data\n",
      " |              y_val: Pandas Series of ground truth labels for the validation data (internal representation)\n",
      " |              y_test: Pandas Series of ground truth labels for the test data (internal representation)\n",
      " |              eval_metric: The string name of the evaluation metric (obtained via `predictor.eval_metric.name`)\n",
      " |              problem_type: The problem type (obtained via `predictor.problem_type`)\n",
      " |              problem_type_transform: The transformed (internal) problem type (obtained via `predictor._learner.label_cleaner.problem_type_transform,`)\n",
      " |              ordered_class_labels: The original class labels (`predictor._learner.label_cleaner.ordered_class_labels`)\n",
      " |              ordered_Class_labels_transformed: The transformed (internal) class labels (`predictor._learner.label_cleaner.ordered_class_labels_transformed`)\n",
      " |              num_classes: The number of internal classes (`self._learner.label_cleaner.num_classes`)\n",
      " |              label: The label column name (`predictor.label`)\n",
      " |  \n",
      " |  transform_features(self, data: 'pd.DataFrame | str' = None, model: 'str' = None, base_models: 'list[str]' = None, return_original_features: 'bool' = True) -> 'pd.DataFrame'\n",
      " |      Transforms data features through the AutoGluon feature generator.\n",
      " |      This is useful to gain an understanding of how AutoGluon interprets the data features.\n",
      " |      The output of this function can be used to train further models, even outside of AutoGluon.\n",
      " |      This can be useful for training your own models on the same data representation as AutoGluon.\n",
      " |      Individual AutoGluon models like the neural network may apply additional feature transformations that are not reflected in this method.\n",
      " |      This method only applies universal transforms employed by all AutoGluon models.\n",
      " |      When `data=None`, `base_models=[{best_model}], and bagging was enabled during fit():\n",
      " |          This returns the out-of-fold predictions of the best model, which can be used as training input to a custom user stacker model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data: :class:`pd.DataFrame` or str (optional)\n",
      " |          The data to apply feature transformation to.\n",
      " |          This data does not require the label column.\n",
      " |          If str is passed, `data` will be loaded using the str value as the file path.\n",
      " |          If not specified, the original data used during fit() will be used if fit() was previously called with `cache_data=True`. Otherwise, an exception will be raised.\n",
      " |              For non-bagged mode predictors:\n",
      " |                  The data used when not specified is the validation set.\n",
      " |                  This can either be an automatically generated validation set or the user-defined `tuning_data` if passed during fit().\n",
      " |                  If all parameters are unspecified, then the output is equivalent to `predictor.load_data_internal(data='val', return_X=True, return_y=False)[0]`.\n",
      " |                  To get the label values of the output, call `predictor.load_data_internal(data='val', return_X=False, return_y=True)[1]`.\n",
      " |                  If the original training set is desired, it can be passed in through `data`.\n",
      " |                      Warning: Do not pass the original training set if `model` or `base_models` are set. This will result in overfit feature transformation.\n",
      " |              For bagged mode predictors:\n",
      " |                  The data used when not specified is the full training set.\n",
      " |                  If all parameters are unspecified, then the output is equivalent to `predictor.load_data_internal(data='train', return_X=True, return_y=False)[0]`.\n",
      " |                  To get the label values of the output, call `predictor.load_data_internal(data='train', return_X=False, return_y=True)[1]`.\n",
      " |                  `base_model` features generated in this instance will be from out-of-fold predictions.\n",
      " |                  Note that the training set may differ from the training set originally passed during fit(), as AutoGluon may choose to drop or duplicate rows during training.\n",
      " |                  Warning: Do not pass the original training set through `data` if `model` or `base_models` are set. This will result in overfit feature transformation. Instead set `data=None`.\n",
      " |      model: str, default = None\n",
      " |          Model to generate input features for.\n",
      " |          The output data will be equivalent to the input data that would be sent into `model.predict_proba(data)`.\n",
      " |              Note: This only applies to cases where `data` is not the training data.\n",
      " |          If `None`, then only return generically preprocessed features prior to any model fitting.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |          Specifying a `refit_full` model will cause an exception if `data=None`.\n",
      " |          `base_models=None` is a requirement when specifying `model`.\n",
      " |      base_models: List[str], default = None\n",
      " |          List of model names to use as base_models for a hypothetical stacker model when generating input features.\n",
      " |          If `None`, then only return generically preprocessed features prior to any model fitting.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names()`.\n",
      " |          If a stacker model S exists with `base_models=M`, then setting `base_models=M` is equivalent to setting `model=S`.\n",
      " |          `model=None` is a requirement when specifying `base_models`.\n",
      " |      return_original_features: bool, default = True\n",
      " |          Whether to return the original features.\n",
      " |          If False, only returns the additional output columns from specifying `model` or `base_models`.\n",
      " |              This is useful to set to False if the intent is to use the output as input to further stacker models without the original features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pd.DataFrame` of the provided `data` after feature transformation has been applied.\n",
      " |      This output does not include the label column, and will remove it if present in the supplied `data`.\n",
      " |      If a transformed label column is desired, use `predictor.transform_labels`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from autogluon.tabular import TabularPredictor\n",
      " |      >>> predictor = TabularPredictor(label='class').fit('train.csv', label='class', auto_stack=True)  # predictor is in bagged mode.\n",
      " |      >>> model = 'WeightedEnsemble_L2'\n",
      " |      >>> train_data_transformed = predictor.transform_features(model=model)  # Internal training DataFrame used as input to `model.fit()` for each model trained in predictor.fit()`\n",
      " |      >>> test_data_transformed = predictor.transform_features('test.csv', model=model)  # Internal test DataFrame used as input to `model.predict_proba()` during `predictor.predict_proba(test_data, model=model)`\n",
      " |  \n",
      " |  transform_labels(self, labels: 'np.ndarray | pd.Series', inverse: 'bool' = False, proba: 'bool' = False) -> 'pd.Series | pd.DataFrame'\n",
      " |      Transforms data labels to the internal label representation.\n",
      " |      This can be useful for training your own models on the same data label representation as AutoGluon.\n",
      " |      Regression problems do not differ between original and internal representation, and thus this method will return the provided labels.\n",
      " |      Warning: When `inverse=False`, it is possible for the output to contain NaN label values in multiclass problems if the provided label was dropped during training.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels: :class:`np.ndarray` or :class:`pd.Series`\n",
      " |          Labels to transform.\n",
      " |          If `proba=False`, an example input would be the output of `predictor.predict(test_data)`.\n",
      " |          If `proba=True`, an example input would be the output of `predictor.predict_proba(test_data, as_multiclass=False)`.\n",
      " |      inverse: bool, default = False\n",
      " |          When `True`, the input labels are treated as being in the internal representation and the original representation is outputted.\n",
      " |      proba: bool, default = False\n",
      " |          When `True`, the input labels are treated as probabilities and the output will be the internal representation of probabilities.\n",
      " |              In this case, it is expected that `labels` be a :class:`pd.DataFrame` or :class:`np.ndarray`.\n",
      " |              If the `problem_type` is multiclass:\n",
      " |                  The input column order must be equal to `predictor.class_labels`.\n",
      " |                  The output column order will be equal to `predictor.class_labels_internal`.\n",
      " |                  if `inverse=True`, the same logic applies, but with input and output columns interchanged.\n",
      " |          When `False`, the input labels are treated as actual labels and the output will be the internal representation of the labels.\n",
      " |              In this case, it is expected that `labels` be a :class:`pd.Series` or :class:`np.ndarray`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`pd.Series` of labels if `proba=False` or :class:`pd.DataFrame` of label probabilities if `proba=True`.\n",
      " |  \n",
      " |  unpersist(self, models='all') -> 'List[str]'\n",
      " |      Unpersist models in memory for reduced memory usage.\n",
      " |      If models are not persisted in memory, they are loaded from disk every time they are asked to make predictions.\n",
      " |      Note: Another way to reset the predictor and unpersist models is to reload the predictor from disk via `predictor = TabularPredictor.load(predictor.path)`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      models : list of str or str, default = 'all'\n",
      " |          Model names of models to unpersist.\n",
      " |          If 'all' then all models are unpersisted.\n",
      " |          Valid models are listed in this `predictor` by calling `predictor.model_names(persisted=True)`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List of unpersisted model names.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(path: 'str', verbosity: 'int' = None, require_version_match: 'bool' = True, require_py_version_match: 'bool' = True, check_packages: 'bool' = False) -> \"'TabularPredictor'\"\n",
      " |      Load a TabularPredictor object previously produced by `fit()` from file and returns this object. It is highly recommended the predictor be loaded with the exact AutoGluon version it was fit with.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :meth:`autogluon.tabular.TabularPredictor.load` uses `pickle` module implicitly, which is known to\n",
      " |          be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during\n",
      " |          unpickling. Never load data that could have come from an untrusted source, or that could have been tampered\n",
      " |          with. **Only load data you trust.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          The path to directory in which this Predictor was previously saved.\n",
      " |      verbosity : int, default = None\n",
      " |          Sets the verbosity level of this Predictor after it is loaded.\n",
      " |          Valid values range from 0 (least verbose) to 4 (most verbose).\n",
      " |          If None, logging verbosity is not changed from existing values.\n",
      " |          Specify larger values to see more information printed when using Predictor during inference, smaller values to see less information.\n",
      " |          Refer to TabularPredictor init for more information.\n",
      " |      require_version_match : bool, default = True\n",
      " |          If True, will raise an AssertionError if the `autogluon.tabular` version of the loaded predictor does not match the installed version of `autogluon.tabular`.\n",
      " |          If False, will allow loading of models trained on incompatible versions, but is NOT recommended. Users may run into numerous issues if attempting this.\n",
      " |      require_py_version_match : bool, default = True\n",
      " |          If True, will raise an AssertionError if the Python version of the loaded predictor does not match the installed Python version.\n",
      " |              Micro version differences such as 3.9.2 and 3.9.7 will log a warning but will not raise an exception.\n",
      " |          If False, will allow loading of models trained on incompatible python versions, but is NOT recommended. Users may run into numerous issues if attempting this.\n",
      " |      check_packages : bool, default = False\n",
      " |          If True, checks package versions of the loaded predictor against the package versions of the current environment.\n",
      " |          Warnings will be logged for each mismatch of package version.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      predictor : TabularPredictor\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> predictor = TabularPredictor.load(path_to_predictor)\n",
      " |  \n",
      " |  load_log(predictor_path: 'str' = None, log_file_path: 'Optional[str]' = None) -> 'List[str]'\n",
      " |      Load log files of a predictor\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      predictor_path: Optional[str], default = None\n",
      " |          Path to the predictor to load the log.\n",
      " |          This can be used when the predictor was initialized with `log_file_path=\"auto\"` to fetch the log file automatically\n",
      " |      log_file_path: Optional[str], default = None\n",
      " |          Path to the log file.\n",
      " |          If you specified a `log_file_path` while initializing the predictor, you should use `log_file_path` to load the log file instead.\n",
      " |          At least one of `predictor_path` or `log_file_path` must to be specified\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List[str]\n",
      " |          A list containing lines of the log file\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  can_predict_proba\n",
      " |      Return True if predictor can return prediction probabilities via `.predict_proba`, otherwise return False.\n",
      " |      Raises an AssertionError if called before fitting.\n",
      " |  \n",
      " |  class_labels\n",
      " |      Alias to self.classes_\n",
      " |  \n",
      " |  class_labels_internal\n",
      " |      For multiclass problems, this list contains the internal class labels in sorted order of internal `predict_proba()` output.\n",
      " |      For binary problems, this list contains the internal class labels in sorted order of internal `predict_proba(as_multiclass=True)` output.\n",
      " |          The value will always be `class_labels_internal=[0, 1]` for binary problems, with 0 as the negative class, and 1 as the positive class.\n",
      " |      For other problem types, will equal None.\n",
      " |  \n",
      " |  class_labels_internal_map\n",
      " |      For binary and multiclass classification problems, this dictionary contains the mapping of the original labels to the internal labels.\n",
      " |      For example, in binary classification, label values of 'True' and 'False' will be mapped to the internal representation `1` and `0`.\n",
      " |          Therefore, class_labels_internal_map would equal {'True': 1, 'False': 0}\n",
      " |      For other problem types, will equal None.\n",
      " |      For multiclass, it is possible for not all of the label values to have a mapping.\n",
      " |          This indicates that the internal models will never predict those missing labels, and training rows associated with the missing labels were dropped.\n",
      " |  \n",
      " |  classes_\n",
      " |      For multiclass problems, this list contains the class labels in sorted order of `predict_proba()` output.\n",
      " |      For binary problems, this list contains the class labels in sorted order of `predict_proba(as_multiclass=True)` output.\n",
      " |          `classes_[0]` corresponds to internal label = 0 (negative class), `classes_[1]` corresponds to internal label = 1 (positive class).\n",
      " |          This is relevant for certain metrics such as F1 where True and False labels impact the metric score differently.\n",
      " |      For other problem types, will equal None.\n",
      " |      For example if `pred = predict_proba(x, as_multiclass=True)`, then ith index of `pred` provides predicted probability that `x` belongs to class given by `classes_[i]`.\n",
      " |  \n",
      " |  decision_threshold\n",
      " |      The decision threshold used to convert prediction probabilities to predictions.\n",
      " |      Only relevant for binary classification, otherwise the value will be None.\n",
      " |      Valid values are in the range [0.0, 1.0]\n",
      " |      You can obtain an optimized `decision_threshold` by first calling `predictor.calibrate_decision_threshold()`.\n",
      " |      Useful to set for metrics such as `balanced_accuracy` and `f1` as `0.5` is often not an optimal threshold.\n",
      " |      Predictions are calculated via the following logic on the positive class: `1 if pred > decision_threshold else 0`\n",
      " |  \n",
      " |  eval_metric\n",
      " |      The metric used to evaluate predictive performance\n",
      " |  \n",
      " |  feature_metadata\n",
      " |      Returns the internal FeatureMetadata.\n",
      " |      \n",
      " |      Inferred data type of each predictive variable after preprocessing transformation (i.e. column of training data table used to predict `label`).\n",
      " |      Contains both raw dtype and special dtype information. Each feature has exactly 1 raw dtype (such as 'int', 'float', 'category') and zero to many special dtypes (such as 'datetime_as_int', 'text', 'text_ngram').\n",
      " |      Special dtypes are AutoGluon specific feature types that are used to identify features with meaning beyond what the raw dtype can convey.\n",
      " |          `feature_metadata.type_map_raw`: Dictionary of feature name -> raw dtype mappings.\n",
      " |          `feature_metadata.type_group_map_special`: Dictionary of lists of special feature names, grouped by special feature dtype.\n",
      " |  \n",
      " |  feature_metadata_in\n",
      " |      Returns the input FeatureMetadata.\n",
      " |      \n",
      " |      Inferred data type of each predictive variable before preprocessing transformation.\n",
      " |      Contains both raw dtype and special dtype information. Each feature has exactly 1 raw dtype (such as 'int', 'float', 'category') and zero to many special dtypes (such as 'datetime_as_int', 'text', 'text_ngram').\n",
      " |      Special dtypes are AutoGluon specific feature types that are used to identify features with meaning beyond what the raw dtype can convey.\n",
      " |          `feature_metadata.type_map_raw`: Dictionary of feature name -> raw dtype mappings.\n",
      " |          `feature_metadata.type_group_map_special`: Dictionary of lists of special feature names, grouped by special feature dtype.\n",
      " |  \n",
      " |  has_val\n",
      " |      Return True if holdout validation data was used during fit, else return False.\n",
      " |  \n",
      " |  is_fit\n",
      " |      Return True if `predictor.fit` has been called, otherwise return False.\n",
      " |  \n",
      " |  label\n",
      " |      Name of table column that contains data from the variable to predict (often referred to as: labels, response variable, target variable, dependent variable, y, etc).\n",
      " |  \n",
      " |  model_best\n",
      " |      Returns the string model name of the best model by validation score that can infer.\n",
      " |      This is the same model used during inference when `predictor.predict` is called without specifying a model.\n",
      " |      This can be updated to be a model other than the model with best validation score by methods such as refit_full and set_model_best.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      String model name of the best model\n",
      " |  \n",
      " |  original_features\n",
      " |      Original features user passed in to fit before processing\n",
      " |  \n",
      " |  path\n",
      " |      Path to directory where all models used by this Predictor are stored\n",
      " |  \n",
      " |  positive_class\n",
      " |      Returns the positive class name in binary classification. Useful for computing metrics such as F1 which require a positive and negative class.\n",
      " |      In binary classification, :class:`TabularPredictor.predict_proba(as_multiclass=False)` returns the estimated probability that each row belongs to the positive class.\n",
      " |      Will print a warning and return None if called when `predictor.problem_type != 'binary'`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The positive class name in binary classification or None if the problem is not binary classification.\n",
      " |  \n",
      " |  problem_type\n",
      " |      What type of prediction problem this Predictor has been trained for\n",
      " |  \n",
      " |  quantile_levels\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Dataset = <class 'autogluon.common.dataset.TabularDataset'>\n",
      " |      A dataset in tabular format (with rows = samples, columns = features/variables).\n",
      " |      This class returns a :class:`pd.DataFrame` when initialized and all existing pandas methods can be applied to it.\n",
      " |      For full list of methods/attributes, see pandas Dataframe documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
      " |      \n",
      " |      The purpose of this class is to provide an easy-to-use shorthand for loading a pandas DataFrame to use in AutoGluon.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : str, :class:`pd.DataFrame`, :class:`np.ndarray`, Iterable, or dict\n",
      " |          If str, path to data file (CSV or Parquet format).\n",
      " |          If you already have your data in a :class:`pd.DataFrame`, you can specify it here. In this case, the same DataFrame will be returned with no changes.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pandas as pd\n",
      " |      >>> from autogluon.common import TabularDataset\n",
      " |      >>> train_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\")\n",
      " |      >>> train_data_pd = pd.read_csv(\"https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\")\n",
      " |      >>> assert isinstance(train_data, pd.DataFrame)  # True\n",
      " |      >>> assert train_data.equals(train_data_pd)  # True\n",
      " |      >>> assert type(train_data) == type(train_data_pd)  # True\n",
      " |  \n",
      " |  \n",
      " |  predictor_file_name = 'predictor.pkl'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from autogluon.tabular.predictor._deprecated_methods.TabularPredictorDeprecatedMixin:\n",
      " |  \n",
      " |  get_model_best(self) -> 'str'\n",
      " |      Deprecated method. Use `model_best` instead.\n",
      " |  \n",
      " |  get_model_full_dict(self, *args, **kwargs) -> 'Dict[str, str]'\n",
      " |      Deprecated method. Use `model_refit_map` instead.\n",
      " |  \n",
      " |  get_model_names(self, *args, **kwargs) -> 'List[str]'\n",
      " |      Deprecated method. Use `model_names` instead.\n",
      " |  \n",
      " |  get_model_names_persisted(self) -> 'List[str]'\n",
      " |      Deprecated method. Use `model_names(persisted=True)` instead.\n",
      " |  \n",
      " |  get_oof_pred(self, *args, **kwargs) -> 'pd.Series'\n",
      " |      Deprecated method. Use `predict_oof` instead.\n",
      " |  \n",
      " |  get_oof_pred_proba(self, *args, **kwargs) -> 'pd.DataFrame | pd.Series'\n",
      " |      Deprecated method. Use `predict_proba_oof` instead.\n",
      " |  \n",
      " |  get_pred_from_proba(self, *args, **kwargs) -> 'pd.Series | np.array'\n",
      " |      Deprecated method. Use `predict_from_proba` instead.\n",
      " |  \n",
      " |  get_size_disk(self) -> 'int'\n",
      " |      Deprecated method. Use `disk_usage` instead.\n",
      " |  \n",
      " |  get_size_disk_per_file(self, *args, **kwargs) -> 'pd.Series'\n",
      " |      Deprecated method. Use `disk_usage_per_file` instead.\n",
      " |  \n",
      " |  persist_models(self, *args, **kwargs) -> 'List[str]'\n",
      " |      Deprecated method. Use `persist` instead.\n",
      " |  \n",
      " |  unpersist_models(self, *args, **kwargs) -> 'List[str]'\n",
      " |      Deprecated method. Use `unpersist` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from autogluon.tabular.predictor._deprecated_methods.TabularPredictorDeprecatedMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(TabularPredictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b070d740-72ec-465d-b349-a08bc1e27244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module autogluon.tabular.predictor.predictor:\n",
      "\n",
      "fit(self, train_data: 'pd.DataFrame | str', tuning_data: 'pd.DataFrame | str' = None, time_limit: 'float' = None, presets: 'List[str] | str' = None, hyperparameters: 'dict | str' = None, feature_metadata='infer', infer_limit: 'float' = None, infer_limit_batch_size: 'int' = None, fit_weighted_ensemble: 'bool' = True, fit_full_last_level_weighted_ensemble: 'bool' = True, full_weighted_ensemble_additionally: 'bool' = False, dynamic_stacking: 'bool | str' = False, calibrate_decision_threshold: 'bool | str' = 'auto', num_cpus: 'int | str' = 'auto', num_gpus: 'int | str' = 'auto', fit_strategy: \"Literal['sequential', 'parallel']\" = 'sequential', memory_limit: 'float | str' = 'auto', callbacks: 'List[AbstractCallback]' = None, **kwargs) -> \"'TabularPredictor'\"\n",
      "    Fit models to predict a column of a data table (label) based on the other columns (features).\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    train_data : :class:`pd.DataFrame` or str\n",
      "        Table of the training data as a pandas DataFrame.\n",
      "        If str is passed, `train_data` will be loaded using the str value as the file path.\n",
      "    tuning_data : :class:`pd.DataFrame` or str, optional\n",
      "        Another dataset containing validation data reserved for tuning processes such as early stopping and hyperparameter tuning.\n",
      "        This dataset should be in the same format as `train_data`.\n",
      "        If str is passed, `tuning_data` will be loaded using the str value as the file path.\n",
      "        Note: final model returned may be fit on `tuning_data` as well as `train_data`. Do not provide your evaluation test data here!\n",
      "        In particular, when `num_bag_folds` > 0 or `num_stack_levels` > 0, models will be trained on both `tuning_data` and `train_data`.\n",
      "        If `tuning_data = None`, `fit()` will automatically hold out some random validation examples from `train_data`.\n",
      "    time_limit : int, default = None\n",
      "        Approximately how long `fit()` should run for (wallclock time in seconds).\n",
      "        If not specified, `fit()` will run until all models have completed training, but will not repeatedly bag models unless `num_bag_sets` is specified.\n",
      "    presets : list or str or dict, default = ['medium_quality']\n",
      "        List of preset configurations for various arguments in `fit()`. Can significantly impact predictive accuracy, memory-footprint, and inference latency of trained models, and various other properties of the returned `predictor`.\n",
      "        It is recommended to specify presets and avoid specifying most other `fit()` arguments or model hyperparameters prior to becoming familiar with AutoGluon.\n",
      "        As an example, to get the most accurate overall predictor (regardless of its efficiency), set `presets='best_quality'`.\n",
      "        To get good quality with minimal disk usage, set `presets=['good_quality', 'optimize_for_deployment']`\n",
      "        Any user-specified arguments in `fit()` will override the values used by presets.\n",
      "        If specifying a list of presets, later presets will override earlier presets if they alter the same argument.\n",
      "        For precise definitions of the provided presets, see file: `autogluon/tabular/configs/presets_configs.py`.\n",
      "        Users can specify custom presets by passing in a dictionary of argument values as an element to the list.\n",
      "    \n",
      "        Available Presets: ['best_quality', 'high_quality', 'good_quality', 'medium_quality', 'experimental_quality', 'optimize_for_deployment', 'interpretable', 'ignore_text']\n",
      "    \n",
      "        It is recommended to only use one `quality` based preset in a given call to `fit()` as they alter many of the same arguments and are not compatible with each-other.\n",
      "    \n",
      "        In-depth Preset Info:\n",
      "            best_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'zeroshot'}\n",
      "                Best predictive accuracy with little consideration to inference time or disk usage. Achieve even better results by specifying a large time_limit value.\n",
      "                Recommended for applications that benefit from the best possible model accuracy.\n",
      "    \n",
      "            high_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'zeroshot', 'refit_full': True, 'set_best_to_refit_full': True, 'save_bag_folds': False}\n",
      "                High predictive accuracy with fast inference. ~8x faster inference and ~8x lower disk usage than `best_quality`.\n",
      "                Recommended for applications that require reasonable inference speed and/or model size.\n",
      "    \n",
      "            good_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'light', 'refit_full': True, 'set_best_to_refit_full': True, 'save_bag_folds': False}\n",
      "                Good predictive accuracy with very fast inference. ~4x faster inference and ~4x lower disk usage than `high_quality`.\n",
      "                Recommended for applications that require fast inference speed.\n",
      "    \n",
      "            medium_quality={'auto_stack': False}\n",
      "                Medium predictive accuracy with very fast inference and very fast training time. ~20x faster training than `good_quality`.\n",
      "                This is the default preset in AutoGluon, but should generally only be used for quick prototyping, as `good_quality` results in significantly better predictive accuracy and faster inference time.\n",
      "    \n",
      "            experimental_quality={'auto_stack': True, 'dynamic_stacking': 'auto', 'hyperparameters': 'experimental', 'fit_strategy': 'parallel', 'num_gpus': 0}\n",
      "                This preset acts as a testing ground for cutting edge features and models which could later be added to the `best_quality` preset in future releases.\n",
      "                Recommended when `best_quality` was already being used and the user wants to push performance even further.\n",
      "    \n",
      "            optimize_for_deployment={'keep_only_best': True, 'save_space': True}\n",
      "                Optimizes result immediately for deployment by deleting unused models and removing training artifacts.\n",
      "                Often can reduce disk usage by ~2-4x with no negatives to model accuracy or inference speed.\n",
      "                This will disable numerous advanced functionality, but has no impact on inference.\n",
      "                This will make certain functionality less informative, such as `predictor.leaderboard()` and `predictor.fit_summary()`.\n",
      "                    Because unused models will be deleted under this preset, methods like `predictor.leaderboard()` and `predictor.fit_summary()` will no longer show the full set of models that were trained during `fit()`.\n",
      "                Recommended for applications where the inner details of AutoGluon's training is not important and there is no intention of manually choosing between the final models.\n",
      "                This preset pairs well with the other presets such as `good_quality` to make a very compact final model.\n",
      "                Identical to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` and `predictor.save_space()` directly after `fit()`.\n",
      "    \n",
      "            interpretable={'auto_stack': False, 'hyperparameters': 'interpretable'}\n",
      "                Fits only interpretable rule-based models from the imodels package.\n",
      "                Trades off predictive accuracy for conciseness.\n",
      "    \n",
      "            ignore_text={'_feature_generator_kwargs': {'enable_text_ngram_features': False, 'enable_text_special_features': False, 'enable_raw_text_features': False}}\n",
      "                Disables automated feature generation when text features are detected.\n",
      "                This is useful to determine how beneficial text features are to the end result, as well as to ensure features are not mistaken for text when they are not.\n",
      "                Ignored if `feature_generator` was also specified.\n",
      "    \n",
      "    hyperparameters : str or dict, default = 'default'\n",
      "        Determines the hyperparameters used by the models.\n",
      "        If `str` is passed, will use a preset hyperparameter configuration.\n",
      "            Valid `str` options: ['default', 'zeroshot', 'light', 'very_light', 'toy', 'multimodal']\n",
      "                'default': Default AutoGluon hyperparameters intended to get strong accuracy with reasonable disk usage and inference time. Used in the 'medium_quality' preset.\n",
      "                'zeroshot': A powerful model portfolio learned from TabRepo's ensemble simulation on 200 datasets. Contains ~100 models and is used in 'best_quality' and 'high_quality' presets.\n",
      "                'light': Results in smaller models. Generally will make inference speed much faster and disk usage much lower, but with worse accuracy. Used in the 'good_quality' preset.\n",
      "                'very_light': Results in much smaller models. Behaves similarly to 'light', but in many cases with over 10x less disk usage and a further reduction in accuracy.\n",
      "                'toy': Results in extremely small models. Only use this when prototyping, as the model quality will be severely reduced.\n",
      "                'multimodal': [EXPERIMENTAL] Trains a multimodal transformer model alongside tabular models. Requires that some text columns appear in the data and GPU.\n",
      "                    When combined with 'best_quality' `presets` option, this can achieve extremely strong results in multimodal data tables that contain columns with text in addition to numeric/categorical columns.\n",
      "            Reference `autogluon/tabular/configs/hyperparameter_configs.py` for information on the hyperparameters associated with each preset.\n",
      "        Keys are strings that indicate which model types to train.\n",
      "            Stable model options include:\n",
      "                'GBM' (LightGBM)\n",
      "                'CAT' (CatBoost)\n",
      "                'XGB' (XGBoost)\n",
      "                'RF' (random forest)\n",
      "                'XT' (extremely randomized trees)\n",
      "                'KNN' (k-nearest neighbors)\n",
      "                'LR' (linear regression)\n",
      "                'NN_TORCH' (neural network implemented in Pytorch)\n",
      "                'FASTAI' (neural network with FastAI backend)\n",
      "                'AG_AUTOMM' (`MultimodalPredictor` from `autogluon.multimodal`. Supports Tabular, Text, and Image modalities. GPU is required.)\n",
      "            Experimental model options include:\n",
      "                'FT_TRANSFORMER' (Tabular Transformer, GPU is recommended. Does not scale well to >100 features.)\n",
      "                'FASTTEXT' (FastText. Note: Has not been tested for a long time.)\n",
      "                'TABPFN' (TabPFN. Does not scale well to >100 features or >1000 rows, and does not support regression. Extremely slow inference speed.)\n",
      "                'VW' (VowpalWabbit. Note: Has not been tested for a long time.)\n",
      "                'AG_TEXT_NN' (Multimodal Text+Tabular model, GPU is required. Recommended to instead use its successor, 'AG_AUTOMM'.)\n",
      "                'AG_IMAGE_NN' (Image model, GPU is required. Recommended to instead use its successor, 'AG_AUTOMM'.)\n",
      "            If a certain key is missing from hyperparameters, then `fit()` will not train any models of that type. Omitting a model key from hyperparameters is equivalent to including this model key in `excluded_model_types`.\n",
      "            For example, set `hyperparameters = { 'NN_TORCH':{...} }` if say you only want to train (PyTorch) neural networks and no other types of models.\n",
      "        Values = dict of hyperparameter settings for each model type, or list of dicts.\n",
      "            Each hyperparameter can either be a single fixed value or a search space containing many possible values.\n",
      "            Unspecified hyperparameters will be set to default values (or default search spaces if `hyperparameter_tune_kwargs='auto'`).\n",
      "            Caution: Any provided search spaces will error if `hyperparameter_tune_kwargs=None` (Default).\n",
      "            To train multiple models of a given type, set the value to a list of hyperparameter dictionaries.\n",
      "                For example, `hyperparameters = {'RF': [{'criterion': 'gini'}, {'criterion': 'entropy'}]}` will result in 2 random forest models being trained with separate hyperparameters.\n",
      "        Advanced functionality: Bring your own model / Custom model support\n",
      "            AutoGluon fully supports custom models. For a detailed tutorial on creating and using custom models with AutoGluon, refer to https://auto.gluon.ai/stable/tutorials/tabular/advanced/tabular-custom-model.html\n",
      "        Advanced functionality: Custom stack levels\n",
      "            By default, AutoGluon re-uses the same models and model hyperparameters at each level during stack ensembling.\n",
      "            To customize this behaviour, create a hyperparameters dictionary separately for each stack level, and then add them as values to a new dictionary, with keys equal to the stack level.\n",
      "                Example: `hyperparameters = {1: {'RF': rf_params1}, 2: {'CAT': [cat_params1, cat_params2], 'NN_TORCH': {}}}`\n",
      "                This will result in a stack ensemble that has one custom random forest in level 1 followed by two CatBoost models with custom hyperparameters and a default neural network in level 2, for a total of 4 models.\n",
      "            If a level is not specified in `hyperparameters`, it will default to using the highest specified level to train models. This can also be explicitly controlled by adding a 'default' key.\n",
      "    \n",
      "        Default:\n",
      "            hyperparameters = {\n",
      "                'NN_TORCH': {},\n",
      "                'GBM': [\n",
      "                    {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
      "                    {},\n",
      "                    {\n",
      "                        \"learning_rate\": 0.03,\n",
      "                        \"num_leaves\": 128,\n",
      "                        \"feature_fraction\": 0.9,\n",
      "                        \"min_data_in_leaf\": 3,\n",
      "                        \"ag_args\": {\"name_suffix\": \"Large\", \"priority\": 0, \"hyperparameter_tune_kwargs\": None},\n",
      "                    },\n",
      "                ],\n",
      "                'CAT': {},\n",
      "                'XGB': {},\n",
      "                'FASTAI': {},\n",
      "                'RF': [\n",
      "                    {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
      "                    {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
      "                    {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
      "                ],\n",
      "                'XT': [\n",
      "                    {'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n",
      "                    {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n",
      "                    {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n",
      "                ],\n",
      "                'KNN': [\n",
      "                    {'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}},\n",
      "                    {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}},\n",
      "                ],\n",
      "            }\n",
      "    \n",
      "        Details regarding the hyperparameters you can specify for each model are provided in the following files:\n",
      "            NN: `autogluon.tabular.models.tabular_nn.hyperparameters.parameters`\n",
      "                Note: certain hyperparameter settings may cause these neural networks to train much slower.\n",
      "            GBM: `autogluon.tabular.models.lgb.hyperparameters.parameters`\n",
      "                 See also the lightGBM docs: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
      "            CAT: `autogluon.tabular.models.catboost.hyperparameters.parameters`\n",
      "                 See also the CatBoost docs: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
      "            XGB: `autogluon.tabular.models.xgboost.hyperparameters.parameters`\n",
      "                 See also the XGBoost docs: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
      "            FASTAI: `autogluon.tabular.models.fastainn.hyperparameters.parameters`\n",
      "                 See also the FastAI docs: https://docs.fast.ai/tabular.learner.html\n",
      "            RF: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
      "                Note: Hyperparameter tuning is disabled for this model.\n",
      "            XT: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
      "                Note: Hyperparameter tuning is disabled for this model.\n",
      "            KNN: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
      "                Note: Hyperparameter tuning is disabled for this model.\n",
      "            LR: `autogluon.tabular.models.lr.hyperparameters.parameters`\n",
      "                Note: Hyperparameter tuning is disabled for this model.\n",
      "                Note: 'penalty' parameter can be used for regression to specify regularization method: 'L1' and 'L2' values are supported.\n",
      "            Advanced functionality: Custom AutoGluon model arguments\n",
      "                These arguments are optional and can be specified in any model's hyperparameters.\n",
      "                    Example: `hyperparameters = {'RF': {..., 'ag_args': {'name_suffix': 'CustomModelSuffix', 'disable_in_hpo': True}}`\n",
      "                ag_args: Dictionary of customization options related to meta properties of the model such as its name, the order it is trained, the problem types it is valid for, and the type of HPO it utilizes.\n",
      "                    Valid keys:\n",
      "                        name: (str) The name of the model. This overrides AutoGluon's naming logic and all other name arguments if present.\n",
      "                        name_main: (str) The main name of the model. Example: 'RandomForest'.\n",
      "                        name_prefix: (str) Add a custom prefix to the model name. Unused by default.\n",
      "                        name_suffix: (str) Add a custom suffix to the model name. Unused by default.\n",
      "                        priority: (int) Determines the order in which the model is trained. Larger values result in the model being trained earlier. Default values range from 100 (KNN) to 0 (custom), dictated by model type. If you want this model to be trained first, set priority = 999.\n",
      "                        problem_types: (list) List of valid problem types for the model. `problem_types=['binary']` will result in the model only being trained if `problem_type` is 'binary'.\n",
      "                        disable_in_hpo: (bool) If True, the model will only be trained if `hyperparameter_tune_kwargs=None`.\n",
      "                        valid_stacker: (bool) If False, the model will not be trained as a level 2 or higher stacker model.\n",
      "                        valid_base: (bool) If False, the model will not be trained as a level 1 (base) model.\n",
      "                        hyperparameter_tune_kwargs: (dict) Refer to :meth:`TabularPredictor.fit` hyperparameter_tune_kwargs argument. If specified here, will override global HPO settings for this model.\n",
      "                    Reference the default hyperparameters for example usage of these options.\n",
      "                ag_args_fit: Dictionary of model fit customization options related to how and with what constraints the model is trained. These parameters affect stacker fold models, but not stacker models themselves.\n",
      "                    Clarification: `time_limit` is the internal time in seconds given to a particular model to train, which is dictated in part by the `time_limit` argument given during `predictor.fit()` but is not the same.\n",
      "                    Valid keys:\n",
      "                        stopping_metric: (str or :class:`autogluon.core.metrics.Scorer`, default=None) The metric to use for early stopping of the model. If None, model will decide.\n",
      "                        max_memory_usage_ratio: (float, default=1.0) The ratio of memory usage relative to the default to allow before early stopping or killing the model. Values greater than 1.0 will be increasingly prone to out-of-memory errors.\n",
      "                        max_time_limit_ratio: (float, default=1.0) The ratio of the provided time_limit to use during model `fit()`. If `time_limit=10` and `max_time_limit_ratio=0.3`, time_limit would be changed to 3. Does not alter max_time_limit or min_time_limit values.\n",
      "                        max_time_limit: (float, default=None) Maximum amount of time to allow this model to train for (in sec). If the provided time_limit is greater than this value, it will be replaced by max_time_limit.\n",
      "                        min_time_limit: (float, default=0) Allow this model to train for at least this long (in sec), regardless of the time limit it would otherwise be granted.\n",
      "                            If `min_time_limit >= max_time_limit`, time_limit will be set to min_time_limit.\n",
      "                            If `min_time_limit=None`, time_limit will be set to None and the model will have no training time restriction.\n",
      "                        num_cpus : (int or str, default='auto')\n",
      "                            How many CPUs to use during model fit.\n",
      "                            If 'auto', model will decide.\n",
      "                        num_gpus : (int or str, default='auto')\n",
      "                            How many GPUs to use during model fit.\n",
      "                            If 'auto', model will decide. Some models can use GPUs but don't by default due to differences in model quality.\n",
      "                            Set to 0 to disable usage of GPUs.\n",
      "                ag_args_ensemble: Dictionary of hyperparameters shared by all models that control how they are ensembled, if bag mode is enabled.\n",
      "                    Valid keys:\n",
      "                        use_orig_features: [True, False, \"never\"], default True\n",
      "                            Whether a stack model will use the original features along with the stack features to train (akin to skip-connections).\n",
      "                            If True, will use the original data features.\n",
      "                            If False, will discard the original data features and only use stack features, except when no stack features exist (such as in layer 1).\n",
      "                            If \"never\", will always discard the original data features. Will be skipped in layer 1.\n",
      "                        valid_stacker : bool, default True\n",
      "                            If True, will be marked as valid to include as a stacker model.\n",
      "                            If False, will only be fit as a base model (layer 1) and will not be fit in stack layers (layer 2+).\n",
      "                        max_base_models : int, default 0\n",
      "                            Maximum number of base models whose predictions form the features input to this stacker model.\n",
      "                            If more than `max_base_models` base models are available, only the top `max_base_models` models with highest validation score are used.\n",
      "                            If 0, the logic is skipped.\n",
      "                        max_base_models_per_type : int | str, default \"auto\"\n",
      "                            Similar to `max_base_models`. If more than `max_base_models_per_type` of any particular model type are available,\n",
      "                            only the top `max_base_models_per_type` of that type are used. This occurs before the `max_base_models` filter.\n",
      "                            If \"auto\", the value will be adaptively set based on the number of training samples.\n",
      "                                More samples will lead to larger values, starting at 1 with <1000 samples, increasing up to 12 at >=50000 samples.\n",
      "                            If 0, the logic is skipped.\n",
      "                        num_folds: (int, default=None) If specified, the number of folds to fit in the bagged model.\n",
      "                            If specified, overrides any other value used to determine the number of folds such as predictor.fit `num_bag_folds` argument.\n",
      "                        max_sets: (int, default=None) If specified, the maximum sets to fit in the bagged model.\n",
      "                            The lesser of `max_sets` and the predictor.fit `num_bag_sets` argument will be used for the given model.\n",
      "                            Useful if a particular model is expensive relative to others and you want to avoid repeated bagging of the expensive model while still repeated bagging the cheaper models.\n",
      "                        save_bag_folds: (bool, default=True)\n",
      "                            If True, bagged models will save their fold models (the models from each individual fold of bagging). This is required to use bagged models for prediction.\n",
      "                            If False, bagged models will not save their fold models. This means that bagged models will not be valid models during inference.\n",
      "                                This should only be set to False when planning to call `predictor.refit_full()` or when `refit_full` is set and `set_best_to_refit_full=True`.\n",
      "                                Particularly useful if disk usage is a concern. By not saving the fold models, bagged models will use only very small amounts of disk space during training.\n",
      "                                In many training runs, this will reduce peak disk usage by >10x.\n",
      "                        fold_fitting_strategy: (AbstractFoldFittingStrategy default=auto) Whether to fit folds in parallel or in sequential order.\n",
      "                            If parallel_local, folds will be trained in parallel with evenly distributed computing resources. This could bring 2-4x speedup compared to SequentialLocalFoldFittingStrategy, but could consume much more memory.\n",
      "                            If sequential_local, folds will be trained in sequential.\n",
      "                            If auto, strategy will be determined by OS and whether ray is installed or not. MacOS support for parallel_local is unstable, and may crash if enabled.\n",
      "                        num_folds_parallel: (int or str, default='auto') Number of folds to be trained in parallel if using ParallelLocalFoldFittingStrategy. Consider lowering this value if you encounter either out of memory issue or CUDA out of memory issue(when trained on gpu).\n",
      "                            if 'auto', will try to train all folds in parallel.\n",
      "    \n",
      "    feature_metadata : :class:`autogluon.tabular.FeatureMetadata` or str, default = 'infer'\n",
      "        The feature metadata used in various inner logic in feature preprocessing.\n",
      "        If 'infer', will automatically construct a FeatureMetadata object based on the properties of `train_data`.\n",
      "        In this case, `train_data` is input into :meth:`autogluon.tabular.FeatureMetadata.from_df` to infer `feature_metadata`.\n",
      "        If 'infer' incorrectly assumes the dtypes of features, consider explicitly specifying `feature_metadata`.\n",
      "    infer_limit : float, default = None\n",
      "        The inference time limit in seconds per row to adhere to during fit.\n",
      "        If infer_limit=0.05 and infer_limit_batch_size=1000, AutoGluon will avoid training models that take longer than 50 ms/row to predict when given a batch of 1000 rows to predict (must predict 1000 rows in no more than 50 seconds).\n",
      "        If bagging is enabled, the inference time limit will be respected based on estimated inference speed of `_FULL` models after refit_full is called, NOT on the inference speed of the bagged ensembles.\n",
      "        The inference times calculated for models are assuming `predictor.persist('all')` is called after fit.\n",
      "        If None, no limit is enforced.\n",
      "        If it is impossible to satisfy the constraint, an exception will be raised.\n",
      "    infer_limit_batch_size : int, default = None\n",
      "        The batch size to use when predicting in bulk to estimate per-row inference time.\n",
      "        Must be an integer greater than 0.\n",
      "        If None and `infer_limit` is specified, will default to 10000.\n",
      "        It is recommended to set to 10000 unless you must satisfy an online-inference scenario.\n",
      "        Small values, especially `infer_limit_batch_size=1`, will result in much larger per-row inference times and should be avoided if possible.\n",
      "        Refer to `infer_limit` for more details on how this is used.\n",
      "        If specified when `infer_limit=None`, the inference time will be logged during training but will not be limited.\n",
      "    fit_weighted_ensemble : bool, default = True\n",
      "        If True, a WeightedEnsembleModel will be fit in each stack layer.\n",
      "        A weighted ensemble will often be stronger than an individual model while being very fast to train.\n",
      "        It is recommended to keep this value set to True to maximize predictive quality.\n",
      "    fit_full_last_level_weighted_ensemble : bool, default = True\n",
      "        If True, the WeightedEnsembleModel of the last stacking level will be fit with all (successful) models from all previous layers as base models.\n",
      "        If stacking is disabled, settings this to True or False makes no difference because the WeightedEnsembleModel L2 always uses all models from L1.\n",
      "        It is recommended to keep this value set to True to maximize predictive quality.\n",
      "    full_weighted_ensemble_additionally : bool, default = False\n",
      "        If True, AutoGluon will fit two WeightedEnsembleModels after training all stacking levels. Setting this to True, simulates calling\n",
      "        `fit_weighted_ensemble()` after calling `fit()`. Has no affect if `fit_full_last_level_weighted_ensemble` is False and does not fit an additional\n",
      "        WeightedEnsembleModel if stacking is disabled.\n",
      "    dynamic_stacking: bool | str, default = False\n",
      "        If True and `num_stack_levels` > 0, AutoGluon will dynamically determine whether to use stacking or not by first validating AutoGluon's stacking\n",
      "        behavior. This is done to avoid so-called stacked overfitting that can make traditional multi-layer stacking, as used in AutoGluon, fail drastically\n",
      "        and produce unreliable validation scores.\n",
      "        It is recommended to keep this value set to True or \"auto\" when using stacking,\n",
      "        as long as it is unknown whether the data is affected by stacked overfitting.\n",
      "        If it is known that the data is unaffected by stacked overfitting, then setting this value to False is expected to maximize predictive quality.\n",
      "        If enabled, by default, AutoGluon performs dynamic stacking by spending 25% of the provided time limit for detection and all remaining\n",
      "        time for fitting AutoGluon. This can be adjusted by specifying `ds_args` with different parameters to `fit()`.\n",
      "        If \"auto\", will be set to `not use_bag_holdout`.\n",
      "        See the documentation of `ds_args` for more information.\n",
      "    calibrate_decision_threshold : bool | str, default = \"auto\"\n",
      "        If True, will automatically calibrate the decision threshold at the end of fit for calls to `.predict` based on the evaluation metric.\n",
      "        If \"auto\", will be set to True if `eval_metric.needs_class=True` and `problem_type=\"binary\"`.\n",
      "        By default, the decision threshold is `0.5`, however for some metrics such as `f1` and `balanced_accuracy`,\n",
      "        scores can be significantly improved by choosing a threshold other than `0.5`.\n",
      "        Only valid for `problem_type='binary'`. Ignored for all other problem types.\n",
      "    num_cpus: int | str, default = \"auto\"\n",
      "        The total amount of cpus you want AutoGluon predictor to use.\n",
      "        Auto means AutoGluon will make the decision based on the total number of cpus available and the model requirement for best performance.\n",
      "        Users generally don't need to set this value\n",
      "    num_gpus: int | str, default = \"auto\"\n",
      "        The total amount of gpus you want AutoGluon predictor to use.\n",
      "        Auto means AutoGluon will make the decision based on the total number of gpus available and the model requirement for best performance.\n",
      "        Users generally don't need to set this value\n",
      "    fit_strategy: Literal[\"sequential\", \"parallel\"], default = \"sequential\"\n",
      "        The strategy used to fit models.\n",
      "        If \"sequential\", models will be fit sequentially. This is the most stable option with the most readable logging.\n",
      "        If \"parallel\", models will be fit in parallel with ray, splitting available compute between them.\n",
      "            Note: \"parallel\" is experimental and may run into issues. It was first added in version 1.2.0.\n",
      "        For machines with 16 or more CPU cores, it is likely that \"parallel\" will be faster than \"sequential\".\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    memory_limit: float | str, default = \"auto\"\n",
      "        The total amount of memory in GB you want AutoGluon predictor to use. \"auto\" means AutoGluon will use all available memory on the system\n",
      "        (that is detectable by psutil).\n",
      "        Note that this is only a soft limit! AutoGluon uses this limit to skip training models that are expected to require too much memory or stop\n",
      "        training a model that would exceed the memory limit. AutoGluon does not guarantee the enforcement of this limit (yet). Nevertheless, we expect\n",
      "        AutoGluon to abide by the limit in most cases or, at most, go over the limit by a small margin.\n",
      "        For most virtualized systems (e.g., in the cloud) and local usage on a server or laptop, \"auto\" is ideal for this parameter. We recommend manually\n",
      "        setting the memory limit (and any other resources) on systems with shared resources that are controlled by the operating system (e.g., SLURM and\n",
      "        cgroups). Otherwise, AutoGluon might wrongly assume more resources are available for fitting a model than the operating system allows,\n",
      "        which can result in model training failing or being very inefficient.\n",
      "    callbacks : List[AbstractCallback], default = None\n",
      "        :::{warning}\n",
      "        Callbacks are an experimental feature and may change in future releases without warning.\n",
      "        Callback support is preliminary and targeted towards developers.\n",
      "        :::\n",
      "        A list of callback objects inheriting from `autogluon.core.callbacks.AbstractCallback`.\n",
      "        These objects will be called before and after each model fit within trainer.\n",
      "        They have the ability to skip models or early stop the training process.\n",
      "        They can also theoretically change the entire logical flow of the trainer code by interacting with the passed `trainer` object.\n",
      "        For more details, refer to `AbstractCallback` source code.\n",
      "        If None, no callback objects will be used.\n",
      "    \n",
      "        [Note] Callback objects can be mutated in-place by the fit call if they are stateful.\n",
      "        Ensure that you avoid re-using a mutated callback object between multiple fit calls.\n",
      "    \n",
      "        [Note] Callback objects are deleted from trainer at the end of the fit call. They will not impact operations such as `refit_full` or `fit_extra`.\n",
      "    **kwargs :\n",
      "        auto_stack : bool, default = False\n",
      "            Whether AutoGluon should automatically utilize bagging and multi-layer stack ensembling to boost predictive accuracy.\n",
      "            Set this = True if you are willing to tolerate longer training times in order to maximize predictive accuracy!\n",
      "            Automatically sets `num_bag_folds` and `num_stack_levels` arguments based on dataset properties.\n",
      "            Note: Setting `num_bag_folds` and `num_stack_levels` arguments will override `auto_stack`.\n",
      "            Note: This can increase training time (and inference time) by up to 20x, but can greatly improve predictive performance.\n",
      "        num_bag_folds : int, default = None\n",
      "            Number of folds used for bagging of models. When `num_bag_folds = k`, training time is roughly increased by a factor of `k` (set = 0 to disable bagging).\n",
      "            Disabled by default (0), but we recommend values between 5-10 to maximize predictive performance.\n",
      "            Increasing num_bag_folds will result in models with lower bias but that are more prone to overfitting.\n",
      "            `num_bag_folds = 1` is an invalid value, and will raise a ValueError.\n",
      "            Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting.\n",
      "            To further improve predictions, avoid increasing `num_bag_folds` much beyond 10 and instead increase `num_bag_sets`.\n",
      "        num_bag_sets : int, default = None\n",
      "            Number of repeats of kfold bagging to perform (values must be >= 1). Total number of models trained during bagging = `num_bag_folds * num_bag_sets`.\n",
      "            Defaults to 1 when unspecified. Value is ignored if `num_bag_folds<=2`.\n",
      "            Values greater than 1 will result in superior predictive performance, especially on smaller problems and with stacking enabled (reduces overall variance).\n",
      "            Be warned: This will drastically increase overall runtime, and if using a time limit, can very commonly lead to worse performance.\n",
      "            It is recommended to increase this value only as a last resort, as it is the least computationally efficient method to improve performance.\n",
      "        num_stack_levels : int, default = None\n",
      "            Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of `num_stack_levels+1` (set = 0 to disable stack ensembling).\n",
      "            Disabled by default (0), but we recommend `num_stack_levels=1` to maximize predictive performance.\n",
      "            To prevent overfitting, `num_bag_folds >= 2` must also be set or else a ValueError will be raised.\n",
      "        delay_bag_sets : bool, default = False\n",
      "            Controls when repeats of kfold bagging are executed in AutoGluon when under a time limit.\n",
      "            We suggest sticking to `False` to avoid overfitting.\n",
      "                If True, AutoGluon delays repeating kfold bagging until after evaluating all models\n",
      "                    from `hyperparameters`, if there is enough time. This allows AutoGluon to explore\n",
      "                    more hyperparameters to obtain a better final performance but it may lead to\n",
      "                    more overfitting.\n",
      "                If False, AutoGluon repeats kfold bagging immediately after evaluating each model.\n",
      "                    Thus, AutoGluon might evaluate fewer models with less overfitting.\n",
      "        holdout_frac : float, default = None\n",
      "            Fraction of train_data to holdout as tuning data for optimizing hyperparameters (ignored unless `tuning_data = None`, ignored if `num_bag_folds != 0` unless `use_bag_holdout == True`).\n",
      "            Default value (if None) is selected based on the number of rows in the training data. Default values range from 0.2 at 2,500 rows to 0.01 at 250,000 rows.\n",
      "            Default value is doubled if `hyperparameter_tune_kwargs` is set, up to a maximum of 0.2.\n",
      "            Disabled if `num_bag_folds >= 2` unless `use_bag_holdout == True`.\n",
      "        use_bag_holdout : bool | str, default = False\n",
      "            If True, a `holdout_frac` portion of the data is held-out from model bagging.\n",
      "            This held-out data is only used to score models and determine weighted ensemble weights.\n",
      "            Enable this if there is a large gap between score_val and score_test in stack models.\n",
      "            Note: If `tuning_data` was specified, `tuning_data` is used as the holdout data.\n",
      "            Disabled if not bagging.\n",
      "            If \"auto\", will be set to True if the training data has >= 1000000 rows, else it will be set to False.\n",
      "        hyperparameter_tune_kwargs : str or dict, default = None\n",
      "            Hyperparameter tuning strategy and kwargs (for example, how many HPO trials to run).\n",
      "            If None, then hyperparameter tuning will not be performed.\n",
      "            You can either choose to provide a preset\n",
      "                Valid preset values:\n",
      "                    'auto': Performs HPO via bayesian optimization search on NN_TORCH and FASTAI models, and random search on other models using local scheduler.\n",
      "                    'random': Performs HPO via random search using local scheduler.\n",
      "            Or provide a dict to specify searchers and schedulers\n",
      "                Valid keys:\n",
      "                    'num_trials': How many HPO trials to run\n",
      "                    'scheduler': Which scheduler to use\n",
      "                        Valid values:\n",
      "                            'local': Local scheduler that schedule trials FIFO\n",
      "                    'searcher': Which searching algorithm to use\n",
      "                        'local_random': Uses the 'random' searcher\n",
      "                        'random': Perform random search\n",
      "                        'auto': Perform bayesian optimization search on NN_TORCH and FASTAI models. Perform random search on other models.\n",
      "                The 'scheduler' and 'searcher' key are required when providing a dict.\n",
      "                An example of a valid dict:\n",
      "                    hyperparameter_tune_kwargs = {\n",
      "                        'num_trials': 5,\n",
      "                        'scheduler' : 'local',\n",
      "                        'searcher': 'auto',\n",
      "                    }\n",
      "        feature_prune_kwargs: dict, default = None\n",
      "            Performs layer-wise feature pruning via recursive feature elimination with permutation feature importance.\n",
      "            This fits all models in a stack layer once, discovers a pruned set of features, fits all models in the stack layer\n",
      "            again with the pruned set of features, and updates input feature lists for models whose validation score improved.\n",
      "            If None, do not perform feature pruning. If empty dictionary, perform feature pruning with default configurations.\n",
      "            For valid dictionary keys, refer to :class:`autogluon.core.utils.feature_selection.FeatureSelector` and\n",
      "            `autogluon.core.trainer.abstract_trainer.AbstractTrainer._proxy_model_feature_prune` documentation.\n",
      "            To force all models to work with the pruned set of features, set force_prune=True in the dictionary.\n",
      "        ag_args : dict, default = None\n",
      "            Keyword arguments to pass to all models (i.e. common hyperparameters shared by all AutoGluon models).\n",
      "            See the `ag_args` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
      "            Identical to specifying `ag_args` parameter for all models in `hyperparameters`.\n",
      "            If a key in `ag_args` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
      "        ag_args_fit : dict, default = None\n",
      "            Keyword arguments to pass to all models.\n",
      "            See the `ag_args_fit` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
      "            Identical to specifying `ag_args_fit` parameter for all models in `hyperparameters`.\n",
      "            If a key in `ag_args_fit` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
      "        ag_args_ensemble : dict, default = None\n",
      "            Keyword arguments to pass to all models.\n",
      "            See the `ag_args_ensemble` argument from \"Advanced functionality: Custom AutoGluon model arguments\" in the `hyperparameters` argument documentation for valid values.\n",
      "            Identical to specifying `ag_args_ensemble` parameter for all models in `hyperparameters`.\n",
      "            If a key in `ag_args_ensemble` is already specified for a model in `hyperparameters`, it will not be altered through this argument.\n",
      "        ds_args : dict, see below for default\n",
      "            Keyword arguments for dynamic stacking, only used if `dynamic_stacking=True`. These keyword arguments control the behavior of dynamic stacking\n",
      "            and determine how AutoGluon tries to detect stacked overfitting. To detect stacked overfitting, AutoGluon will fit itself (so called sub-fits)\n",
      "            on a subset (for holdout) or multiple subsets (for repeated cross-validation) and use the predictions of AutoGluon on the validation data to\n",
      "            detect stacked overfitting. The sub-fits stop and stacking will be disabled if any sub-fit shows stacked overfitting.\n",
      "            Allowed keys and values are:\n",
      "                `detection_time_frac` : float in (0,1), default = 1/4\n",
      "                    Determines how much of the original training time is used for detecting stacked overfitting.\n",
      "                    When using (repeated) cross-validation, each sub-fit will be fit for `1/n_splits * detection_time_frac * time_limit`.\n",
      "                    If no time limit is given to AutoGluon, this parameter is ignored and AutoGluon is fit without a time limit in the sub-fit.\n",
      "                `validation_procedure`: str, default = 'holdout'\n",
      "                    Determines the validation procedure used to detect stacked overfitting. Can be either `cv` or `holdout`.\n",
      "                        If `validation_procedure='holdout'` and `holdout_data` is not specified (default), then `holdout_frac` determines the holdout data.\n",
      "                        If `validation_procedure='holdout'` and `holdout_data` is specified, then the provided `holdout_data` is used for validation.\n",
      "                        If `validation_procedure='cv'`, `n_folds` and `n_repeats` determine the kind cross-validation procedure.\n",
      "                `holdout_frac` : float in (0,1), default = 1/9\n",
      "                    Determines how much of the original training data is used for the holdout data during holdout validation.\n",
      "                    Ignored if `holdout_data` is not None.\n",
      "                `n_folds` : int in [2, +inf), default = 2\n",
      "                    Number of folds to use for cross-validation.\n",
      "                `n_repeats` : int [1, +inf), default = 1\n",
      "                    Number of repeats to use for repeated cross-validation. If set to 1, performs 1-repeated cross-validation which is equivalent to\n",
      "                    cross-validation without repeats.\n",
      "                `memory_safe_fits` : bool, default = True\n",
      "                    If True, AutoGluon runs each sub-fit in a ray-based subprocess to avoid memory leakage that exist due to Python's lackluster\n",
      "                    garbage collector.\n",
      "                `clean_up_fits` : bool, default = True\n",
      "                    If True, AutoGluon will remove all saved information from sub-fits from disk.\n",
      "                    If False, the sub-fits are kept on disk and `self._sub_fits` will store paths to the sub-fits, which can be loaded just like any other\n",
      "                    predictor from disk using `TabularPredictor.load()`.\n",
      "                `enable_ray_logging` : bool, default = True\n",
      "                    If True, will log the dynamic stacking sub-fit when ray is used (`memory_safe_fits=True`).\n",
      "                    Note that because of how ray works, this may cause extra unwanted logging in the main fit process after dynamic stacking completes.\n",
      "                `enable_callbacks` : bool, default = False\n",
      "                    If True, will perform a deepcopy on the specified user callbacks and enable them during the DyStack call.\n",
      "                    If False, will not include callbacks in the DyStack call.\n",
      "                `holdout_data`: str or :class:`pd.DataFrame`, default = None\n",
      "                    Another dataset containing validation data reserved for detecting stacked overfitting. This dataset should be in the same format as\n",
      "                    `train_data`. If str is passed, `holdout_data` will be loaded using the str value as the file path.\n",
      "                    If `holdout_data` is not None, the sub-fit is fit on all of `train_data` and the full fit is fit on all of `train_data` and\n",
      "                    `holdout_data` combined.\n",
      "        included_model_types : list, default = None\n",
      "            To only include listed model types for training during `fit()`.\n",
      "            Models that are listed in `included_model_types` but not in `hyperparameters` will be ignored.\n",
      "            Reference `hyperparameters` documentation for what models correspond to each value.\n",
      "            Useful when only a subset of model needs to be trained and the `hyperparameters` dictionary is difficult or time-consuming.\n",
      "                Example: To include both 'GBM' and 'FASTAI' models, specify `included_model_types=['GBM', 'FASTAI']`.\n",
      "        excluded_model_types : list, default = None\n",
      "            Banned subset of model types to avoid training during `fit()`, even if present in `hyperparameters`.\n",
      "            Reference `hyperparameters` documentation for what models correspond to each value.\n",
      "            Useful when a particular model type such as 'KNN' or 'custom' is not desired but altering the `hyperparameters` dictionary is difficult or time-consuming.\n",
      "                Example: To exclude both 'KNN' and 'custom' models, specify `excluded_model_types=['KNN', 'custom']`.\n",
      "        refit_full : bool or str, default = False\n",
      "            Whether to retrain all models on all of the data (training + validation) after the normal training procedure.\n",
      "            This is equivalent to calling `predictor.refit_full(model=refit_full)` after fit.\n",
      "            If `refit_full=True`, it will be treated as `refit_full='all'`.\n",
      "            If `refit_full=False`, refitting will not occur.\n",
      "            Valid str values:\n",
      "                `all`: refits all models.\n",
      "                `best`: refits only the best model (and its ancestors if it is a stacker model).\n",
      "                `{model_name}`: refits only the specified model (and its ancestors if it is a stacker model).\n",
      "            For bagged models:\n",
      "                Reduces a model's inference time by collapsing bagged ensembles into a single model fit on all of the training data.\n",
      "                This process will typically result in a slight accuracy reduction and a large inference speedup.\n",
      "                The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.\n",
      "                    The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bag_folds`) and n is the number of finished repeats (`num_bag_sets`) in the bagged ensemble.\n",
      "                The runtime is generally 10% or less of the original fit runtime.\n",
      "                    The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.\n",
      "            For non-bagged models:\n",
      "                Optimizes a model's accuracy by retraining on 100% of the data without using a validation set.\n",
      "                Will typically result in a slight accuracy increase and no change to inference time.\n",
      "                The runtime will be approximately equal to the original fit runtime.\n",
      "            This process does not alter the original models, but instead adds additional models.\n",
      "            If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.\n",
      "            Models produced by this process will not have validation scores, as they use all of the data for training.\n",
      "                Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(test_data)`.\n",
      "                If the user does not have additional test data, they should reference the original model's score for an estimate of the performance of the refit_full model.\n",
      "                    Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.\n",
      "            The time taken by this process is not enforced by `time_limit`.\n",
      "        save_bag_folds : bool, default = True\n",
      "            If True, will save the bagged fold models to disk.\n",
      "            If False, will not save the bagged fold models, only keeping their metadata and out-of-fold predictions.\n",
      "                Note: The bagged models will not be available for prediction, only use this if you intend to call `refit_full`.\n",
      "                The purpose of setting it to False is that it greatly decreases the peak disk usage of the predictor during the fit call when bagging.\n",
      "                Note that this makes refit_full slightly more likely to crash in scenarios where the dataset is large relative to available system memory.\n",
      "                This is because by default, refit_full will fall back to cloning the first fold of the bagged model in case it lacks memory to refit.\n",
      "                However, if `save_bag_folds=False`, this fallback isn't possible, as there is not fold model to clone because it wasn't saved.\n",
      "                In this scenario, refit will raise an exception for `save_bag_folds=False`, but will succeed if `save_bag_folds=True`.\n",
      "            Final disk usage of predictor will be identical regardless of the setting after `predictor.delete_models(models_to_keep=\"best\", dry_run=False)` is called post-fit.\n",
      "        set_best_to_refit_full : bool, default = False\n",
      "            If True, will change the default model that Predictor uses for prediction when model is not specified to the refit_full version of the model that exhibited the highest validation score.\n",
      "            Only valid if `refit_full` is set.\n",
      "        keep_only_best : bool, default = False\n",
      "            If True, only the best model and its ancestor models are saved in the outputted `predictor`. All other models are deleted.\n",
      "                If you only care about deploying the most accurate predictor with the smallest file-size and no longer need any of the other trained models or functionality beyond prediction on new data, then set: `keep_only_best=True`, `save_space=True`.\n",
      "                This is equivalent to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` directly after `fit()`.\n",
      "            If used with `refit_full` and `set_best_to_refit_full`, the best model will be the refit_full model, and the original bagged best model will be deleted.\n",
      "                `refit_full` will be automatically set to 'best' in this case to avoid training models which will be later deleted.\n",
      "        save_space : bool, default = False\n",
      "            If True, reduces the memory and disk size of predictor by deleting auxiliary model files that aren't needed for prediction on new data.\n",
      "                This is equivalent to calling `predictor.save_space()` directly after `fit()`.\n",
      "            This has NO impact on inference accuracy.\n",
      "            It is recommended if the only goal is to use the trained model for prediction.\n",
      "            Certain advanced functionality may no longer be available if `save_space=True`. Refer to `predictor.save_space()` documentation for more details.\n",
      "        feature_generator : :class:`autogluon.features.generators.AbstractFeatureGenerator`, default = :class:`autogluon.features.generators.AutoMLPipelineFeatureGenerator`\n",
      "            The feature generator used by AutoGluon to process the input data to the form sent to the models. This often includes automated feature generation and data cleaning.\n",
      "            It is generally recommended to keep the default feature generator unless handling an advanced use-case.\n",
      "            To control aspects of the default feature generation process, you can pass in an :class:`AutoMLPipelineFeatureGenerator` object constructed using some of these kwargs:\n",
      "                enable_numeric_features : bool, default True\n",
      "                    Whether to keep features of 'int' and 'float' raw types.\n",
      "                    These features are passed without alteration to the models.\n",
      "                    Appends IdentityFeatureGenerator(infer_features_in_args=dict(valid_raw_types=['int', 'float']))) to the generator group.\n",
      "                enable_categorical_features : bool, default True\n",
      "                    Whether to keep features of 'object' and 'category' raw types.\n",
      "                    These features are processed into memory optimized 'category' features.\n",
      "                    Appends CategoryFeatureGenerator() to the generator group.\n",
      "                enable_datetime_features : bool, default True\n",
      "                    Whether to keep features of 'datetime' raw type and 'object' features identified as 'datetime_as_object' features.\n",
      "                    These features will be converted to 'int' features representing milliseconds since epoch.\n",
      "                    Appends DatetimeFeatureGenerator() to the generator group.\n",
      "                enable_text_special_features : bool, default True\n",
      "                    Whether to use 'object' features identified as 'text' features to generate 'text_special' features such as word count, capital letter ratio, and symbol counts.\n",
      "                    Appends TextSpecialFeatureGenerator() to the generator group.\n",
      "                enable_text_ngram_features : bool, default True\n",
      "                    Whether to use 'object' features identified as 'text' features to generate 'text_ngram' features.\n",
      "                    Appends TextNgramFeatureGenerator(vectorizer=vectorizer) to the generator group.\n",
      "                enable_raw_text_features : bool, default False\n",
      "                    Whether to keep the raw text features.\n",
      "                    Appends IdentityFeatureGenerator(infer_features_in_args=dict(required_special_types=['text'])) to the generator group.\n",
      "                vectorizer : CountVectorizer, default CountVectorizer(min_df=30, ngram_range=(1, 3), max_features=10000, dtype=np.uint8)\n",
      "                    sklearn CountVectorizer object to use in TextNgramFeatureGenerator.\n",
      "                    Only used if `enable_text_ngram_features=True`.\n",
      "        unlabeled_data : pd.DataFrame, default = None\n",
      "            [Experimental Parameter]\n",
      "            Collection of data without labels that we can use to pretrain on. This is the same schema as train_data, except\n",
      "            without the labels. Currently, unlabeled_data is only used for pretraining a TabTransformer model.\n",
      "            If you do not specify 'TRANSF' with unlabeled_data, then no pretraining will occur and unlabeled_data will be ignored!\n",
      "            After the pretraining step, we will finetune using the TabTransformer model as well. If TabTransformer is ensembled\n",
      "            with other models, like in typical AutoGluon fashion, then the output of this \"pretrain/finetune\" will be ensembled\n",
      "            with other models, which will not used the unlabeled_data. The \"pretrain/finetune flow\" is also known as semi-supervised learning.\n",
      "            The typical use case for unlabeled_data is to add signal to your model where you may not have sufficient training\n",
      "            data. e.g. 500 hand-labeled samples (perhaps a hard human task), whole data set (unlabeled) is thousands/millions.\n",
      "            However, this isn't the only use case. Given enough unlabeled data(millions of rows), you may see improvements\n",
      "            to any amount of labeled data.\n",
      "        verbosity : int\n",
      "            If specified, overrides the existing `predictor.verbosity` value.\n",
      "        raise_on_no_models_fitted: bool, default = True\n",
      "            If True, will raise a RuntimeError if no models were successfully fit during `fit()`.\n",
      "        calibrate: bool or str, default = 'auto'\n",
      "            Note: It is recommended to use ['auto', False] as the values and avoid True.\n",
      "            If 'auto' will automatically set to True if the problem_type and eval_metric are suitable for calibration.\n",
      "            If True and the problem_type is classification, temperature scaling will be used to calibrate the Predictor's estimated class probabilities\n",
      "            (which may improve metrics like log_loss) and will train a scalar parameter on the validation set.\n",
      "            If True and the problem_type is quantile regression, conformalization will be used to calibrate the Predictor's estimated quantiles\n",
      "            (which may improve the prediction interval coverage, and bagging could further improve it) and will compute a set of scalar parameters on the validation set.\n",
      "        test_data : str or :class:`pd.DataFrame`, default = None\n",
      "            Table of the test data.\n",
      "            If str is passed, `test_data` will be loaded using the str value as the file path.\n",
      "            NOTE: This test_data is NEVER SEEN by the model during training and, if specified, is only used for logging purposes (i.e. for learning curve generation).\n",
      "            This test_data should be treated the same way test data is used in predictor.leaderboard.\n",
      "        learning_curves : bool or dict, default = None\n",
      "            If bool and is True, default learning curve hyperparameter ag_args will be initialized for each of the models included in the ensemble.\n",
      "                By default, learning curves will include eval_metric scores specified in fit call arguments.\n",
      "                This can be overwritten as shown below.\n",
      "            If dict, user can pass learning_curves parameters to be initialized as ag_args in the following format:\n",
      "                learning_curves = {\n",
      "                    \"metrics\": str or list(str) or Scorer or list(Scorer):\n",
      "                        autogluon metric scorer(s) to be calculated at each iteration, represented as Scorer object(s) or scorer name(s) (str)\n",
      "                    \"use_error\": bool : whether to use error or score format for metrics listed above\n",
      "                }\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`TabularPredictor` object. Returns self.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from autogluon.tabular import TabularDataset, TabularPredictor\n",
      "    >>> train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
      "    >>> label = 'class'\n",
      "    >>> predictor = TabularPredictor(label=label).fit(train_data)\n",
      "    >>> test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
      "    >>> leaderboard = predictor.leaderboard(test_data)\n",
      "    >>> y_test = test_data[label]\n",
      "    >>> test_data = test_data.drop(columns=[label])\n",
      "    >>> y_pred = predictor.predict(test_data)\n",
      "    >>> perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred)\n",
      "    \n",
      "    To maximize predictive performance, use the following:\n",
      "    \n",
      "    >>> eval_metric = 'roc_auc'  # set this to the metric you ultimately care about\n",
      "    >>> time_limit = 3600  # set as long as you are willing to wait (in sec)\n",
      "    >>> predictor = TabularPredictor(label=label, eval_metric=eval_metric).fit(train_data, presets=['best_quality'], time_limit=time_limit)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(TabularPredictor.fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb86af0-ed08-43ed-b329-657ccaf51181",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\n",
      "CPU Count:          32\n",
      "Memory Avail:       49.70 GB / 61.92 GB (80.3%)\n",
      "Disk Space Avail:   9.33 GB / 906.65 GB (1.0%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Beginning AutoGluon training ... Time limit = 3600s\n",
      "AutoGluon will save models to \"/home/fnerin/Desktop/allodb_new/models/pockets_physchem_ablation\"\n",
      "Train Data Rows:    4112\n",
      "Train Data Columns: 166\n",
      "Label Column:       Label_label\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    50879.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.21 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Amino acids_label_comp_id_X']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['Graphein_hphob_fauchere']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['Graphein_hphob_fauchere']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 164 | ['Amino acids_label_comp_id_A', 'Amino acids_label_comp_id_C', 'Amino acids_label_comp_id_D', 'Amino acids_label_comp_id_E', 'Amino acids_label_comp_id_F', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 164 | ['Amino acids_label_comp_id_A', 'Amino acids_label_comp_id_C', 'Amino acids_label_comp_id_D', 'Amino acids_label_comp_id_E', 'Amino acids_label_comp_id_F', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t164 features in original data used to generate 164 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.15 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.12159533073929961, Train Rows: 3612, Val Rows: 500\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 3599.86s of the 3599.86s of remaining time.\n",
      "\t0.0401\t = Validation score   (mcc)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3599.82s of the 3599.81s of remaining time.\n",
      "\t0.0401\t = Validation score   (mcc)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3599.80s of the 3599.79s of remaining time.\n",
      "\t0.38\t = Validation score   (mcc)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 3598.14s of the 3598.13s of remaining time.\n",
      "\t0.4207\t = Validation score   (mcc)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 3596.60s of the 3596.60s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 3596.10s of the 3596.10s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 3595.74s of the 3595.74s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 3594.24s of the 3594.23s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 3593.93s of the 3593.92s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 3593.64s of the 3593.64s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.1222\t = Validation score   (mcc)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 3591.27s of the 3591.27s of remaining time.\n",
      "\t0.38\t = Validation score   (mcc)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 3589.41s of the 3589.41s of remaining time.\n",
      "\t0.4122\t = Validation score   (mcc)\n",
      "\t8.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 3580.87s of the 3580.86s of remaining time.\n",
      "\t0.38\t = Validation score   (mcc)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177 ... Training model for up to 3578.13s of the 3578.13s of remaining time.\n",
      "\t0.3759\t = Validation score   (mcc)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79 ... Training model for up to 3576.76s of the 3576.76s of remaining time.\n",
      "\t0.4153\t = Validation score   (mcc)\n",
      "\t3.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131 ... Training model for up to 3573.70s of the 3573.70s of remaining time.\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191 ... Training model for up to 3572.28s of the 3572.27s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.1645\t = Validation score   (mcc)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9 ... Training model for up to 3570.47s of the 3570.47s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t6.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96 ... Training model for up to 3563.60s of the 3563.60s of remaining time.\n",
      "\t0.3252\t = Validation score   (mcc)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22 ... Training model for up to 3562.31s of the 3562.31s of remaining time.\n",
      "\t0.3924\t = Validation score   (mcc)\n",
      "\t4.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33 ... Training model for up to 3557.58s of the 3557.58s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t2.76s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42 ... Training model for up to 3554.81s of the 3554.81s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137 ... Training model for up to 3554.34s of the 3554.34s of remaining time.\n",
      "\t0.2106\t = Validation score   (mcc)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102 ... Training model for up to 3553.66s of the 3553.66s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 8: early stopping\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13 ... Training model for up to 3553.32s of the 3553.31s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t7.95s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195 ... Training model for up to 3545.37s of the 3545.36s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188 ... Training model for up to 3542.90s of the 3542.90s of remaining time.\n",
      "\t0.3252\t = Validation score   (mcc)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145 ... Training model for up to 3541.06s of the 3541.06s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.1975\t = Validation score   (mcc)\n",
      "\t2.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89 ... Training model for up to 3538.26s of the 3538.25s of remaining time.\n",
      "\t0.4632\t = Validation score   (mcc)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30 ... Training model for up to 3535.78s of the 3535.78s of remaining time.\n",
      "\t0.3824\t = Validation score   (mcc)\n",
      "\t4.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130 ... Training model for up to 3531.08s of the 3531.08s of remaining time.\n",
      "\t0.3759\t = Validation score   (mcc)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86 ... Training model for up to 3529.73s of the 3529.72s of remaining time.\n",
      "\t0.3918\t = Validation score   (mcc)\n",
      "\t3.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50 ... Training model for up to 3526.58s of the 3526.58s of remaining time.\n",
      "\t0.2765\t = Validation score   (mcc)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11 ... Training model for up to 3525.43s of the 3525.43s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 10: early stopping\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t3.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194 ... Training model for up to 3521.62s of the 3521.62s of remaining time.\n",
      "\t0.4207\t = Validation score   (mcc)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172 ... Training model for up to 3519.90s of the 3519.89s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69 ... Training model for up to 3519.45s of the 3519.44s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103 ... Training model for up to 3518.54s of the 3518.54s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.1834\t = Validation score   (mcc)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14 ... Training model for up to 3517.07s of the 3517.07s of remaining time.\n",
      "\t0.3927\t = Validation score   (mcc)\n",
      "\t1.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_r161 ... Training model for up to 3515.42s of the 3515.41s of remaining time.\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t4.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143 ... Training model for up to 3510.86s of the 3510.85s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r70 ... Training model for up to 3510.10s of the 3510.09s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t2.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r156 ... Training model for up to 3507.52s of the 3507.52s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_r196 ... Training model for up to 3506.99s of the 3506.99s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForest_r39 ... Training model for up to 3505.65s of the 3505.64s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167 ... Training model for up to 3503.24s of the 3503.24s of remaining time.\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t2.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r95 ... Training model for up to 3501.18s of the 3501.18s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 8: early stopping\n",
      "\t0.4251\t = Validation score   (mcc)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r41 ... Training model for up to 3498.17s of the 3498.17s of remaining time.\n",
      "\t0.4233\t = Validation score   (mcc)\n",
      "\t5.7s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_r98 ... Training model for up to 3492.45s of the 3492.45s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t4.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_r15 ... Training model for up to 3488.31s of the 3488.31s of remaining time.\n",
      "\t0.2765\t = Validation score   (mcc)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r158 ... Training model for up to 3486.79s of the 3486.79s of remaining time.\n",
      "\t0.424\t = Validation score   (mcc)\n",
      "\t4.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r86 ... Training model for up to 3481.97s of the 3481.97s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t5.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r37 ... Training model for up to 3476.16s of the 3476.15s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 9: early stopping\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r197 ... Training model for up to 3475.02s of the 3475.02s of remaining time.\n",
      "\t0.4075\t = Validation score   (mcc)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r49 ... Training model for up to 3473.16s of the 3473.16s of remaining time.\n",
      "\t0.2106\t = Validation score   (mcc)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r49 ... Training model for up to 3472.43s of the 3472.42s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_r143 ... Training model for up to 3472.14s of the 3472.14s of remaining time.\n",
      "\t0.3252\t = Validation score   (mcc)\n",
      "\t3.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForest_r127 ... Training model for up to 3469.00s of the 3468.99s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t2.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r134 ... Training model for up to 3466.30s of the 3466.30s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 24: early stopping\n",
      "\t0.2428\t = Validation score   (mcc)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForest_r34 ... Training model for up to 3465.36s of the 3465.35s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_r94 ... Training model for up to 3464.03s of the 3464.03s of remaining time.\n",
      "\t0.3759\t = Validation score   (mcc)\n",
      "\t0.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r143 ... Training model for up to 3463.28s of the 3463.28s of remaining time.\n",
      "\t0.4422\t = Validation score   (mcc)\n",
      "\t7.54s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost_r128 ... Training model for up to 3455.72s of the 3455.72s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t6.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r111 ... Training model for up to 3449.26s of the 3449.26s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.2052\t = Validation score   (mcc)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r31 ... Training model for up to 3448.95s of the 3448.94s of remaining time.\n",
      "\t0.347\t = Validation score   (mcc)\n",
      "\t2.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r4 ... Training model for up to 3446.37s of the 3446.37s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r65 ... Training model for up to 3445.94s of the 3445.94s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.2501\t = Validation score   (mcc)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r88 ... Training model for up to 3445.24s of the 3445.24s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 5: early stopping\n",
      "\t-0.0107\t = Validation score   (mcc)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_r30 ... Training model for up to 3444.76s of the 3444.75s of remaining time.\n",
      "\t0.3252\t = Validation score   (mcc)\n",
      "\t2.85s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost_r49 ... Training model for up to 3441.88s of the 3441.88s of remaining time.\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r5 ... Training model for up to 3440.01s of the 3440.01s of remaining time.\n",
      "\t0.2106\t = Validation score   (mcc)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r87 ... Training model for up to 3439.22s of the 3439.22s of remaining time.\n",
      "\t0.4468\t = Validation score   (mcc)\n",
      "\t7.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r71 ... Training model for up to 3431.37s of the 3431.36s of remaining time.\n",
      "\t0.3886\t = Validation score   (mcc)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r143 ... Training model for up to 3429.65s of the 3429.65s of remaining time.\n",
      "\t0.1251\t = Validation score   (mcc)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r178 ... Training model for up to 3427.48s of the 3427.48s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForest_r166 ... Training model for up to 3427.14s of the 3427.14s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_r31 ... Training model for up to 3426.80s of the 3426.80s of remaining time.\n",
      "\t0.2765\t = Validation score   (mcc)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r185 ... Training model for up to 3424.78s of the 3424.78s of remaining time.\n",
      "\t0.4422\t = Validation score   (mcc)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r160 ... Training model for up to 3420.82s of the 3420.82s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r60 ... Training model for up to 3418.70s of the 3418.70s of remaining time.\n",
      "\t0.3252\t = Validation score   (mcc)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForest_r15 ... Training model for up to 3417.59s of the 3417.59s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_r135 ... Training model for up to 3415.45s of the 3415.45s of remaining time.\n",
      "\t0.3759\t = Validation score   (mcc)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost_r22 ... Training model for up to 3413.59s of the 3413.59s of remaining time.\n",
      "\t0.3481\t = Validation score   (mcc)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r69 ... Training model for up to 3412.38s of the 3412.37s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.2765\t = Validation score   (mcc)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: CatBoost_r6 ... Training model for up to 3410.36s of the 3410.36s of remaining time.\n",
      "\t0.2106\t = Validation score   (mcc)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r138 ... Training model for up to 3409.26s of the 3409.26s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 9: early stopping\n",
      "\t0.3502\t = Validation score   (mcc)\n",
      "\t3.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM_r121 ... Training model for up to 3405.86s of the 3405.86s of remaining time.\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t5.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r172 ... Training model for up to 3399.86s of the 3399.86s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 3: early stopping\n",
      "\t-0.0215\t = Validation score   (mcc)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r180 ... Training model for up to 3399.17s of the 3399.16s of remaining time.\n",
      "\t0.1251\t = Validation score   (mcc)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r76 ... Training model for up to 3395.33s of the 3395.33s of remaining time.\n",
      "\t0.3865\t = Validation score   (mcc)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r197 ... Training model for up to 3393.60s of the 3393.59s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r121 ... Training model for up to 3393.12s of the 3393.11s of remaining time.\n",
      "\t0.3451\t = Validation score   (mcc)\n",
      "\t3.72s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r127 ... Training model for up to 3389.38s of the 3389.38s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.0571\t = Validation score   (mcc)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForest_r16 ... Training model for up to 3388.93s of the 3388.93s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r194 ... Training model for up to 3385.50s of the 3385.50s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.1975\t = Validation score   (mcc)\n",
      "\t1.2s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r12 ... Training model for up to 3384.29s of the 3384.29s of remaining time.\n",
      "\t0.1251\t = Validation score   (mcc)\n",
      "\t3.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r135 ... Training model for up to 3380.49s of the 3380.49s of remaining time.\n",
      "\t0.4122\t = Validation score   (mcc)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r4 ... Training model for up to 3376.53s of the 3376.53s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 10: early stopping\n",
      "\t0.2106\t = Validation score   (mcc)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r126 ... Training model for up to 3375.25s of the 3375.24s of remaining time.\n",
      "\t0.0\t = Validation score   (mcc)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r36 ... Training model for up to 3374.84s of the 3374.84s of remaining time.\n",
      "\t0.4616\t = Validation score   (mcc)\n",
      "\t3.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r100 ... Training model for up to 3371.80s of the 3371.80s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "\t0.2234\t = Validation score   (mcc)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r163 ... Training model for up to 3370.98s of the 3370.98s of remaining time.\n",
      "\t0.2652\t = Validation score   (mcc)\n",
      "\t0.92s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_r198 ... Training model for up to 3370.06s of the 3370.06s of remaining time.\n",
      "\t0.1874\t = Validation score   (mcc)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r187 ... Training model for up to 3368.64s of the 3368.64s of remaining time.\n",
      "Metric mcc is not supported by this model - using log_loss instead\n",
      "No improvement since epoch 20: early stopping\n",
      "\t0.38\t = Validation score   (mcc)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r19 ... Training model for up to 3367.86s of the 3367.86s of remaining time.\n",
      "\t0.3813\t = Validation score   (mcc)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost_r95 ... Training model for up to 3365.83s of the 3365.83s of remaining time.\n",
      "\t0.3317\t = Validation score   (mcc)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost_r34 ... Training model for up to 3364.57s of the 3364.57s of remaining time.\n",
      "\t0.3252\t = Validation score   (mcc)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM_r42 ... Training model for up to 3362.03s of the 3362.02s of remaining time.\n",
      "\t0.3759\t = Validation score   (mcc)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r1 ... Training model for up to 3360.48s of the 3360.48s of remaining time.\n",
      "\t0.38\t = Validation score   (mcc)\n",
      "\t8.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r89 ... Training model for up to 3352.08s of the 3352.08s of remaining time.\n",
      "\t0.4793\t = Validation score   (mcc)\n",
      "\t3.57s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 251.53s ... Best model: NeuralNetTorch_r89 | Estimated inference throughput: 32881.0 rows/s (500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/fnerin/Desktop/allodb_new/models/pockets_physchem_ablation\")\n"
     ]
    }
   ],
   "source": [
    "path = \"pockets_physchem_ablation\"\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    predictor = TabularPredictor(\n",
    "        label='Label_label', problem_type=\"binary\", eval_metric=mcc,\n",
    "        path=path, log_to_file=True\n",
    "    )\n",
    "    \n",
    "    fit = predictor.fit(\n",
    "        traindataset,\n",
    "        presets=\"best_quality\",\n",
    "        time_limit=3600,\n",
    "        num_gpus=0, num_cpus=30,\n",
    "        verbosity=2,\n",
    "        auto_stack=False, num_stack_levels=0,\n",
    "        fit_weighted_ensemble=False\n",
    "    )\n",
    "else:\n",
    "    fit = TabularPredictor.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f008a54b-8e64-4723-95f9-1016937ef9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NeuralNetTorch_r143</td>\n",
       "      <td>0.460795</td>\n",
       "      <td>0.442195</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>7.539353</td>\n",
       "      <td>0.025071</td>\n",
       "      <td>0.015195</td>\n",
       "      <td>7.539353</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetTorch_r158</td>\n",
       "      <td>0.429101</td>\n",
       "      <td>0.424024</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.024204</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>4.807785</td>\n",
       "      <td>0.024204</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>4.807785</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetTorch_r79</td>\n",
       "      <td>0.419933</td>\n",
       "      <td>0.415331</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>3.040627</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>3.040627</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NeuralNetTorch_r135</td>\n",
       "      <td>0.408870</td>\n",
       "      <td>0.412188</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.024545</td>\n",
       "      <td>0.015079</td>\n",
       "      <td>3.940091</td>\n",
       "      <td>0.024545</td>\n",
       "      <td>0.015079</td>\n",
       "      <td>3.940091</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NeuralNetTorch_r185</td>\n",
       "      <td>0.407865</td>\n",
       "      <td>0.442195</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.015429</td>\n",
       "      <td>3.943083</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.015429</td>\n",
       "      <td>3.943083</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NeuralNetTorch_r197</td>\n",
       "      <td>0.401657</td>\n",
       "      <td>0.407531</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.020844</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>1.839982</td>\n",
       "      <td>0.020844</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>1.839982</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NeuralNetTorch_r89</td>\n",
       "      <td>0.389372</td>\n",
       "      <td>0.479347</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>3.571486</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>3.571486</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NeuralNetTorch_r19</td>\n",
       "      <td>0.388613</td>\n",
       "      <td>0.381334</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.021315</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>2.009060</td>\n",
       "      <td>0.021315</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>2.009060</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NeuralNetTorch_r36</td>\n",
       "      <td>0.386729</td>\n",
       "      <td>0.461613</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.024295</td>\n",
       "      <td>0.015149</td>\n",
       "      <td>3.017707</td>\n",
       "      <td>0.024295</td>\n",
       "      <td>0.015149</td>\n",
       "      <td>3.017707</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NeuralNetTorch_r30</td>\n",
       "      <td>0.383146</td>\n",
       "      <td>0.382448</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.025249</td>\n",
       "      <td>0.015809</td>\n",
       "      <td>4.679788</td>\n",
       "      <td>0.025249</td>\n",
       "      <td>0.015809</td>\n",
       "      <td>4.679788</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NeuralNetTorch_r31</td>\n",
       "      <td>0.377732</td>\n",
       "      <td>0.347004</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.014422</td>\n",
       "      <td>2.561244</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.014422</td>\n",
       "      <td>2.561244</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NeuralNetTorch_r121</td>\n",
       "      <td>0.376516</td>\n",
       "      <td>0.345106</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>0.015080</td>\n",
       "      <td>3.719054</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>0.015080</td>\n",
       "      <td>3.719054</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>XGBoost_r89</td>\n",
       "      <td>0.373623</td>\n",
       "      <td>0.463194</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>2.466855</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>2.466855</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NeuralNetTorch_r22</td>\n",
       "      <td>0.373426</td>\n",
       "      <td>0.392414</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.023947</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>4.707685</td>\n",
       "      <td>0.023947</td>\n",
       "      <td>0.015301</td>\n",
       "      <td>4.707685</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.368070</td>\n",
       "      <td>0.380008</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>1.838584</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>0.003192</td>\n",
       "      <td>1.838584</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NeuralNetTorch_r86</td>\n",
       "      <td>0.367549</td>\n",
       "      <td>0.391803</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.022416</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>3.127667</td>\n",
       "      <td>0.022416</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>3.127667</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NeuralNetTorch_r71</td>\n",
       "      <td>0.359562</td>\n",
       "      <td>0.388556</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.020053</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>1.693741</td>\n",
       "      <td>0.020053</td>\n",
       "      <td>0.014971</td>\n",
       "      <td>1.693741</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NeuralNetTorch_r14</td>\n",
       "      <td>0.356090</td>\n",
       "      <td>0.392671</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.021784</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>1.640875</td>\n",
       "      <td>0.021784</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>1.640875</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>0.346595</td>\n",
       "      <td>0.412188</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.030424</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>8.522192</td>\n",
       "      <td>0.030424</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>8.522192</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LightGBM_r130</td>\n",
       "      <td>0.343488</td>\n",
       "      <td>0.375870</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>1.345641</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>1.345641</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LightGBM_r94</td>\n",
       "      <td>0.328264</td>\n",
       "      <td>0.375870</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.747781</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.747781</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LightGBM_r15</td>\n",
       "      <td>0.328264</td>\n",
       "      <td>0.276539</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>1.506858</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>1.506858</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>XGBoost_r49</td>\n",
       "      <td>0.328264</td>\n",
       "      <td>0.265246</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>1.863967</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>1.863967</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NeuralNetTorch_r87</td>\n",
       "      <td>0.323382</td>\n",
       "      <td>0.446777</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>7.841057</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.012751</td>\n",
       "      <td>7.841057</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LightGBMLarge</td>\n",
       "      <td>0.322993</td>\n",
       "      <td>0.380008</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>2.713019</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>2.713019</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LightGBM_r96</td>\n",
       "      <td>0.322345</td>\n",
       "      <td>0.325185</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>1.290275</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>1.290275</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CatBoost_r50</td>\n",
       "      <td>0.322345</td>\n",
       "      <td>0.276539</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>1.142268</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>1.142268</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NeuralNetFastAI_r65</td>\n",
       "      <td>0.321172</td>\n",
       "      <td>0.250140</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.686172</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.686172</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LightGBMXT</td>\n",
       "      <td>0.318269</td>\n",
       "      <td>0.380008</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1.651080</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1.651080</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NeuralNetFastAI_r95</td>\n",
       "      <td>0.317708</td>\n",
       "      <td>0.425072</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>2.996214</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>2.996214</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>XGBoost_r95</td>\n",
       "      <td>0.314637</td>\n",
       "      <td>0.331725</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>1.250620</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>1.250620</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NeuralNetTorch_r41</td>\n",
       "      <td>0.302761</td>\n",
       "      <td>0.423340</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>5.701407</td>\n",
       "      <td>0.022578</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>5.701407</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LightGBM_r135</td>\n",
       "      <td>0.302338</td>\n",
       "      <td>0.375870</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>1.850274</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>1.850274</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>XGBoost_r31</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.276539</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>2.009889</td>\n",
       "      <td>0.013926</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>2.009889</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NeuralNetFastAI_r100</td>\n",
       "      <td>0.299665</td>\n",
       "      <td>0.223390</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.806701</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.806701</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NeuralNetFastAI_r138</td>\n",
       "      <td>0.299664</td>\n",
       "      <td>0.350206</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>3.380754</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>3.380754</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NeuralNetTorch_r76</td>\n",
       "      <td>0.298274</td>\n",
       "      <td>0.386450</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>1.712792</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>1.712792</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>CatBoost_r60</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>0.325185</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>1.100647</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>1.100647</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGBoost_r34</td>\n",
       "      <td>0.284028</td>\n",
       "      <td>0.325185</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>2.534856</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>2.534856</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CatBoost_r5</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.210603</td>\n",
       "      <td>mcc</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.781817</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.781817</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  score_test  score_val eval_metric  pred_time_test  \\\n",
       "0    NeuralNetTorch_r143    0.460795   0.442195         mcc        0.025071   \n",
       "1    NeuralNetTorch_r158    0.429101   0.424024         mcc        0.024204   \n",
       "2     NeuralNetTorch_r79    0.419933   0.415331         mcc        0.023978   \n",
       "3    NeuralNetTorch_r135    0.408870   0.412188         mcc        0.024545   \n",
       "4    NeuralNetTorch_r185    0.407865   0.442195         mcc        0.024202   \n",
       "5    NeuralNetTorch_r197    0.401657   0.407531         mcc        0.020844   \n",
       "6     NeuralNetTorch_r89    0.389372   0.479347         mcc        0.021985   \n",
       "7     NeuralNetTorch_r19    0.388613   0.381334         mcc        0.021315   \n",
       "8     NeuralNetTorch_r36    0.386729   0.461613         mcc        0.024295   \n",
       "9     NeuralNetTorch_r30    0.383146   0.382448         mcc        0.025249   \n",
       "10    NeuralNetTorch_r31    0.377732   0.347004         mcc        0.021482   \n",
       "11   NeuralNetTorch_r121    0.376516   0.345106         mcc        0.026294   \n",
       "12           XGBoost_r89    0.373623   0.463194         mcc        0.012451   \n",
       "13    NeuralNetTorch_r22    0.373426   0.392414         mcc        0.023947   \n",
       "14               XGBoost    0.368070   0.380008         mcc        0.009834   \n",
       "15    NeuralNetTorch_r86    0.367549   0.391803         mcc        0.022416   \n",
       "16    NeuralNetTorch_r71    0.359562   0.388556         mcc        0.020053   \n",
       "17    NeuralNetTorch_r14    0.356090   0.392671         mcc        0.021784   \n",
       "18        NeuralNetTorch    0.346595   0.412188         mcc        0.030424   \n",
       "19         LightGBM_r130    0.343488   0.375870         mcc        0.002170   \n",
       "20          LightGBM_r94    0.328264   0.375870         mcc        0.002367   \n",
       "21          LightGBM_r15    0.328264   0.276539         mcc        0.003092   \n",
       "22           XGBoost_r49    0.328264   0.265246         mcc        0.010130   \n",
       "23    NeuralNetTorch_r87    0.323382   0.446777         mcc        0.022109   \n",
       "24         LightGBMLarge    0.322993   0.380008         mcc        0.006338   \n",
       "25          LightGBM_r96    0.322345   0.325185         mcc        0.003252   \n",
       "26          CatBoost_r50    0.322345   0.276539         mcc        0.005274   \n",
       "27   NeuralNetFastAI_r65    0.321172   0.250140         mcc        0.008542   \n",
       "28            LightGBMXT    0.318269   0.380008         mcc        0.002434   \n",
       "29   NeuralNetFastAI_r95    0.317708   0.425072         mcc        0.017784   \n",
       "30           XGBoost_r95    0.314637   0.331725         mcc        0.008070   \n",
       "31    NeuralNetTorch_r41    0.302761   0.423340         mcc        0.022578   \n",
       "32         LightGBM_r135    0.302338   0.375870         mcc        0.004158   \n",
       "33           XGBoost_r31    0.299914   0.276539         mcc        0.013926   \n",
       "34  NeuralNetFastAI_r100    0.299665   0.223390         mcc        0.008333   \n",
       "35  NeuralNetFastAI_r138    0.299664   0.350206         mcc        0.016389   \n",
       "36    NeuralNetTorch_r76    0.298274   0.386450         mcc        0.022093   \n",
       "37          CatBoost_r60    0.294958   0.325185         mcc        0.005248   \n",
       "38           XGBoost_r34    0.284028   0.325185         mcc        0.009394   \n",
       "39           CatBoost_r5    0.275943   0.210603         mcc        0.005321   \n",
       "\n",
       "    pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
       "0        0.015195  7.539353                 0.025071                0.015195   \n",
       "1        0.012500  4.807785                 0.024204                0.012500   \n",
       "2        0.015152  3.040627                 0.023978                0.015152   \n",
       "3        0.015079  3.940091                 0.024545                0.015079   \n",
       "4        0.015429  3.943083                 0.024202                0.015429   \n",
       "5        0.014140  1.839982                 0.020844                0.014140   \n",
       "6        0.015206  3.571486                 0.021985                0.015206   \n",
       "7        0.014812  2.009060                 0.021315                0.014812   \n",
       "8        0.015149  3.017707                 0.024295                0.015149   \n",
       "9        0.015809  4.679788                 0.025249                0.015809   \n",
       "10       0.014422  2.561244                 0.021482                0.014422   \n",
       "11       0.015080  3.719054                 0.026294                0.015080   \n",
       "12       0.003137  2.466855                 0.012451                0.003137   \n",
       "13       0.015301  4.707685                 0.023947                0.015301   \n",
       "14       0.003192  1.838584                 0.009834                0.003192   \n",
       "15       0.015332  3.127667                 0.022416                0.015332   \n",
       "16       0.014971  1.693741                 0.020053                0.014971   \n",
       "17       0.014454  1.640875                 0.021784                0.014454   \n",
       "18       0.014717  8.522192                 0.030424                0.014717   \n",
       "19       0.000855  1.345641                 0.002170                0.000855   \n",
       "20       0.000568  0.747781                 0.002367                0.000568   \n",
       "21       0.001024  1.506858                 0.003092                0.001024   \n",
       "22       0.002652  1.863967                 0.010130                0.002652   \n",
       "23       0.012751  7.841057                 0.022109                0.012751   \n",
       "24       0.001348  2.713019                 0.006338                0.001348   \n",
       "25       0.001017  1.290275                 0.003252                0.001017   \n",
       "26       0.002265  1.142268                 0.005274                0.002265   \n",
       "27       0.003220  0.686172                 0.008542                0.003220   \n",
       "28       0.000981  1.651080                 0.002434                0.000981   \n",
       "29       0.006193  2.996214                 0.017784                0.006193   \n",
       "30       0.002532  1.250620                 0.008070                0.002532   \n",
       "31       0.015705  5.701407                 0.022578                0.015705   \n",
       "32       0.001724  1.850274                 0.004158                0.001724   \n",
       "33       0.002738  2.009889                 0.013926                0.002738   \n",
       "34       0.003723  0.806701                 0.008333                0.003723   \n",
       "35       0.006454  3.380754                 0.016389                0.006454   \n",
       "36       0.014363  1.712792                 0.022093                0.014363   \n",
       "37       0.001716  1.100647                 0.005248                0.001716   \n",
       "38       0.002813  2.534856                 0.009394                0.002813   \n",
       "39       0.001634  0.781817                 0.005321                0.001634   \n",
       "\n",
       "    fit_time_marginal  stack_level  can_infer  fit_order  \n",
       "0            7.539353            1       True         62  \n",
       "1            4.807785            1       True         51  \n",
       "2            3.040627            1       True         15  \n",
       "3            3.940091            1       True         97  \n",
       "4            3.943083            1       True         78  \n",
       "5            1.839982            1       True         54  \n",
       "6            3.571486            1       True        110  \n",
       "7            2.009060            1       True        105  \n",
       "8            3.017707            1       True        100  \n",
       "9            4.679788            1       True         30  \n",
       "10           2.561244            1       True         65  \n",
       "11           3.719054            1       True         92  \n",
       "12           2.466855            1       True         29  \n",
       "13           4.707685            1       True         20  \n",
       "14           1.838584            1       True         11  \n",
       "15           3.127667            1       True         32  \n",
       "16           1.693741            1       True         73  \n",
       "17           1.640875            1       True         39  \n",
       "18           8.522192            1       True         12  \n",
       "19           1.345641            1       True         31  \n",
       "20           0.747781            1       True         61  \n",
       "21           1.506858            1       True         50  \n",
       "22           1.863967            1       True         70  \n",
       "23           7.841057            1       True         72  \n",
       "24           2.713019            1       True         13  \n",
       "25           1.290275            1       True         19  \n",
       "26           1.142268            1       True         33  \n",
       "27           0.686172            1       True         67  \n",
       "28           1.651080            1       True          3  \n",
       "29           2.996214            1       True         47  \n",
       "30           1.250620            1       True        106  \n",
       "31           5.701407            1       True         48  \n",
       "32           1.850274            1       True         82  \n",
       "33           2.009889            1       True         77  \n",
       "34           0.806701            1       True        101  \n",
       "35           3.380754            1       True         86  \n",
       "36           1.712792            1       True         90  \n",
       "37           1.100647            1       True         80  \n",
       "38           2.534856            1       True        107  \n",
       "39           0.781817            1       True         71  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard = fit.leaderboard(testdataset)\n",
    "leaderboard.iloc[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "383f0fe6-5719-450a-a7af-9fb5e1cb8624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NeuralNetTorch_r89'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0162a794-b6b4-4dec-b53d-b27689e1dfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47934681769955645"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard.query(f\"model == '{fit.model_best}'\")[\"score_val\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb731f9-ea9c-4c8e-822f-e89ae94f8a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'NeuralNetTorch_r89',\n",
       " 'model_type': 'TabularNeuralNetTorchModel',\n",
       " 'problem_type': 'binary',\n",
       " 'eval_metric': 'mcc',\n",
       " 'stopping_metric': 'mcc',\n",
       " 'fit_time': 3.571485757827759,\n",
       " 'num_classes': 2,\n",
       " 'quantile_levels': None,\n",
       " 'predict_time': 0.015206336975097656,\n",
       " 'val_score': 0.47934681769955645,\n",
       " 'hyperparameters': {'num_epochs': 1000,\n",
       "  'epochs_wo_improve': None,\n",
       "  'activation': 'relu',\n",
       "  'embedding_size_factor': 1.0,\n",
       "  'embed_exponent': 0.56,\n",
       "  'max_embedding_dim': 100,\n",
       "  'y_range': None,\n",
       "  'y_range_extend': 0.05,\n",
       "  'dropout_prob': 0.33567564890346097,\n",
       "  'optimizer': 'adam',\n",
       "  'learning_rate': 0.006746560197328548,\n",
       "  'weight_decay': 1.6470047305392933e-10,\n",
       "  'proc.embed_min_categories': 4,\n",
       "  'proc.impute_strategy': 'median',\n",
       "  'proc.max_category_levels': 100,\n",
       "  'proc.skew_threshold': 0.99,\n",
       "  'use_ngram_features': False,\n",
       "  'num_layers': 3,\n",
       "  'hidden_size': 245,\n",
       "  'max_batch_size': 512,\n",
       "  'use_batchnorm': True,\n",
       "  'loss_function': 'auto'},\n",
       " 'hyperparameters_fit': {'batch_size': 64, 'num_epochs': 8},\n",
       " 'hyperparameters_nondefault': ['activation',\n",
       "  'dropout_prob',\n",
       "  'hidden_size',\n",
       "  'learning_rate',\n",
       "  'num_layers',\n",
       "  'use_batchnorm',\n",
       "  'weight_decay'],\n",
       " 'ag_args_fit': {'max_memory_usage_ratio': 1.0,\n",
       "  'max_time_limit_ratio': 1.0,\n",
       "  'max_time_limit': None,\n",
       "  'min_time_limit': 0,\n",
       "  'valid_raw_types': ['bool', 'int', 'float', 'category'],\n",
       "  'valid_special_types': None,\n",
       "  'ignored_type_group_special': ['text_ngram', 'text_as_category'],\n",
       "  'ignored_type_group_raw': None,\n",
       "  'get_features_kwargs': None,\n",
       "  'get_features_kwargs_extra': None,\n",
       "  'predict_1_batch_size': None,\n",
       "  'temperature_scalar': None},\n",
       " 'num_features': 164,\n",
       " 'features': ['Amino acids_label_comp_id_A',\n",
       "  'Amino acids_label_comp_id_C',\n",
       "  'Amino acids_label_comp_id_D',\n",
       "  'Amino acids_label_comp_id_E',\n",
       "  'Amino acids_label_comp_id_F',\n",
       "  'Amino acids_label_comp_id_G',\n",
       "  'Amino acids_label_comp_id_H',\n",
       "  'Amino acids_label_comp_id_I',\n",
       "  'Amino acids_label_comp_id_K',\n",
       "  'Amino acids_label_comp_id_L',\n",
       "  'Amino acids_label_comp_id_M',\n",
       "  'Amino acids_label_comp_id_N',\n",
       "  'Amino acids_label_comp_id_P',\n",
       "  'Amino acids_label_comp_id_Q',\n",
       "  'Amino acids_label_comp_id_R',\n",
       "  'Amino acids_label_comp_id_T',\n",
       "  'Amino acids_label_comp_id_V',\n",
       "  'Amino acids_label_comp_id_W',\n",
       "  'Amino acids_label_comp_id_Y',\n",
       "  'Amino acids_label_comp_id_S',\n",
       "  'Graphein_dim_1',\n",
       "  'Graphein_dim_2',\n",
       "  'Graphein_dim_3',\n",
       "  'Graphein_dim_4',\n",
       "  'Graphein_dim_5',\n",
       "  'Graphein_dim_6',\n",
       "  'Graphein_dim_7',\n",
       "  'Graphein_pka_cooh_alpha',\n",
       "  'Graphein_pka_nh3',\n",
       "  'Graphein_pka_rgroup',\n",
       "  'Graphein_isoelectric_points',\n",
       "  'Graphein_molecularweight',\n",
       "  'Graphein_numbercodons',\n",
       "  'Graphein_bulkiness',\n",
       "  'Graphein_polarityzimmerman',\n",
       "  'Graphein_polaritygrantham',\n",
       "  'Graphein_refractivity',\n",
       "  'Graphein_recognitionfactors',\n",
       "  'Graphein_hphob_eisenberg',\n",
       "  'Graphein_hphob_sweet',\n",
       "  'Graphein_hphob_woods',\n",
       "  'Graphein_hphob_doolittle',\n",
       "  'Graphein_hphob_manavalan',\n",
       "  'Graphein_hphob_leo',\n",
       "  'Graphein_hphob_black',\n",
       "  'Graphein_hphob_breese',\n",
       "  'Graphein_hphob_guy',\n",
       "  'Graphein_hphob_janin',\n",
       "  'Graphein_hphob_miyazawa',\n",
       "  'Graphein_hphob_argos',\n",
       "  'Graphein_hphob_roseman',\n",
       "  'Graphein_hphob_tanford',\n",
       "  'Graphein_hphob_wolfenden',\n",
       "  'Graphein_hphob_welling',\n",
       "  'Graphein_hphob_wilson',\n",
       "  'Graphein_hphob_parker',\n",
       "  'Graphein_hphob_ph3_4',\n",
       "  'Graphein_hphob_ph7_5',\n",
       "  'Graphein_hphob_mobility',\n",
       "  'Graphein_hplchfba',\n",
       "  'Graphein_hplctfa',\n",
       "  'Graphein_transmembranetendency',\n",
       "  'Graphein_hplc2_1',\n",
       "  'Graphein_hplc7_4',\n",
       "  'Graphein_buriedresidues',\n",
       "  'Graphein_accessibleresidues',\n",
       "  'Graphein_hphob_chothia',\n",
       "  'Graphein_hphob_rose',\n",
       "  'Graphein_ratioside',\n",
       "  'Graphein_averageburied',\n",
       "  'Graphein_averageflexibility',\n",
       "  'Graphein_alpha_helixfasman',\n",
       "  'Graphein_beta_sheetfasman',\n",
       "  'Graphein_beta_turnfasman',\n",
       "  'Graphein_alpha_helixroux',\n",
       "  'Graphein_beta_sheetroux',\n",
       "  'Graphein_beta_turnroux',\n",
       "  'Graphein_coilroux',\n",
       "  'Graphein_alpha_helixlevitt',\n",
       "  'Graphein_beta_sheetlevitt',\n",
       "  'Graphein_beta_turnlevitt',\n",
       "  'Graphein_totalbeta_strand',\n",
       "  'Graphein_antiparallelbeta_strand',\n",
       "  'Graphein_parallelbeta_strand',\n",
       "  'Graphein_a_a_composition',\n",
       "  'Graphein_a_a_swiss_prot',\n",
       "  'Graphein_relativemutability',\n",
       "  'FreeSASA_area_total',\n",
       "  'FreeSASA_area_polar',\n",
       "  'FreeSASA_area_apolar',\n",
       "  'FreeSASA_area_main-chain',\n",
       "  'FreeSASA_area_side-chain',\n",
       "  'FreeSASA_relative-area_total',\n",
       "  'FreeSASA_relative-area_polar',\n",
       "  'FreeSASA_relative-area_apolar',\n",
       "  'FreeSASA_relative-area_main-chain',\n",
       "  'DSSP_phi',\n",
       "  'DSSP_psi',\n",
       "  'DSSP_NH_O_1_relidx',\n",
       "  'DSSP_NH_O_1_energy',\n",
       "  'DSSP_O_NH_1_relidx',\n",
       "  'DSSP_O_NH_1_energy',\n",
       "  'DSSP_NH_O_2_relidx',\n",
       "  'DSSP_NH_O_2_energy',\n",
       "  'DSSP_O_NH_2_relidx',\n",
       "  'DSSP_O_NH_2_energy',\n",
       "  'DSSP_relative ASA',\n",
       "  'DSSP_secondary structure_H',\n",
       "  'DSSP_secondary structure_B',\n",
       "  'DSSP_secondary structure_E',\n",
       "  'DSSP_secondary structure_G',\n",
       "  'DSSP_secondary structure_I',\n",
       "  'DSSP_secondary structure_T',\n",
       "  'DSSP_secondary structure_S',\n",
       "  'DSSP_secondary structure_-',\n",
       "  'Melodia_curvature',\n",
       "  'Melodia_torsion',\n",
       "  'Melodia_arc_len',\n",
       "  'Melodia_writhing',\n",
       "  'Melodia_phi',\n",
       "  'Melodia_psi',\n",
       "  'Melodia_propensity',\n",
       "  'Biopython_EXP_HSE_B_U',\n",
       "  'Biopython_EXP_HSE_B_D',\n",
       "  'Biopython_EXP_CN',\n",
       "  'Biopython_EXP_RD',\n",
       "  'Biopython_EXP_RD_CA',\n",
       "  'PyRosetta_ddG',\n",
       "  'ProDy_prs_effectiveness',\n",
       "  'ProDy_prs_sensitivity',\n",
       "  'ProDy_mechstiff',\n",
       "  'ProDy_rmsf',\n",
       "  'ProDy_essa',\n",
       "  'TransferEntropy_TE',\n",
       "  'HHBlits_A',\n",
       "  'HHBlits_C',\n",
       "  'HHBlits_D',\n",
       "  'HHBlits_E',\n",
       "  'HHBlits_F',\n",
       "  'HHBlits_G',\n",
       "  'HHBlits_H',\n",
       "  'HHBlits_I',\n",
       "  'HHBlits_K',\n",
       "  'HHBlits_L',\n",
       "  'HHBlits_M',\n",
       "  'HHBlits_N',\n",
       "  'HHBlits_P',\n",
       "  'HHBlits_Q',\n",
       "  'HHBlits_R',\n",
       "  'HHBlits_S',\n",
       "  'HHBlits_T',\n",
       "  'HHBlits_V',\n",
       "  'HHBlits_W',\n",
       "  'HHBlits_Y',\n",
       "  'HHBlits_M->M',\n",
       "  'HHBlits_M->I',\n",
       "  'HHBlits_M->D',\n",
       "  'HHBlits_I->M',\n",
       "  'HHBlits_I->I',\n",
       "  'HHBlits_D->M',\n",
       "  'HHBlits_D->D',\n",
       "  'HHBlits_Neff',\n",
       "  'HHBlits_Neff_I',\n",
       "  'HHBlits_Neff_D'],\n",
       " 'feature_metadata': <autogluon.common.features.feature_metadata.FeatureMetadata at 0x7b70f73dbf90>,\n",
       " 'memory_size': 1134849,\n",
       " 'compile_time': None,\n",
       " 'is_initialized': True,\n",
       " 'is_fit': True,\n",
       " 'is_valid': True,\n",
       " 'can_infer': True,\n",
       " 'has_learning_curves': False,\n",
       " 'val_in_fit': True,\n",
       " 'unlabeled_in_fit': False,\n",
       " 'num_cpus': 30,\n",
       " 'num_gpus': 0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.info()[\"model_info\"][fit.model_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149acf9d-63bd-4f88-940a-58785eaa7a57",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "369160ad-55f9-4bd7-b52a-0496d47ef6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.5058837656548224,\n",
       " 'accuracy': 0.9537937743190662,\n",
       " 'balanced_accuracy': 0.7234228612810224,\n",
       " 'roc_auc': 0.9618041164333959,\n",
       " 'f1': 0.525,\n",
       " 'precision': 0.603448275862069,\n",
       " 'recall': 0.4646017699115044,\n",
       " 'confusion_matrix':       0    1\n",
       " 0  3817   69\n",
       " 1   121  105,\n",
       " 'classification_report': {'0': {'precision': 0.9692737430167597,\n",
       "   'recall': 0.9822439526505404,\n",
       "   'f1-score': 0.9757157464212679,\n",
       "   'support': 3886.0},\n",
       "  '1': {'precision': 0.603448275862069,\n",
       "   'recall': 0.4646017699115044,\n",
       "   'f1-score': 0.525,\n",
       "   'support': 226.0},\n",
       "  'accuracy': 0.9537937743190662,\n",
       "  'macro avg': {'precision': 0.7863610094394143,\n",
       "   'recall': 0.7234228612810224,\n",
       "   'f1-score': 0.750357873210634,\n",
       "   'support': 4112.0},\n",
       "  'weighted avg': {'precision': 0.9491675767772266,\n",
       "   'recall': 0.9537937743190662,\n",
       "   'f1-score': 0.9509439179457799,\n",
       "   'support': 4112.0}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainresults = fit.evaluate(traindataset, detailed_report=True)\n",
    "trainresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5dfca53-f48c-4c92-b310-6a80ba0568d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.3893717588798483,\n",
       " 'accuracy': 0.9433656957928802,\n",
       " 'balanced_accuracy': 0.677000777000777,\n",
       " 'roc_auc': 0.9113830613830614,\n",
       " 'f1': 0.4166666666666667,\n",
       " 'precision': 0.46296296296296297,\n",
       " 'recall': 0.3787878787878788,\n",
       " 'confusion_matrix':       0   1\n",
       " 0  1141  29\n",
       " 1    41  25,\n",
       " 'classification_report': {'0': {'precision': 0.9653130287648054,\n",
       "   'recall': 0.9752136752136752,\n",
       "   'f1-score': 0.9702380952380952,\n",
       "   'support': 1170.0},\n",
       "  '1': {'precision': 0.46296296296296297,\n",
       "   'recall': 0.3787878787878788,\n",
       "   'f1-score': 0.4166666666666667,\n",
       "   'support': 66.0},\n",
       "  'accuracy': 0.9433656957928802,\n",
       "  'macro avg': {'precision': 0.7141379958638842,\n",
       "   'recall': 0.677000777000777,\n",
       "   'f1-score': 0.6934523809523809,\n",
       "   'support': 1236.0},\n",
       "  'weighted avg': {'precision': 0.938488510688008,\n",
       "   'recall': 0.9433656957928802,\n",
       "   'f1-score': 0.9406784558483586,\n",
       "   'support': 1236.0}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fit.evaluate(testdataset, detailed_report=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bbc67ff-d21d-4315-99e2-11c14011ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcc 0.3893717588798483\n",
      "macro-f1 0.6934523809523809\n",
      "      0   1\n",
      "0  1141  29\n",
      "1    41  25\n"
     ]
    }
   ],
   "source": [
    "print(\"mcc\", results[\"mcc\"])\n",
    "print(\"macro-f1\", results[\"classification_report\"][\"macro avg\"][\"f1-score\"])\n",
    "print(results[\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "721980e4-5cd3-4cda-8a0c-1a8fa546ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix\n",
    "def print_top1_results(modelfit, dataset):\n",
    "    top1 = (\n",
    "        modelfit.predict_proba(dataset)\n",
    "        .assign(pdb=dataset.index.map(lambda x: x.split(\"_\")[0]))\n",
    "        .assign(\n",
    "            pred=lambda df: df.groupby(\"pdb\")[1].transform(\n",
    "                lambda x: (x == x.max()).astype(int)\n",
    "            )\n",
    "        )\n",
    "        [\"pred\"]\n",
    "    )\n",
    "    print( \n",
    "        \"mcc\", matthews_corrcoef(dataset[\"Label_label\"], top1), \"\\n\",\n",
    "        \"macro-f1\", f1_score(dataset[\"Label_label\"], top1, average=\"macro\"), \"\\n\",\n",
    "        pd.DataFrame(confusion_matrix(dataset[\"Label_label\"], top1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6604928a-22d5-41f4-af7c-693d1bdca24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcc 0.46543432579592975 \n",
      " macro-f1 0.7323915073275687 \n",
      "       0   1\n",
      "0  1141  29\n",
      "1    35  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "print_top1_results(fit, testdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab8b06c-dbec-4764-8aaa-60bf7d5e221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topx_results(modelfit, dataset):\n",
    "    top1 = (\n",
    "        modelfit.predict_proba(dataset)\n",
    "        .assign(pdb=dataset.index.map(lambda x: x.split(\"_\")[0]))\n",
    "        .assign(\n",
    "            pred=lambda df: (\n",
    "                # Start from a Series where each value/row (sample/pocket) is the total number of pos. labels on its PDB\n",
    "                (\n",
    "                    dataset\n",
    "                    .assign(pdb=dataset.index.map(lambda x: x.split(\"_\")[0]))\n",
    "                    .groupby(\"pdb\")[\"Label_label\"].transform(\"sum\")\n",
    "                )\n",
    "                # Then subtract this \"total num. of pos. in a PDB\" by the rank of each pocket in a PDB, sorted by the probability of 1\n",
    "                .sub(df.groupby(\"pdb\")[1].rank(method=\"first\", ascending=False))\n",
    "                # If the subtraction is positive or 0 it means that the pocket is in the topX and will be assigned 1\n",
    "                >= 0).astype(int)\n",
    "        )\n",
    "        [\"pred\"]\n",
    "    )\n",
    "    print( \n",
    "        \"mcc\", matthews_corrcoef(dataset[\"Label_label\"], top1), \"\\n\",\n",
    "        \"macro-f1\", f1_score(dataset[\"Label_label\"], top1, average=\"macro\"), \"\\n\",\n",
    "        pd.DataFrame(confusion_matrix(dataset[\"Label_label\"], top1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a3eb5e-a548-441f-bb9c-63600d2457c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcc 0.4397824397824398 \n",
      " macro-f1 0.71989121989122 \n",
      "       0   1\n",
      "0  1135  35\n",
      "1    35  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "print_topx_results(fit, testdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb0a42-4220-4cde-9a00-04912bf7404c",
   "metadata": {},
   "source": [
    "## Extra set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "821038bb-6cac-4d4e-b34a-c9b4d3085b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3571648/2666238220.py:4: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  )).drop(columns=\"FPocket\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label_label</th>\n",
       "      <th>Amino acids_label_comp_id_A</th>\n",
       "      <th>Amino acids_label_comp_id_C</th>\n",
       "      <th>Amino acids_label_comp_id_D</th>\n",
       "      <th>Amino acids_label_comp_id_E</th>\n",
       "      <th>Amino acids_label_comp_id_F</th>\n",
       "      <th>Amino acids_label_comp_id_G</th>\n",
       "      <th>Amino acids_label_comp_id_H</th>\n",
       "      <th>Amino acids_label_comp_id_I</th>\n",
       "      <th>Amino acids_label_comp_id_K</th>\n",
       "      <th>...</th>\n",
       "      <th>HHBlits_M-&gt;M</th>\n",
       "      <th>HHBlits_M-&gt;I</th>\n",
       "      <th>HHBlits_M-&gt;D</th>\n",
       "      <th>HHBlits_I-&gt;M</th>\n",
       "      <th>HHBlits_I-&gt;I</th>\n",
       "      <th>HHBlits_D-&gt;M</th>\n",
       "      <th>HHBlits_D-&gt;D</th>\n",
       "      <th>HHBlits_Neff</th>\n",
       "      <th>HHBlits_Neff_I</th>\n",
       "      <th>HHBlits_Neff_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket11</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.019343</td>\n",
       "      <td>0.165788</td>\n",
       "      <td>0.834238</td>\n",
       "      <td>0.136031</td>\n",
       "      <td>0.863919</td>\n",
       "      <td>12.327583</td>\n",
       "      <td>1.418000</td>\n",
       "      <td>2.713667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974569</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.018859</td>\n",
       "      <td>0.260971</td>\n",
       "      <td>0.453309</td>\n",
       "      <td>0.306579</td>\n",
       "      <td>0.622019</td>\n",
       "      <td>11.424571</td>\n",
       "      <td>0.760786</td>\n",
       "      <td>1.323571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962382</td>\n",
       "      <td>0.023266</td>\n",
       "      <td>0.014359</td>\n",
       "      <td>0.206804</td>\n",
       "      <td>0.543209</td>\n",
       "      <td>0.291031</td>\n",
       "      <td>0.625635</td>\n",
       "      <td>12.056000</td>\n",
       "      <td>1.076917</td>\n",
       "      <td>1.590083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962150</td>\n",
       "      <td>0.031724</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>0.232774</td>\n",
       "      <td>0.767232</td>\n",
       "      <td>0.238607</td>\n",
       "      <td>0.761441</td>\n",
       "      <td>12.314714</td>\n",
       "      <td>1.545143</td>\n",
       "      <td>2.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974479</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.216619</td>\n",
       "      <td>0.383377</td>\n",
       "      <td>0.343837</td>\n",
       "      <td>0.456162</td>\n",
       "      <td>10.979400</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>1.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876567</td>\n",
       "      <td>0.062246</td>\n",
       "      <td>0.061196</td>\n",
       "      <td>0.213815</td>\n",
       "      <td>0.508359</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.989279</td>\n",
       "      <td>7.976833</td>\n",
       "      <td>0.797944</td>\n",
       "      <td>6.292278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993104</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.310072</td>\n",
       "      <td>0.356584</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.894323</td>\n",
       "      <td>7.297444</td>\n",
       "      <td>0.680889</td>\n",
       "      <td>1.186889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927772</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>0.056076</td>\n",
       "      <td>0.507445</td>\n",
       "      <td>0.492593</td>\n",
       "      <td>0.249363</td>\n",
       "      <td>0.750681</td>\n",
       "      <td>7.837917</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>1.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket21</th>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928850</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.060229</td>\n",
       "      <td>0.352138</td>\n",
       "      <td>0.647857</td>\n",
       "      <td>0.108113</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>7.866333</td>\n",
       "      <td>1.062083</td>\n",
       "      <td>3.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990717</td>\n",
       "      <td>0.005898</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.265164</td>\n",
       "      <td>0.449084</td>\n",
       "      <td>0.308671</td>\n",
       "      <td>0.691299</td>\n",
       "      <td>5.812214</td>\n",
       "      <td>0.736286</td>\n",
       "      <td>1.167286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Label_label  Amino acids_label_comp_id_A  \\\n",
       "7gqu_pocket11            0                     0.000000   \n",
       "7gqu_pocket15            0                     0.071429   \n",
       "7gqu_pocket7             0                     0.000000   \n",
       "7gqu_pocket13            0                     0.000000   \n",
       "7gqu_pocket14            0                     0.000000   \n",
       "...                    ...                          ...   \n",
       "9dnm_pocket3             0                     0.333333   \n",
       "9dnm_pocket12            0                     0.111111   \n",
       "9dnm_pocket19            0                     0.083333   \n",
       "9dnm_pocket21            0                     0.083333   \n",
       "9dnm_pocket6             0                     0.071429   \n",
       "\n",
       "               Amino acids_label_comp_id_C  Amino acids_label_comp_id_D  \\\n",
       "7gqu_pocket11                          0.0                     0.083333   \n",
       "7gqu_pocket15                          0.0                     0.000000   \n",
       "7gqu_pocket7                           0.0                     0.000000   \n",
       "7gqu_pocket13                          0.0                     0.000000   \n",
       "7gqu_pocket14                          0.2                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "9dnm_pocket3                           0.0                     0.000000   \n",
       "9dnm_pocket12                          0.0                     0.111111   \n",
       "9dnm_pocket19                          0.0                     0.083333   \n",
       "9dnm_pocket21                          0.0                     0.083333   \n",
       "9dnm_pocket6                           0.0                     0.071429   \n",
       "\n",
       "               Amino acids_label_comp_id_E  Amino acids_label_comp_id_F  \\\n",
       "7gqu_pocket11                     0.166667                     0.000000   \n",
       "7gqu_pocket15                     0.000000                     0.142857   \n",
       "7gqu_pocket7                      0.083333                     0.083333   \n",
       "7gqu_pocket13                     0.071429                     0.142857   \n",
       "7gqu_pocket14                     0.000000                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "9dnm_pocket3                      0.000000                     0.000000   \n",
       "9dnm_pocket12                     0.111111                     0.000000   \n",
       "9dnm_pocket19                     0.000000                     0.000000   \n",
       "9dnm_pocket21                     0.000000                     0.083333   \n",
       "9dnm_pocket6                      0.000000                     0.071429   \n",
       "\n",
       "               Amino acids_label_comp_id_G  Amino acids_label_comp_id_H  \\\n",
       "7gqu_pocket11                     0.000000                     0.000000   \n",
       "7gqu_pocket15                     0.000000                     0.000000   \n",
       "7gqu_pocket7                      0.000000                     0.083333   \n",
       "7gqu_pocket13                     0.071429                     0.071429   \n",
       "7gqu_pocket14                     0.100000                     0.000000   \n",
       "...                                    ...                          ...   \n",
       "9dnm_pocket3                      0.000000                     0.000000   \n",
       "9dnm_pocket12                     0.000000                     0.000000   \n",
       "9dnm_pocket19                     0.000000                     0.083333   \n",
       "9dnm_pocket21                     0.000000                     0.000000   \n",
       "9dnm_pocket6                      0.000000                     0.000000   \n",
       "\n",
       "               Amino acids_label_comp_id_I  Amino acids_label_comp_id_K  ...  \\\n",
       "7gqu_pocket11                     0.000000                     0.083333  ...   \n",
       "7gqu_pocket15                     0.000000                     0.071429  ...   \n",
       "7gqu_pocket7                      0.000000                     0.083333  ...   \n",
       "7gqu_pocket13                     0.071429                     0.000000  ...   \n",
       "7gqu_pocket14                     0.000000                     0.000000  ...   \n",
       "...                                    ...                          ...  ...   \n",
       "9dnm_pocket3                      0.111111                     0.111111  ...   \n",
       "9dnm_pocket12                     0.000000                     0.111111  ...   \n",
       "9dnm_pocket19                     0.083333                     0.083333  ...   \n",
       "9dnm_pocket21                     0.000000                     0.083333  ...   \n",
       "9dnm_pocket6                      0.071429                     0.071429  ...   \n",
       "\n",
       "               HHBlits_M->M  HHBlits_M->I  HHBlits_M->D  HHBlits_I->M  \\\n",
       "7gqu_pocket11      0.955484      0.025227      0.019343      0.165788   \n",
       "7gqu_pocket15      0.974569      0.006574      0.018859      0.260971   \n",
       "7gqu_pocket7       0.962382      0.023266      0.014359      0.206804   \n",
       "7gqu_pocket13      0.962150      0.031724      0.006096      0.232774   \n",
       "7gqu_pocket14      0.974479      0.004489      0.021067      0.216619   \n",
       "...                     ...           ...           ...           ...   \n",
       "9dnm_pocket3       0.876567      0.062246      0.061196      0.213815   \n",
       "9dnm_pocket12      0.993104      0.001887      0.004977      0.310072   \n",
       "9dnm_pocket19      0.927772      0.016111      0.056076      0.507445   \n",
       "9dnm_pocket21      0.928850      0.010917      0.060229      0.352138   \n",
       "9dnm_pocket6       0.990717      0.005898      0.003373      0.265164   \n",
       "\n",
       "               HHBlits_I->I  HHBlits_D->M  HHBlits_D->D  HHBlits_Neff  \\\n",
       "7gqu_pocket11      0.834238      0.136031      0.863919     12.327583   \n",
       "7gqu_pocket15      0.453309      0.306579      0.622019     11.424571   \n",
       "7gqu_pocket7       0.543209      0.291031      0.625635     12.056000   \n",
       "7gqu_pocket13      0.767232      0.238607      0.761441     12.314714   \n",
       "7gqu_pocket14      0.383377      0.343837      0.456162     10.979400   \n",
       "...                     ...           ...           ...           ...   \n",
       "9dnm_pocket3       0.508359      0.010684      0.989279      7.976833   \n",
       "9dnm_pocket12      0.356584      0.105642      0.894323      7.297444   \n",
       "9dnm_pocket19      0.492593      0.249363      0.750681      7.837917   \n",
       "9dnm_pocket21      0.647857      0.108113      0.891929      7.866333   \n",
       "9dnm_pocket6       0.449084      0.308671      0.691299      5.812214   \n",
       "\n",
       "               HHBlits_Neff_I  HHBlits_Neff_D  \n",
       "7gqu_pocket11        1.418000        2.713667  \n",
       "7gqu_pocket15        0.760786        1.323571  \n",
       "7gqu_pocket7         1.076917        1.590083  \n",
       "7gqu_pocket13        1.545143        2.180000  \n",
       "7gqu_pocket14        0.637500        1.446600  \n",
       "...                       ...             ...  \n",
       "9dnm_pocket3         0.797944        6.292278  \n",
       "9dnm_pocket12        0.680889        1.186889  \n",
       "9dnm_pocket19        1.095000        1.295000  \n",
       "9dnm_pocket21        1.062083        3.568000  \n",
       "9dnm_pocket6         0.736286        1.167286  \n",
       "\n",
       "[245 rows x 167 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../training_data/7.Extra_set/pockets_features.pkl\", \"rb\") as f:\n",
    "    extra = process_dataframe(pd.concat((\n",
    "        pickle.load(f).values()\n",
    "    )).drop(columns=\"FPocket\"))\n",
    "\n",
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "531dc670-ab8c-48b2-b433-3cecc19cdab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcc 0.5386064030131826\n",
      "macro-f1 0.7693032015065914\n",
      "     0  1\n",
      "0  232  4\n",
      "1    4  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "extra_results = fit.evaluate(extra, detailed_report=True)\n",
    "print(\"mcc\", extra_results[\"mcc\"])\n",
    "print(\"macro-f1\", extra_results[\"classification_report\"][\"macro avg\"][\"f1-score\"])\n",
    "print(extra_results[\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d726d378-0767-4de5-9cac-b9aa77a6db75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8v81_pocket1</th>\n",
       "      <td>0.394688</td>\n",
       "      <td>0.605312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket43</th>\n",
       "      <td>0.400857</td>\n",
       "      <td>0.599143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8qni_pocket1</th>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket17</th>\n",
       "      <td>0.414007</td>\n",
       "      <td>0.585993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket4</th>\n",
       "      <td>0.416432</td>\n",
       "      <td>0.583568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket1</th>\n",
       "      <td>0.419184</td>\n",
       "      <td>0.580816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f4s_pocket1</th>\n",
       "      <td>0.425856</td>\n",
       "      <td>0.574144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket63</th>\n",
       "      <td>0.467054</td>\n",
       "      <td>0.532946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket1</th>\n",
       "      <td>0.494095</td>\n",
       "      <td>0.505905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket4</th>\n",
       "      <td>0.502155</td>\n",
       "      <td>0.497845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8uk6_pocket1</th>\n",
       "      <td>0.511685</td>\n",
       "      <td>0.488315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket6</th>\n",
       "      <td>0.515149</td>\n",
       "      <td>0.484851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket30</th>\n",
       "      <td>0.546492</td>\n",
       "      <td>0.453508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket32</th>\n",
       "      <td>0.571771</td>\n",
       "      <td>0.428229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8uk6_pocket13</th>\n",
       "      <td>0.597577</td>\n",
       "      <td>0.402423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket3</th>\n",
       "      <td>0.608907</td>\n",
       "      <td>0.391093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket20</th>\n",
       "      <td>0.610384</td>\n",
       "      <td>0.389616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket3</th>\n",
       "      <td>0.620413</td>\n",
       "      <td>0.379587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket25</th>\n",
       "      <td>0.630300</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket2</th>\n",
       "      <td>0.631520</td>\n",
       "      <td>0.368480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1\n",
       "8v81_pocket1   0.394688  0.605312\n",
       "8v81_pocket43  0.400857  0.599143\n",
       "8qni_pocket1   0.401600  0.598400\n",
       "8v81_pocket17  0.414007  0.585993\n",
       "8jp0_pocket4   0.416432  0.583568\n",
       "8jp0_pocket1   0.419184  0.580816\n",
       "8f4s_pocket1   0.425856  0.574144\n",
       "7yg5_pocket63  0.467054  0.532946\n",
       "7gqu_pocket1   0.494095  0.505905\n",
       "8v81_pocket4   0.502155  0.497845\n",
       "8uk6_pocket1   0.511685  0.488315\n",
       "8v81_pocket6   0.515149  0.484851\n",
       "8v81_pocket30  0.546492  0.453508\n",
       "8v81_pocket32  0.571771  0.428229\n",
       "8uk6_pocket13  0.597577  0.402423\n",
       "8v81_pocket3   0.608907  0.391093\n",
       "8v81_pocket20  0.610384  0.389616\n",
       "7yg5_pocket3   0.620413  0.379587\n",
       "8jp0_pocket25  0.630300  0.369700\n",
       "9dnm_pocket2   0.631520  0.368480"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = fit.predict_proba(extra).sort_values(1, ascending=False)\n",
    "probs.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f6e6ddc-0346-481d-b578-213674fd5bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8v81_pocket1</th>\n",
       "      <td>0.394688</td>\n",
       "      <td>0.605312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8qni_pocket1</th>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket1</th>\n",
       "      <td>0.419184</td>\n",
       "      <td>0.580816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f4s_pocket1</th>\n",
       "      <td>0.425856</td>\n",
       "      <td>0.574144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket1</th>\n",
       "      <td>0.494095</td>\n",
       "      <td>0.505905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8uk6_pocket1</th>\n",
       "      <td>0.511685</td>\n",
       "      <td>0.488315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket27</th>\n",
       "      <td>0.994806</td>\n",
       "      <td>0.005194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8aq6_pocket2</th>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.000784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket9</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1\n",
       "8v81_pocket1   0.394688  0.605312\n",
       "8qni_pocket1   0.401600  0.598400\n",
       "8jp0_pocket1   0.419184  0.580816\n",
       "8f4s_pocket1   0.425856  0.574144\n",
       "7gqu_pocket1   0.494095  0.505905\n",
       "8uk6_pocket1   0.511685  0.488315\n",
       "7yg5_pocket27  0.994806  0.005194\n",
       "8aq6_pocket2   0.999216  0.000784\n",
       "9dnm_pocket9   0.999998  0.000002"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions of real positive pockets\n",
    "probs.loc[extra[\"Label_label\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7713b9c6-bf0b-4493-b494-964f4488c035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8v81_pocket43</th>\n",
       "      <td>0.400857</td>\n",
       "      <td>0.599143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket17</th>\n",
       "      <td>0.414007</td>\n",
       "      <td>0.585993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket4</th>\n",
       "      <td>0.416432</td>\n",
       "      <td>0.583568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket63</th>\n",
       "      <td>0.467054</td>\n",
       "      <td>0.532946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1\n",
       "8v81_pocket43  0.400857  0.599143\n",
       "8v81_pocket17  0.414007  0.585993\n",
       "8jp0_pocket4   0.416432  0.583568\n",
       "7yg5_pocket63  0.467054  0.532946"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False positive predictions\n",
    "probs.loc[ (probs[1] >= 0.5) & (extra[\"Label_label\"] == 0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "797b27bf-d719-4ae7-b08c-a6f9a0ce6c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcc 0.5386064030131826 \n",
      " macro-f1 0.7693032015065914 \n",
      "      0  1\n",
      "0  232  4\n",
      "1    4  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnerin/miniconda3/envs/pybiomed/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "print_top1_results(fit, extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d18b0c-7871-4315-9a33-37c40c6ea326",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7700e989-5295-4ef7-ae45-a892a07667e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7gqu': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   3420           X1L             D               4            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   3420                 ?        1002          X1L            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   3420                  1             1002                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   3420                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            VAL             A               1           55                 ?   \n",
       "   1            MET             A               1           56                 ?   \n",
       "   2            ALA             A               1           57                 ?   \n",
       "   3            THR             A               1          188                 ?   \n",
       "   4            ALA             A               1          189                 ?   \n",
       "   5            THR             A               1          190                 ?   \n",
       "   6            ALA             A               1          191                 ?   \n",
       "   7            SER             A               1          192                 ?   \n",
       "   8            ARG             A               1          196                 ?   \n",
       "   9            ILE             A               1          210                 ?   \n",
       "   10           THR             A               1          211                 ?   \n",
       "   11           CYS             A               1          212                 ?   \n",
       "   12           THR             A               1          213                 ?   \n",
       "   13           GLY             A               1          214                 ?   \n",
       "   14           ASP             A               1          329                 ?   \n",
       "   15           MET             A               1          330                 ?   \n",
       "   16           GLU             A               1          331                 ?   \n",
       "   17           SER             A               1          332                 ?   \n",
       "   18           TYR             A               1          334                 ?   \n",
       "   19           GLN             A               1          335                 ?   \n",
       "   20           TYR             A               1          376                 ?   \n",
       "   21           MET             A               1          380                 ?   \n",
       "   22           LYS             A               1          383                 ?   \n",
       "   23           ILE             A               1          398                 ?   \n",
       "   24           HIS             A               1          401                 ?   \n",
       "   25           PHE             A               1          402                 ?   \n",
       "   26           GLU             A               1          403                 ?   \n",
       "   27           ASP             A               1          404                 ?   \n",
       "   28           LYS             A               1          405                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          570          VAL            A                  1               55   \n",
       "   1          571          MET            A                  1               56   \n",
       "   2          572          ALA            A                  1               57   \n",
       "   3          703          THR            A                  1              188   \n",
       "   4          704          ALA            A                  1              189   \n",
       "   5          705          THR            A                  1              190   \n",
       "   6          706          ALA            A                  1              191   \n",
       "   7          707          SER            A                  1              192   \n",
       "   8          711          ARG            A                  1              196   \n",
       "   9          725          ILE            A                  1              210   \n",
       "   10         726          THR            A                  1              211   \n",
       "   11         727          CYS            A                  1              212   \n",
       "   12         728          THR            A                  1              213   \n",
       "   13         729          GLY            A                  1              214   \n",
       "   14         844          ASP            A                  1              329   \n",
       "   15         845          MET            A                  1              330   \n",
       "   16         846          GLU            A                  1              331   \n",
       "   17         847          SER            A                  1              332   \n",
       "   18         849          TYR            A                  1              334   \n",
       "   19         850          GLN            A                  1              335   \n",
       "   20         891          TYR            A                  1              376   \n",
       "   21         895          MET            A                  1              380   \n",
       "   22         898          LYS            A                  1              383   \n",
       "   23         913          ILE            A                  1              398   \n",
       "   24         916          HIS            A                  1              401   \n",
       "   25         917          PHE            A                  1              402   \n",
       "   26         918          GLU            A                  1              403   \n",
       "   27         919          ASP            A                  1              404   \n",
       "   28         920          LYS            A                  1              405   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q14191                    570   \n",
       "   1                      UNP                 Q14191                    571   \n",
       "   2                      UNP                 Q14191                    572   \n",
       "   3                      UNP                 Q14191                    703   \n",
       "   4                      UNP                 Q14191                    704   \n",
       "   5                      UNP                 Q14191                    705   \n",
       "   6                      UNP                 Q14191                    706   \n",
       "   7                      UNP                 Q14191                    707   \n",
       "   8                      UNP                 Q14191                    711   \n",
       "   9                      UNP                 Q14191                    725   \n",
       "   10                     UNP                 Q14191                    726   \n",
       "   11                     UNP                 Q14191                    727   \n",
       "   12                     UNP                 Q14191                    728   \n",
       "   13                     UNP                 Q14191                    729   \n",
       "   14                     UNP                 Q14191                    844   \n",
       "   15                     UNP                 Q14191                    845   \n",
       "   16                     UNP                 Q14191                    846   \n",
       "   17                     UNP                 Q14191                    847   \n",
       "   18                     UNP                 Q14191                    849   \n",
       "   19                     UNP                 Q14191                    850   \n",
       "   20                     UNP                 Q14191                    891   \n",
       "   21                     UNP                 Q14191                    895   \n",
       "   22                     UNP                 Q14191                    898   \n",
       "   23                     UNP                 Q14191                    913   \n",
       "   24                     UNP                 Q14191                    916   \n",
       "   25                     UNP                 Q14191                    917   \n",
       "   26                     UNP                 Q14191                    918   \n",
       "   27                     UNP                 Q14191                    919   \n",
       "   28                     UNP                 Q14191                    920   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       V  \n",
       "   1                       M  \n",
       "   2                       A  \n",
       "   3                       T  \n",
       "   4                       A  \n",
       "   5                       T  \n",
       "   6                       A  \n",
       "   7                       S  \n",
       "   8                       R  \n",
       "   9                       I  \n",
       "   10                      T  \n",
       "   11                      C  \n",
       "   12                      T  \n",
       "   13                      G  \n",
       "   14                      D  \n",
       "   15                      M  \n",
       "   16                      E  \n",
       "   17                      S  \n",
       "   18                      Y  \n",
       "   19                      Q  \n",
       "   20                      Y  \n",
       "   21                      M  \n",
       "   22                      K  \n",
       "   23                      I  \n",
       "   24                      H  \n",
       "   25                      F  \n",
       "   26                      E  \n",
       "   27                      D  \n",
       "   28                      K  }],\n",
       " '7x6v': [{'mod':     label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   0             LEU             A               1           27   \n",
       "   8             GLY             A               1           28   \n",
       "   12            PRO             A               1           29   \n",
       "   19            LEU             A               1           30   \n",
       "   24            SER             A               1           31   \n",
       "   30            CYS             A               1           32   \n",
       "   36            LYS             A               1           33   \n",
       "   45            SER             A               1           34   \n",
       "   51            CYS             A               1           35   \n",
       "   57            TRP             A               1           36   \n",
       "   71            GLN             A               1           37   \n",
       "   80            LYS             A               1           38   \n",
       "   89            PHE             A               1           39   \n",
       "   100           ASP             A               1           40   \n",
       "   105           SER             A               1           41   \n",
       "   111           LEU             A               1           42   \n",
       "   116           VAL             A               1           43   \n",
       "   123           ARG             A               1           44   \n",
       "   134           CYS             A               1           45   \n",
       "   140           HIS             A               1           46   \n",
       "   150           ASP             A               1           47   \n",
       "   155           HIS             A               1           48   \n",
       "   165           TYR             A               1           49   \n",
       "   177           LEU             A               1           50   \n",
       "   182           CYS             A               1           51   \n",
       "   188           ARG             A               1           52   \n",
       "   199           HIS             A               1           53   \n",
       "   209           CYS             A               1           54   \n",
       "   215           LEU             A               1           55   \n",
       "   223           ASN             A               1           56   \n",
       "   231           LEU             A               1           57   \n",
       "   239           LEU             A               1           58   \n",
       "   247           LEU             A               1           59   \n",
       "   255           SER             A               1           60   \n",
       "   261           VAL             A               1           61   \n",
       "   268           SER             A               1           62   \n",
       "   274           ASP             A               1           63   \n",
       "   282           ARG             A               1           64   \n",
       "   293           CYS             A               1           65   \n",
       "   299           PRO             A               1           66   \n",
       "   306           LEU             A               1           67   \n",
       "   314           CYS             A               1           68   \n",
       "   320           LYS             A               1           69   \n",
       "   329           TYR             A               1           70   \n",
       "   334           PRO             A               1           71   \n",
       "   341           LEU             A               1           72   \n",
       "   346           PRO             A               1           73   \n",
       "   353           THR             A               1           74   \n",
       "   \n",
       "       pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   0                   ?          27          LEU            B   \n",
       "   8                   ?          28          GLY            B   \n",
       "   12                  ?          29          PRO            B   \n",
       "   19                  ?          30          LEU            B   \n",
       "   24                  ?          31          SER            B   \n",
       "   30                  ?          32          CYS            B   \n",
       "   36                  ?          33          LYS            B   \n",
       "   45                  ?          34          SER            B   \n",
       "   51                  ?          35          CYS            B   \n",
       "   57                  ?          36          TRP            B   \n",
       "   71                  ?          37          GLN            B   \n",
       "   80                  ?          38          LYS            B   \n",
       "   89                  ?          39          PHE            B   \n",
       "   100                 ?          40          ASP            B   \n",
       "   105                 ?          41          SER            B   \n",
       "   111                 ?          42          LEU            B   \n",
       "   116                 ?          43          VAL            B   \n",
       "   123                 ?          44          ARG            B   \n",
       "   134                 ?          45          CYS            B   \n",
       "   140                 ?          46          HIS            B   \n",
       "   150                 ?          47          ASP            B   \n",
       "   155                 ?          48          HIS            B   \n",
       "   165                 ?          49          TYR            B   \n",
       "   177                 ?          50          LEU            B   \n",
       "   182                 ?          51          CYS            B   \n",
       "   188                 ?          52          ARG            B   \n",
       "   199                 ?          53          HIS            B   \n",
       "   209                 ?          54          CYS            B   \n",
       "   215                 ?          55          LEU            B   \n",
       "   223                 ?          56          ASN            B   \n",
       "   231                 ?          57          LEU            B   \n",
       "   239                 ?          58          LEU            B   \n",
       "   247                 ?          59          LEU            B   \n",
       "   255                 ?          60          SER            B   \n",
       "   261                 ?          61          VAL            B   \n",
       "   268                 ?          62          SER            B   \n",
       "   274                 ?          63          ASP            B   \n",
       "   282                 ?          64          ARG            B   \n",
       "   293                 ?          65          CYS            B   \n",
       "   299                 ?          66          PRO            B   \n",
       "   306                 ?          67          LEU            B   \n",
       "   314                 ?          68          CYS            B   \n",
       "   320                 ?          69          LYS            B   \n",
       "   329                 ?          70          TYR            B   \n",
       "   334                 ?          71          PRO            B   \n",
       "   341                 ?          72          LEU            B   \n",
       "   346                 ?          73          PRO            B   \n",
       "   353                 ?          74          THR            B   \n",
       "   \n",
       "       pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   0                    1               27                     UNP   \n",
       "   8                    1               28                     UNP   \n",
       "   12                   1               29                     UNP   \n",
       "   19                   1               30                     UNP   \n",
       "   24                   1               31                     UNP   \n",
       "   30                   1               32                     UNP   \n",
       "   36                   1               33                     UNP   \n",
       "   45                   1               34                     UNP   \n",
       "   51                   1               35                     UNP   \n",
       "   57                   1               36                     UNP   \n",
       "   71                   1               37                     UNP   \n",
       "   80                   1               38                     UNP   \n",
       "   89                   1               39                     UNP   \n",
       "   100                  1               40                     UNP   \n",
       "   105                  1               41                     UNP   \n",
       "   111                  1               42                     UNP   \n",
       "   116                  1               43                     UNP   \n",
       "   123                  1               44                     UNP   \n",
       "   134                  1               45                     UNP   \n",
       "   140                  1               46                     UNP   \n",
       "   150                  1               47                     UNP   \n",
       "   155                  1               48                     UNP   \n",
       "   165                  1               49                     UNP   \n",
       "   177                  1               50                     UNP   \n",
       "   182                  1               51                     UNP   \n",
       "   188                  1               52                     UNP   \n",
       "   199                  1               53                     UNP   \n",
       "   209                  1               54                     UNP   \n",
       "   215                  1               55                     UNP   \n",
       "   223                  1               56                     UNP   \n",
       "   231                  1               57                     UNP   \n",
       "   239                  1               58                     UNP   \n",
       "   247                  1               59                     UNP   \n",
       "   255                  1               60                     UNP   \n",
       "   261                  1               61                     UNP   \n",
       "   268                  1               62                     UNP   \n",
       "   274                  1               63                     UNP   \n",
       "   282                  1               64                     UNP   \n",
       "   293                  1               65                     UNP   \n",
       "   299                  1               66                     UNP   \n",
       "   306                  1               67                     UNP   \n",
       "   314                  1               68                     UNP   \n",
       "   320                  1               69                     UNP   \n",
       "   329                  1               70                     UNP   \n",
       "   334                  1               71                     UNP   \n",
       "   341                  1               72                     UNP   \n",
       "   346                  1               73                     UNP   \n",
       "   353                  1               74                     UNP   \n",
       "   \n",
       "       pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   0                   P18541                     27                      L  \n",
       "   8                   P18541                     28                      G  \n",
       "   12                  P18541                     29                      P  \n",
       "   19                  P18541                     30                      L  \n",
       "   24                  P18541                     31                      S  \n",
       "   30                  P18541                     32                      C  \n",
       "   36                  P18541                     33                      K  \n",
       "   45                  P18541                     34                      S  \n",
       "   51                  P18541                     35                      C  \n",
       "   57                  P18541                     36                      W  \n",
       "   71                  P18541                     37                      Q  \n",
       "   80                  P18541                     38                      K  \n",
       "   89                  P18541                     39                      F  \n",
       "   100                 P18541                     40                      D  \n",
       "   105                 P18541                     41                      S  \n",
       "   111                 P18541                     42                      L  \n",
       "   116                 P18541                     43                      V  \n",
       "   123                 P18541                     44                      R  \n",
       "   134                 P18541                     45                      C  \n",
       "   140                 P18541                     46                      H  \n",
       "   150                 P18541                     47                      D  \n",
       "   155                 P18541                     48                      H  \n",
       "   165                 P18541                     49                      Y  \n",
       "   177                 P18541                     50                      L  \n",
       "   182                 P18541                     51                      C  \n",
       "   188                 P18541                     52                      R  \n",
       "   199                 P18541                     53                      H  \n",
       "   209                 P18541                     54                      C  \n",
       "   215                 P18541                     55                      L  \n",
       "   223                 P18541                     56                      N  \n",
       "   231                 P18541                     57                      L  \n",
       "   239                 P18541                     58                      L  \n",
       "   247                 P18541                     59                      L  \n",
       "   255                 P18541                     60                      S  \n",
       "   261                 P18541                     61                      V  \n",
       "   268                 P18541                     62                      S  \n",
       "   274                 P18541                     63                      D  \n",
       "   282                 P18541                     64                      R  \n",
       "   293                 P18541                     65                      C  \n",
       "   299                 P18541                     66                      P  \n",
       "   306                 P18541                     67                      L  \n",
       "   314                 P18541                     68                      C  \n",
       "   320                 P18541                     69                      K  \n",
       "   329                 P18541                     70                      Y  \n",
       "   334                 P18541                     71                      P  \n",
       "   341                 P18541                     72                      L  \n",
       "   346                 P18541                     73                      P  \n",
       "   353                 P18541                     74                      T  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   48           ARG             B               2          594                 ?   \n",
       "   49           SER             B               2          595                 ?   \n",
       "   50           CYS             B               2          596                 ?   \n",
       "   51           PRO             B               2          597                 ?   \n",
       "   52           LEU             B               2          599                 ?   \n",
       "   53           VAL             B               2          640                 ?   \n",
       "   54           SER             B               2          641                 ?   \n",
       "   55           GLY             B               2          679                 ?   \n",
       "   56           GLU             B               2          680                 ?   \n",
       "   57           LYS             B               2          681                 ?   \n",
       "   58           VAL             B               2          682                 ?   \n",
       "   59           LEU             B               2          683                 ?   \n",
       "   60           LEU             B               2          684                 ?   \n",
       "   61           SER             B               2          685                 ?   \n",
       "   62           ALA             B               2          686                 ?   \n",
       "   63           PHE             B               2          688                 ?   \n",
       "   64           ARG             B               2         1173                 ?   \n",
       "   65           GLU             B               2         1174                 ?   \n",
       "   66           VAL             B               2         1336                 ?   \n",
       "   67           ARG             B               2         1368                 ?   \n",
       "   68           PHE             B               2         1369                 ?   \n",
       "   69           ALA             B               2         1370                 ?   \n",
       "   70           TYR             B               2         1378                 ?   \n",
       "   71           VAL             B               2         1379                 ?   \n",
       "   72           TRP             B               2         1380                 ?   \n",
       "   73           GLY             B               2         1381                 ?   \n",
       "   74           GLU             B               2         1382                 ?   \n",
       "   75           LEU             B               2         1806                 ?   \n",
       "   76           PHE             B               2         1807                 ?   \n",
       "   77           SER             B               2         1808                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   48         594          ARG            A                  1              594   \n",
       "   49         595          SER            A                  1              595   \n",
       "   50         596          CYS            A                  1              596   \n",
       "   51         597          PRO            A                  1              597   \n",
       "   52         599          LEU            A                  1              599   \n",
       "   53         640          VAL            A                  1              640   \n",
       "   54         641          SER            A                  1              641   \n",
       "   55         679          GLY            A                  1              679   \n",
       "   56         680          GLU            A                  1              680   \n",
       "   57         681          LYS            A                  1              681   \n",
       "   58         682          VAL            A                  1              682   \n",
       "   59         683          LEU            A                  1              683   \n",
       "   60         684          LEU            A                  1              684   \n",
       "   61         685          SER            A                  1              685   \n",
       "   62         686          ALA            A                  1              686   \n",
       "   63         688          PHE            A                  1              688   \n",
       "   64        1173          ARG            A                  1             1173   \n",
       "   65        1174          GLU            A                  1             1174   \n",
       "   66        1336          VAL            A                  1             1336   \n",
       "   67        1368          ARG            A                  1             1368   \n",
       "   68        1369          PHE            A                  1             1369   \n",
       "   69        1370          ALA            A                  1             1370   \n",
       "   70        1378          TYR            A                  1             1378   \n",
       "   71        1379          VAL            A                  1             1379   \n",
       "   72        1380          TRP            A                  1             1380   \n",
       "   73        1381          GLY            A                  1             1381   \n",
       "   74        1382          GLU            A                  1             1382   \n",
       "   75        1806          LEU            A                  1             1806   \n",
       "   76        1807          PHE            A                  1             1807   \n",
       "   77        1808          SER            A                  1             1808   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   48                     UNP                 P14240                    594   \n",
       "   49                     UNP                 P14240                    595   \n",
       "   50                     UNP                 P14240                    596   \n",
       "   51                     UNP                 P14240                    597   \n",
       "   52                     UNP                 P14240                    599   \n",
       "   53                     UNP                 P14240                    640   \n",
       "   54                     UNP                 P14240                    641   \n",
       "   55                     UNP                 P14240                    679   \n",
       "   56                     UNP                 P14240                    680   \n",
       "   57                     UNP                 P14240                    681   \n",
       "   58                     UNP                 P14240                    682   \n",
       "   59                     UNP                 P14240                    683   \n",
       "   60                     UNP                 P14240                    684   \n",
       "   61                     UNP                 P14240                    685   \n",
       "   62                     UNP                 P14240                    686   \n",
       "   63                     UNP                 P14240                    688   \n",
       "   64                     UNP                 P14240                   1173   \n",
       "   65                     UNP                 P14240                   1174   \n",
       "   66                     UNP                 P14240                   1336   \n",
       "   67                     UNP                 P14240                   1368   \n",
       "   68                     UNP                 P14240                   1369   \n",
       "   69                     UNP                 P14240                   1370   \n",
       "   70                     UNP                 P14240                   1378   \n",
       "   71                     UNP                 P14240                   1379   \n",
       "   72                     UNP                 P14240                   1380   \n",
       "   73                     UNP                 P14240                   1381   \n",
       "   74                     UNP                 P14240                   1382   \n",
       "   75                     UNP                 P14240                   1806   \n",
       "   76                     UNP                 P14240                   1807   \n",
       "   77                     UNP                 P14240                   1808   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   48                      R  \n",
       "   49                      S  \n",
       "   50                      C  \n",
       "   51                      P  \n",
       "   52                      L  \n",
       "   53                      V  \n",
       "   54                      S  \n",
       "   55                      G  \n",
       "   56                      E  \n",
       "   57                      K  \n",
       "   58                      V  \n",
       "   59                      L  \n",
       "   60                      L  \n",
       "   61                      S  \n",
       "   62                      A  \n",
       "   63                      F  \n",
       "   64                      R  \n",
       "   65                      E  \n",
       "   66                      V  \n",
       "   67                      R  \n",
       "   68                      F  \n",
       "   69                      A  \n",
       "   70                      Y  \n",
       "   71                      V  \n",
       "   72                      W  \n",
       "   73                      G  \n",
       "   74                      E  \n",
       "   75                      L  \n",
       "   76                      F  \n",
       "   77                      S  }],\n",
       " '7yg5': [{'mod':       label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   20113           TOR             V              10            .   \n",
       "   \n",
       "         pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   20113                 ?        2416          TOR            A   \n",
       "   \n",
       "         pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   20113                  1             2416                       ?   \n",
       "   \n",
       "         pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   20113                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            ILE             A               1          209                 ?   \n",
       "   1            VAL             A               1          210                 ?   \n",
       "   2            SER             A               1          213                 ?   \n",
       "   3            ILE             A               1          214                 ?   \n",
       "   4            ALA             A               1          217                 ?   \n",
       "   5            GLU             A               1          353                 ?   \n",
       "   6            PHE             A               1          354                 ?   \n",
       "   7            GLU             A               1          357                 ?   \n",
       "   8            ARG             A               1          358                 ?   \n",
       "   9            ARG             A               1          360                 ?   \n",
       "   10           ASN             A               1          363                 ?   \n",
       "   11           SER             A               1          595                 ?   \n",
       "   12           LEU             A               1          596                 ?   \n",
       "   13           SER             A               1          599                 ?   \n",
       "   14           MET             A               1          600                 ?   \n",
       "   15           LYS             A               1          601                 ?   \n",
       "   16           SER             A               1          602                 ?   \n",
       "   17           ILE             A               1          603                 ?   \n",
       "   18           ILE             A               1          604                 ?   \n",
       "   19           LEU             A               1          606                 ?   \n",
       "   20           ASN             A               1          696                 ?   \n",
       "   21           VAL             A               1          697                 ?   \n",
       "   22           LEU             A               1          699                 ?   \n",
       "   23           ALA             A               1          700                 ?   \n",
       "   24           ILE             A               1          701                 ?   \n",
       "   25           ALA             A               1          702                 ?   \n",
       "   26           VAL             A               1          703                 ?   \n",
       "   27           ASP             A               1          704                 ?   \n",
       "   28           ASN             A               1          705                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          209          ILE            A                  1              209   \n",
       "   1          210          VAL            A                  1              210   \n",
       "   2          213          SER            A                  1              213   \n",
       "   3          214          ILE            A                  1              214   \n",
       "   4          217          ALA            A                  1              217   \n",
       "   5          353          GLU            A                  1              353   \n",
       "   6          354          PHE            A                  1              354   \n",
       "   7          357          GLU            A                  1              357   \n",
       "   8          358          ARG            A                  1              358   \n",
       "   9          360          ARG            A                  1              360   \n",
       "   10         363          ASN            A                  1              363   \n",
       "   11         595          SER            A                  1              595   \n",
       "   12         596          LEU            A                  1              596   \n",
       "   13         599          SER            A                  1              599   \n",
       "   14         600          MET            A                  1              600   \n",
       "   15         601          LYS            A                  1              601   \n",
       "   16         602          SER            A                  1              602   \n",
       "   17         603          ILE            A                  1              603   \n",
       "   18         604          ILE            A                  1              604   \n",
       "   19         606          LEU            A                  1              606   \n",
       "   20         696          ASN            A                  1              696   \n",
       "   21         697          VAL            A                  1              697   \n",
       "   22         699          LEU            A                  1              699   \n",
       "   23         700          ALA            A                  1              700   \n",
       "   24         701          ILE            A                  1              701   \n",
       "   25         702          ALA            A                  1              702   \n",
       "   26         703          VAL            A                  1              703   \n",
       "   27         704          ASP            A                  1              704   \n",
       "   28         705          ASN            A                  1              705   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q15878                    209   \n",
       "   1                      UNP                 Q15878                    210   \n",
       "   2                      UNP                 Q15878                    213   \n",
       "   3                      UNP                 Q15878                    214   \n",
       "   4                      UNP                 Q15878                    217   \n",
       "   5                      UNP                 Q15878                    353   \n",
       "   6                      UNP                 Q15878                    354   \n",
       "   7                      UNP                 Q15878                    357   \n",
       "   8                      UNP                 Q15878                    358   \n",
       "   9                      UNP                 Q15878                    360   \n",
       "   10                     UNP                 Q15878                    363   \n",
       "   11                     UNP                 Q15878                    595   \n",
       "   12                     UNP                 Q15878                    596   \n",
       "   13                     UNP                 Q15878                    599   \n",
       "   14                     UNP                 Q15878                    600   \n",
       "   15                     UNP                 Q15878                    601   \n",
       "   16                     UNP                 Q15878                    602   \n",
       "   17                     UNP                 Q15878                    603   \n",
       "   18                     UNP                 Q15878                    604   \n",
       "   19                     UNP                 Q15878                    606   \n",
       "   20                     UNP                 Q15878                    696   \n",
       "   21                     UNP                 Q15878                    697   \n",
       "   22                     UNP                 Q15878                    699   \n",
       "   23                     UNP                 Q15878                    700   \n",
       "   24                     UNP                 Q15878                    701   \n",
       "   25                     UNP                 Q15878                    702   \n",
       "   26                     UNP                 Q15878                    703   \n",
       "   27                     UNP                 Q15878                    704   \n",
       "   28                     UNP                 Q15878                    705   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       I  \n",
       "   1                       V  \n",
       "   2                       S  \n",
       "   3                       I  \n",
       "   4                       A  \n",
       "   5                       E  \n",
       "   6                       F  \n",
       "   7                       E  \n",
       "   8                       R  \n",
       "   9                       R  \n",
       "   10                      N  \n",
       "   11                      S  \n",
       "   12                      L  \n",
       "   13                      S  \n",
       "   14                      M  \n",
       "   15                      K  \n",
       "   16                      S  \n",
       "   17                      I  \n",
       "   18                      I  \n",
       "   19                      L  \n",
       "   20                      N  \n",
       "   21                      V  \n",
       "   22                      L  \n",
       "   23                      A  \n",
       "   24                      I  \n",
       "   25                      A  \n",
       "   26                      V  \n",
       "   27                      D  \n",
       "   28                      N  }],\n",
       " '8aq6': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   1349           NT0            IA               7            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   1349                 ?         301          NT0            G   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   1349                  1              301                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   1349                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            ASP             G               1           21                 ?   \n",
       "   1            TRP             G               1           22                 ?   \n",
       "   2            ARG             G               1           23                 ?   \n",
       "   3            ILE             G               1           53                 ?   \n",
       "   4            ARG             G               1           55                 ?   \n",
       "   5            ASP             G               1           67                 ?   \n",
       "   6            HIS             G               1           69                 ?   \n",
       "   7            ILE             G               1           71                 ?   \n",
       "   8            TYR             G               1           93                 ?   \n",
       "   9            PRO             G               1           94                 ?   \n",
       "   10           VAL             G               1           95                 ?   \n",
       "   11           PHE             G               1          100                 ?   \n",
       "   12           LYS             G               1          101                 ?   \n",
       "   13           ILE             G               1          103                 ?   \n",
       "   14           TYR             G               1          106                 ?   \n",
       "   15           ILE             G               1          179                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0            9          ASP            G                  1               21   \n",
       "   1           10          TRP            G                  1               22   \n",
       "   2           11          ARG            G                  1               23   \n",
       "   3           41          ILE            G                  1               53   \n",
       "   4           43          ARG            G                  1               55   \n",
       "   5           55          ASP            G                  1               67   \n",
       "   6           57          HIS            G                  1               69   \n",
       "   7           59          ILE            G                  1               71   \n",
       "   8           81          TYR            G                  1               93   \n",
       "   9           82          PRO            G                  1               94   \n",
       "   10          83          VAL            G                  1               95   \n",
       "   11          88          PHE            G                  1              100   \n",
       "   12          89          LYS            G                  1              101   \n",
       "   13          91          ILE            G                  1              103   \n",
       "   14          94          TYR            G                  1              106   \n",
       "   15         167          ILE            G                  1              179   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9GV45                     36   \n",
       "   1                      UNP                 Q9GV45                     37   \n",
       "   2                      UNP                 Q9GV45                     38   \n",
       "   3                      UNP                 Q9GV45                     68   \n",
       "   4                      UNP                 Q9GV45                     70   \n",
       "   5                      UNP                 Q9GV45                     82   \n",
       "   6                      UNP                 Q9GV45                     84   \n",
       "   7                      UNP                 Q9GV45                     86   \n",
       "   8                      UNP                 Q9GV45                    108   \n",
       "   9                      UNP                 Q9GV45                    109   \n",
       "   10                     UNP                 Q9GV45                    110   \n",
       "   11                     UNP                 Q9GV45                    115   \n",
       "   12                     UNP                 Q9GV45                    116   \n",
       "   13                     UNP                 Q9GV45                    118   \n",
       "   14                     UNP                 Q9GV45                    121   \n",
       "   15                     UNP                 Q9GV45                    194   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       D  \n",
       "   1                       W  \n",
       "   2                       Q  \n",
       "   3                       I  \n",
       "   4                       K  \n",
       "   5                       D  \n",
       "   6                       H  \n",
       "   7                       I  \n",
       "   8                       Y  \n",
       "   9                       P  \n",
       "   10                      V  \n",
       "   11                      F  \n",
       "   12                      K  \n",
       "   13                      I  \n",
       "   14                      Y  \n",
       "   15                      I  }],\n",
       " '8cgw': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   7209           UJX             P               7            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   7209                 ?        1102          UJX            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   7209                  1             1102                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   7209                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            SER             A               1          116                 ?   \n",
       "   1            TYR             A               1          117                 ?   \n",
       "   2            TYR             A               1          118                 ?   \n",
       "   3            GLN             A               1          139                 ?   \n",
       "   4            GLU             A               1          141                 ?   \n",
       "   5            PRO             A               1          142                 ?   \n",
       "   6            GLU             A               1          272                 ?   \n",
       "   7            ALA             A               1          273                 ?   \n",
       "   8            GLY             A               1          274                 ?   \n",
       "   9            ALA             A               1          275                 ?   \n",
       "   10           MET             A               1          276                 ?   \n",
       "   11           ARG             A               1          285                 ?   \n",
       "   12           ILE             A               1          386                 ?   \n",
       "   13           GLU             A               1          387                 ?   \n",
       "   14           GLU             A               1          388                 ?   \n",
       "   15           MET             A               1          389                 ?   \n",
       "   16           PHE             A               1          390                 ?   \n",
       "   17           TYR             A               1          395                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          270          SER            A                  1              116   \n",
       "   1          271          TYR            A                  1              117   \n",
       "   2          272          TYR            A                  1              118   \n",
       "   3          293          GLN            A                  1              139   \n",
       "   4          295          GLU            A                  1              141   \n",
       "   5          296          PRO            A                  1              142   \n",
       "   6          426          GLU            A                  1              272   \n",
       "   7          427          ALA            A                  1              273   \n",
       "   8          428          GLY            A                  1              274   \n",
       "   9          429          ALA            A                  1              275   \n",
       "   10         430          MET            A                  1              276   \n",
       "   11         439          ARG            A                  1              285   \n",
       "   12         540          ILE            A                  1              386   \n",
       "   13         541          GLU            A                  1              387   \n",
       "   14         542          GLU            A                  1              388   \n",
       "   15         543          MET            A                  1              389   \n",
       "   16         544          PHE            A                  1              390   \n",
       "   17         549          TYR            A                  1              395   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9UIQ6                    270   \n",
       "   1                      UNP                 Q9UIQ6                    271   \n",
       "   2                      UNP                 Q9UIQ6                    272   \n",
       "   3                      UNP                 Q9UIQ6                    293   \n",
       "   4                      UNP                 Q9UIQ6                    295   \n",
       "   5                      UNP                 Q9UIQ6                    296   \n",
       "   6                      UNP                 Q9UIQ6                    426   \n",
       "   7                      UNP                 Q9UIQ6                    427   \n",
       "   8                      UNP                 Q9UIQ6                    428   \n",
       "   9                      UNP                 Q9UIQ6                    429   \n",
       "   10                     UNP                 Q9UIQ6                    430   \n",
       "   11                     UNP                 Q9UIQ6                    439   \n",
       "   12                     UNP                 Q9UIQ6                    540   \n",
       "   13                     UNP                 Q9UIQ6                    541   \n",
       "   14                     UNP                 Q9UIQ6                    542   \n",
       "   15                     UNP                 Q9UIQ6                    543   \n",
       "   16                     UNP                 Q9UIQ6                    544   \n",
       "   17                     UNP                 Q9UIQ6                    549   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       S  \n",
       "   1                       Y  \n",
       "   2                       Y  \n",
       "   3                       Q  \n",
       "   4                       E  \n",
       "   5                       P  \n",
       "   6                       E  \n",
       "   7                       A  \n",
       "   8                       G  \n",
       "   9                       A  \n",
       "   10                      M  \n",
       "   11                      R  \n",
       "   12                      I  \n",
       "   13                      E  \n",
       "   14                      E  \n",
       "   15                      M  \n",
       "   16                      F  \n",
       "   17                      Y  }],\n",
       " '8f4s': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   3219           XDU             D               4            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   3219                 ?        7102          XDU            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   3219                  1             7102                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   3219                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            ILE             A               1           71                 ?   \n",
       "   1            PHE             A               1           73                 ?   \n",
       "   2            GLY             A               1           74                 ?   \n",
       "   3            VAL             A               1           99                 ?   \n",
       "   4            SER             A               1          101                 ?   \n",
       "   5            LEU             A               1          114                 ?   \n",
       "   6            GLY             A               1          116                 ?   \n",
       "   7            ASP             A               1          117                 ?   \n",
       "   8            CYS             A               1          118                 ?   \n",
       "   9            ALA             A               1          119                 ?   \n",
       "   10           VAL             A               1          121                 ?   \n",
       "   11           HIS             A               1          122                 ?   \n",
       "   12           THR             A               1          123                 ?   \n",
       "   13           ASN             A               1          125                 ?   \n",
       "   14           TRP             A               1          127                 ?   \n",
       "   15           SER             A               1          132                 ?   \n",
       "   16           MET             A               1          134                 ?   \n",
       "   17           PHE             A               1          152                 ?   \n",
       "   18           PHE             A               1          153                 ?   \n",
       "   19           TYR             A               1          155                 ?   \n",
       "   20           ILE             A               1          156                 ?   \n",
       "   21           PHE             A               1          159                 ?   \n",
       "   22           LYS             A               1          163                 ?   \n",
       "   23           VAL             A               1          292                 ?   \n",
       "   24           ILE             A               1          293                 ?   \n",
       "   25           SER             A               1          294                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0         6866          ILE            A                  1               71   \n",
       "   1         6868          PHE            A                  1               73   \n",
       "   2         6869          GLY            A                  1               74   \n",
       "   3         6894          VAL            A                  1               99   \n",
       "   4         6896          SER            A                  1              101   \n",
       "   5         6909          LEU            A                  1              114   \n",
       "   6         6911          GLY            A                  1              116   \n",
       "   7         6912          ASP            A                  1              117   \n",
       "   8         6913          CYS            A                  1              118   \n",
       "   9         6914          ALA            A                  1              119   \n",
       "   10        6916          VAL            A                  1              121   \n",
       "   11        6917          HIS            A                  1              122   \n",
       "   12        6918          THR            A                  1              123   \n",
       "   13        6920          ASN            A                  1              125   \n",
       "   14        6922          TRP            A                  1              127   \n",
       "   15        6927          SER            A                  1              132   \n",
       "   16        6929          MET            A                  1              134   \n",
       "   17        6947          PHE            A                  1              152   \n",
       "   18        6948          PHE            A                  1              153   \n",
       "   19        6950          TYR            A                  1              155   \n",
       "   20        6951          ILE            A                  1              156   \n",
       "   21        6954          PHE            A                  1              159   \n",
       "   22        6958          LYS            A                  1              163   \n",
       "   23        7087          VAL            A                  1              292   \n",
       "   24        7088          ILE            A                  1              293   \n",
       "   25        7089          SER            A                  1              294   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 P0DTD1                   6866   \n",
       "   1                      UNP                 P0DTD1                   6868   \n",
       "   2                      UNP                 P0DTD1                   6869   \n",
       "   3                      UNP                 P0DTD1                   6894   \n",
       "   4                      UNP                 P0DTD1                   6896   \n",
       "   5                      UNP                 P0DTD1                   6909   \n",
       "   6                      UNP                 P0DTD1                   6911   \n",
       "   7                      UNP                 P0DTD1                   6912   \n",
       "   8                      UNP                 P0DTD1                   6913   \n",
       "   9                      UNP                 P0DTD1                   6914   \n",
       "   10                     UNP                 P0DTD1                   6916   \n",
       "   11                     UNP                 P0DTD1                   6917   \n",
       "   12                     UNP                 P0DTD1                   6918   \n",
       "   13                     UNP                 P0DTD1                   6920   \n",
       "   14                     UNP                 P0DTD1                   6922   \n",
       "   15                     UNP                 P0DTD1                   6927   \n",
       "   16                     UNP                 P0DTD1                   6929   \n",
       "   17                     UNP                 P0DTD1                   6947   \n",
       "   18                     UNP                 P0DTD1                   6948   \n",
       "   19                     UNP                 P0DTD1                   6950   \n",
       "   20                     UNP                 P0DTD1                   6951   \n",
       "   21                     UNP                 P0DTD1                   6954   \n",
       "   22                     UNP                 P0DTD1                   6958   \n",
       "   23                     UNP                 P0DTD1                   7087   \n",
       "   24                     UNP                 P0DTD1                   7088   \n",
       "   25                     UNP                 P0DTD1                   7089   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       I  \n",
       "   1                       F  \n",
       "   2                       G  \n",
       "   3                       V  \n",
       "   4                       S  \n",
       "   5                       L  \n",
       "   6                       G  \n",
       "   7                       D  \n",
       "   8                       C  \n",
       "   9                       A  \n",
       "   10                      V  \n",
       "   11                      H  \n",
       "   12                      T  \n",
       "   13                      N  \n",
       "   14                      W  \n",
       "   15                      S  \n",
       "   16                      M  \n",
       "   17                      F  \n",
       "   18                      F  \n",
       "   19                      Y  \n",
       "   20                      I  \n",
       "   21                      F  \n",
       "   22                      K  \n",
       "   23                      V  \n",
       "   24                      I  \n",
       "   25                      S  }],\n",
       " '8jp0': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   5644           EKY             B               2            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   5644                 ?        1001          EKY            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   5644                  1             1001                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   5644                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            GLU             A               1          132                 ?   \n",
       "   1            THR             A               1          133                 ?   \n",
       "   2            VAL             A               1          134                 ?   \n",
       "   3            SER             A               1          135                 ?   \n",
       "   4            LEU             A               1          137                 ?   \n",
       "   5            THR             A               1          138                 ?   \n",
       "   6            ALA             A               1          141                 ?   \n",
       "   7            HIS             A               1          200                 ?   \n",
       "   8            VAL             A               1          203                 ?   \n",
       "   9            VAL             A               1          206                 ?   \n",
       "   10           THR             A               1          207                 ?   \n",
       "   11           ALA             A               1          208                 ?   \n",
       "   12           TRP             A               1          210                 ?   \n",
       "   13           SER             A               1          211                 ?   \n",
       "   14           PHE             A               1          241                 ?   \n",
       "   15           ILE             A               1          244                 ?   \n",
       "   16           CYS             A               1          245                 ?   \n",
       "   17           PHE             A               1          248                 ?   \n",
       "   18           ALA             A               1          249                 ?   \n",
       "   19           LEU             A               1          256                 ?   \n",
       "   20           GLU             A               1          279                 ?   \n",
       "   21           ASP             A               1          828                 ?   \n",
       "   22           ALA             A               1          829                 ?   \n",
       "   23           SER             A               1          830                 ?   \n",
       "   24           ILE             A               1          831                 ?   \n",
       "   25           GLY             A               1          832                 ?   \n",
       "   26           ASN             A               1          833                 ?   \n",
       "   27           THR             A               1          835                 ?   \n",
       "   28           GLY             A               1          836                 ?   \n",
       "   29           SER             A               1          837                 ?   \n",
       "   30           ALA             A               1          839                 ?   \n",
       "   31           VAL             A               1          840                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          132          GLU            A                  1              132   \n",
       "   1          133          THR            A                  1              133   \n",
       "   2          134          VAL            A                  1              134   \n",
       "   3          135          SER            A                  1              135   \n",
       "   4          137          LEU            A                  1              137   \n",
       "   5          138          THR            A                  1              138   \n",
       "   6          141          ALA            A                  1              141   \n",
       "   7          200          HIS            A                  1              200   \n",
       "   8          203          VAL            A                  1              203   \n",
       "   9          206          VAL            A                  1              206   \n",
       "   10         207          THR            A                  1              207   \n",
       "   11         208          ALA            A                  1              208   \n",
       "   12         210          TRP            A                  1              210   \n",
       "   13         211          SER            A                  1              211   \n",
       "   14         241          PHE            A                  1              241   \n",
       "   15         244          ILE            A                  1              244   \n",
       "   16         245          CYS            A                  1              245   \n",
       "   17         248          PHE            A                  1              248   \n",
       "   18         249          ALA            A                  1              249   \n",
       "   19         256          LEU            A                  1              256   \n",
       "   20         279          GLU            A                  1              279   \n",
       "   21         828          ASP            A                  1              828   \n",
       "   22         829          ALA            A                  1              829   \n",
       "   23         830          SER            A                  1              830   \n",
       "   24         831          ILE            A                  1              831   \n",
       "   25         832          GLY            A                  1              832   \n",
       "   26         833          ASN            A                  1              833   \n",
       "   27         835          THR            A                  1              835   \n",
       "   28         836          GLY            A                  1              836   \n",
       "   29         837          SER            A                  1              837   \n",
       "   30         839          ALA            A                  1              839   \n",
       "   31         840          VAL            A                  1              840   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP               P32418-2                    132   \n",
       "   1                      UNP               P32418-2                    133   \n",
       "   2                      UNP               P32418-2                    134   \n",
       "   3                      UNP               P32418-2                    135   \n",
       "   4                      UNP               P32418-2                    137   \n",
       "   5                      UNP               P32418-2                    138   \n",
       "   6                      UNP               P32418-2                    141   \n",
       "   7                      UNP               P32418-2                    200   \n",
       "   8                      UNP               P32418-2                    203   \n",
       "   9                      UNP               P32418-2                    206   \n",
       "   10                     UNP               P32418-2                    207   \n",
       "   11                     UNP               P32418-2                    208   \n",
       "   12                     UNP               P32418-2                    210   \n",
       "   13                     UNP               P32418-2                    211   \n",
       "   14                     UNP               P32418-2                    241   \n",
       "   15                     UNP               P32418-2                    244   \n",
       "   16                     UNP               P32418-2                    245   \n",
       "   17                     UNP               P32418-2                    248   \n",
       "   18                     UNP               P32418-2                    249   \n",
       "   19                     UNP               P32418-2                    256   \n",
       "   20                     UNP               P32418-2                    279   \n",
       "   21                     UNP               P32418-2                    828   \n",
       "   22                     UNP               P32418-2                    829   \n",
       "   23                     UNP               P32418-2                    830   \n",
       "   24                     UNP               P32418-2                    831   \n",
       "   25                     UNP               P32418-2                    832   \n",
       "   26                     UNP               P32418-2                    833   \n",
       "   27                     UNP               P32418-2                    835   \n",
       "   28                     UNP               P32418-2                    836   \n",
       "   29                     UNP               P32418-2                    837   \n",
       "   30                     UNP               P32418-2                    839   \n",
       "   31                     UNP               P32418-2                    840   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       E  \n",
       "   1                       T  \n",
       "   2                       V  \n",
       "   3                       S  \n",
       "   4                       L  \n",
       "   5                       T  \n",
       "   6                       A  \n",
       "   7                       H  \n",
       "   8                       V  \n",
       "   9                       V  \n",
       "   10                      T  \n",
       "   11                      A  \n",
       "   12                      W  \n",
       "   13                      S  \n",
       "   14                      F  \n",
       "   15                      I  \n",
       "   16                      C  \n",
       "   17                      F  \n",
       "   18                      A  \n",
       "   19                      L  \n",
       "   20                      E  \n",
       "   21                      D  \n",
       "   22                      A  \n",
       "   23                      S  \n",
       "   24                      I  \n",
       "   25                      G  \n",
       "   26                      N  \n",
       "   27                      T  \n",
       "   28                      G  \n",
       "   29                      S  \n",
       "   30                      A  \n",
       "   31                      V  }],\n",
       " '8k9l': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   8895           VV9             G               7            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   8895                 ?         401          VV9            R   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   8895                  1              401                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   8895                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            TYR             A               1          183                 ?   \n",
       "   1            PHE             A               1          229                 ?   \n",
       "   2            ILE             A               1          232                 ?   \n",
       "   3            TRP             A               1          233                 ?   \n",
       "   4            THR             A               1          234                 ?   \n",
       "   5            CYS             A               1          236                 ?   \n",
       "   6            THR             A               1          237                 ?   \n",
       "   7            MET             A               1          238                 ?   \n",
       "   8            SER             A               1          239                 ?   \n",
       "   9            VAL             A               1          240                 ?   \n",
       "   10           ASP             A               1          241                 ?   \n",
       "   11           ILE             A               1          244                 ?   \n",
       "   12           PHE             A               1          255                 ?   \n",
       "   13           ILE             A               1          264                 ?   \n",
       "   14           ASN             A               1          265                 ?   \n",
       "   15           CYS             A               1          267                 ?   \n",
       "   16           ASN             A               1          268                 ?   \n",
       "   17           LEU             A               1          271                 ?   \n",
       "   18           PHE             A               1          316                 ?   \n",
       "   19           ALA             A               1          317                 ?   \n",
       "   20           MET             A               1          320                 ?   \n",
       "   21           PRO             A               1          321                 ?   \n",
       "   22           ILE             A               1          324                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          108          TYR            R                  1              183   \n",
       "   1          154          PHE            R                  1              229   \n",
       "   2          157          ILE            R                  1              232   \n",
       "   3          158          TRP            R                  1              233   \n",
       "   4          159          THR            R                  1              234   \n",
       "   5          161          CYS            R                  1              236   \n",
       "   6          162          THR            R                  1              237   \n",
       "   7          163          MET            R                  1              238   \n",
       "   8          164          SER            R                  1              239   \n",
       "   9          165          VAL            R                  1              240   \n",
       "   10         166          ASP            R                  1              241   \n",
       "   11         169          ILE            R                  1              244   \n",
       "   12         180          PHE            R                  1              255   \n",
       "   13         189          ILE            R                  1              264   \n",
       "   14         190          ASN            R                  1              265   \n",
       "   15         192          CYS            R                  1              267   \n",
       "   16         193          ASN            R                  1              268   \n",
       "   17         196          LEU            R                  1              271   \n",
       "   18         241          PHE            R                  1              316   \n",
       "   19         242          ALA            R                  1              317   \n",
       "   20         245          MET            R                  1              320   \n",
       "   21         246          PRO            R                  1              321   \n",
       "   22         249          ILE            R                  1              324   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 P35372                    108   \n",
       "   1                      UNP                 P35372                    154   \n",
       "   2                      UNP                 P35372                    157   \n",
       "   3                      UNP                 P35372                    158   \n",
       "   4                      UNP                 P35372                    159   \n",
       "   5                      UNP                 P35372                    161   \n",
       "   6                      UNP                 P35372                    162   \n",
       "   7                      UNP                 P35372                    163   \n",
       "   8                      UNP                 P35372                    164   \n",
       "   9                      UNP                 P35372                    165   \n",
       "   10                     UNP                 P35372                    166   \n",
       "   11                     UNP                 P35372                    169   \n",
       "   12                     UNP                 P35372                    180   \n",
       "   13                     UNP                 P35372                    189   \n",
       "   14                     UNP                 P35372                    190   \n",
       "   15                     UNP                 P35372                    192   \n",
       "   16                     UNP                 P35372                    193   \n",
       "   17                     UNP                 P35372                    196   \n",
       "   18                     UNP                 P35372                    241   \n",
       "   19                     UNP                 P35372                    242   \n",
       "   20                     UNP                 P35372                    245   \n",
       "   21                     UNP                 P35372                    246   \n",
       "   22                     UNP                 P35372                    249   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       Y  \n",
       "   1                       F  \n",
       "   2                       I  \n",
       "   3                       F  \n",
       "   4                       T  \n",
       "   5                       C  \n",
       "   6                       T  \n",
       "   7                       M  \n",
       "   8                       S  \n",
       "   9                       V  \n",
       "   10                      D  \n",
       "   11                      I  \n",
       "   12                      F  \n",
       "   13                      I  \n",
       "   14                      N  \n",
       "   15                      C  \n",
       "   16                      N  \n",
       "   17                      L  \n",
       "   18                      F  \n",
       "   19                      A  \n",
       "   20                      M  \n",
       "   21                      P  \n",
       "   22                      I  }],\n",
       " '8qez': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   8304           UF5             M               5            .   \n",
       "   8383           UF5             U               5            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   8304                 ?         310          UF5            A   \n",
       "   8383                 ?         307          UF5            B   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   8304                  1              310                       ?   \n",
       "   8383                  1              307                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   8304                      ?                      ?                      ?  \n",
       "   8383                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            TYR             A               1           35                 ?   \n",
       "   1            THR             A               1           91                 ?   \n",
       "   2            ILE             A               1           92                 ?   \n",
       "   3            THR             A               1           93                 ?   \n",
       "   4            LYS             A               1          104                 ?   \n",
       "   5            PRO             A               1          105                 ?   \n",
       "   6            PHE             A               1          106                 ?   \n",
       "   7            MET             A               1          107                 ?   \n",
       "   8            SER             A               1          108                 ?   \n",
       "   9            LEU             A               1          109                 ?   \n",
       "   10           ASP             A               1          216                 ?   \n",
       "   11           SER             A               1          217                 ?   \n",
       "   12           LYS             A               1          218                 ?   \n",
       "   13           GLY             A               1          219                 ?   \n",
       "   14           TYR             A               1          220                 ?   \n",
       "   15           VAL             A               1          238                 ?   \n",
       "   16           LEU             A               1          239                 ?   \n",
       "   17           SER             A               1          242                 ?   \n",
       "   18           GLU             A               1          243                 ?   \n",
       "   19           LEU             A               1          247                 ?   \n",
       "   20           ASP             A               1          248                 ?   \n",
       "   21           TYR             B               1           35                 ?   \n",
       "   22           THR             B               1           91                 ?   \n",
       "   23           ILE             B               1           92                 ?   \n",
       "   24           THR             B               1           93                 ?   \n",
       "   25           LYS             B               1          104                 ?   \n",
       "   26           PRO             B               1          105                 ?   \n",
       "   27           PHE             B               1          106                 ?   \n",
       "   28           MET             B               1          107                 ?   \n",
       "   29           SER             B               1          108                 ?   \n",
       "   30           LEU             B               1          109                 ?   \n",
       "   31           ASP             B               1          216                 ?   \n",
       "   32           SER             B               1          217                 ?   \n",
       "   33           LYS             B               1          218                 ?   \n",
       "   34           GLY             B               1          219                 ?   \n",
       "   35           TYR             B               1          220                 ?   \n",
       "   36           VAL             B               1          238                 ?   \n",
       "   37           LEU             B               1          239                 ?   \n",
       "   38           SER             B               1          242                 ?   \n",
       "   39           GLU             B               1          243                 ?   \n",
       "   40           LEU             B               1          247                 ?   \n",
       "   41           ASP             B               1          248                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           35          TYR            A                  1               35   \n",
       "   1           91          THR            A                  1               91   \n",
       "   2           92          ILE            A                  1               92   \n",
       "   3           93          THR            A                  1               93   \n",
       "   4          104          LYS            A                  1              104   \n",
       "   5          105          PRO            A                  1              105   \n",
       "   6          106          PHE            A                  1              106   \n",
       "   7          107          MET            A                  1              107   \n",
       "   8          108          SER            A                  1              108   \n",
       "   9          109          LEU            A                  1              109   \n",
       "   10         216          ASP            A                  1              216   \n",
       "   11         217          SER            A                  1              217   \n",
       "   12         218          LYS            A                  1              218   \n",
       "   13         219          GLY            A                  1              219   \n",
       "   14         220          TYR            A                  1              220   \n",
       "   15         238          VAL            A                  1              238   \n",
       "   16         239          LEU            A                  1              239   \n",
       "   17         242          SER            A                  1              242   \n",
       "   18         243          GLU            A                  1              243   \n",
       "   19         247          LEU            A                  1              247   \n",
       "   20         248          ASP            A                  1              248   \n",
       "   21          35          TYR            B                  1               35   \n",
       "   22          91          THR            B                  1               91   \n",
       "   23          92          ILE            B                  1               92   \n",
       "   24          93          THR            B                  1               93   \n",
       "   25         104          LYS            B                  1              104   \n",
       "   26         105          PRO            B                  1              105   \n",
       "   27         106          PHE            B                  1              106   \n",
       "   28         107          MET            B                  1              107   \n",
       "   29         108          SER            B                  1              108   \n",
       "   30         109          LEU            B                  1              109   \n",
       "   31         216          ASP            B                  1              216   \n",
       "   32         217          SER            B                  1              217   \n",
       "   33         218          LYS            B                  1              218   \n",
       "   34         219          GLY            B                  1              219   \n",
       "   35         220          TYR            B                  1              220   \n",
       "   36         238          VAL            B                  1              238   \n",
       "   37         239          LEU            B                  1              239   \n",
       "   38         242          SER            B                  1              242   \n",
       "   39         243          GLU            B                  1              243   \n",
       "   40         247          LEU            B                  1              247   \n",
       "   41         248          ASP            B                  1              248   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 P19491                    445   \n",
       "   1                      UNP                 P19491                    501   \n",
       "   2                      UNP                 P19491                    502   \n",
       "   3                      UNP                 P19491                    503   \n",
       "   4                      UNP                 P19491                    514   \n",
       "   5                      UNP                 P19491                    515   \n",
       "   6                      UNP                 P19491                    516   \n",
       "   7                      UNP                 P19491                    517   \n",
       "   8                      UNP                 P19491                    518   \n",
       "   9                      UNP                 P19491                    519   \n",
       "   10                     UNP                 P19491                    749   \n",
       "   11                     UNP                 P19491                    750   \n",
       "   12                     UNP                 P19491                    751   \n",
       "   13                     UNP                 P19491                    752   \n",
       "   14                     UNP                 P19491                    753   \n",
       "   15                     UNP                 P19491                    771   \n",
       "   16                     UNP                 P19491                    772   \n",
       "   17                     UNP                 P19491                    775   \n",
       "   18                     UNP                 P19491                    776   \n",
       "   19                     UNP                 P19491                    780   \n",
       "   20                     UNP                 P19491                    781   \n",
       "   21                     UNP                 P19491                    445   \n",
       "   22                     UNP                 P19491                    501   \n",
       "   23                     UNP                 P19491                    502   \n",
       "   24                     UNP                 P19491                    503   \n",
       "   25                     UNP                 P19491                    514   \n",
       "   26                     UNP                 P19491                    515   \n",
       "   27                     UNP                 P19491                    516   \n",
       "   28                     UNP                 P19491                    517   \n",
       "   29                     UNP                 P19491                    518   \n",
       "   30                     UNP                 P19491                    519   \n",
       "   31                     UNP                 P19491                    749   \n",
       "   32                     UNP                 P19491                    750   \n",
       "   33                     UNP                 P19491                    751   \n",
       "   34                     UNP                 P19491                    752   \n",
       "   35                     UNP                 P19491                    753   \n",
       "   36                     UNP                 P19491                    771   \n",
       "   37                     UNP                 P19491                    772   \n",
       "   38                     UNP                 P19491                    775   \n",
       "   39                     UNP                 P19491                    776   \n",
       "   40                     UNP                 P19491                    780   \n",
       "   41                     UNP                 P19491                    781   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       Y  \n",
       "   1                       T  \n",
       "   2                       I  \n",
       "   3                       T  \n",
       "   4                       K  \n",
       "   5                       P  \n",
       "   6                       F  \n",
       "   7                       M  \n",
       "   8                       S  \n",
       "   9                       L  \n",
       "   10                      D  \n",
       "   11                      S  \n",
       "   12                      K  \n",
       "   13                      G  \n",
       "   14                      Y  \n",
       "   15                      V  \n",
       "   16                      L  \n",
       "   17                      N  \n",
       "   18                      E  \n",
       "   19                      L  \n",
       "   20                      D  \n",
       "   21                      Y  \n",
       "   22                      T  \n",
       "   23                      I  \n",
       "   24                      T  \n",
       "   25                      K  \n",
       "   26                      P  \n",
       "   27                      F  \n",
       "   28                      M  \n",
       "   29                      S  \n",
       "   30                      L  \n",
       "   31                      D  \n",
       "   32                      S  \n",
       "   33                      K  \n",
       "   34                      G  \n",
       "   35                      Y  \n",
       "   36                      V  \n",
       "   37                      L  \n",
       "   38                      N  \n",
       "   39                      E  \n",
       "   40                      L  \n",
       "   41                      D  }],\n",
       " '8qni': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   3044           W7R             B               2            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   3044                 ?         501          W7R            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   3044                  1              501                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   3044                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            ARG             A               1          108                 ?   \n",
       "   1            THR             A               1          111                 ?   \n",
       "   2            LYS             A               1          112                 ?   \n",
       "   3            LEU             A               1          113                 ?   \n",
       "   4            LEU             A               1          115                 ?   \n",
       "   5            ILE             A               1          116                 ?   \n",
       "   6            HIS             A               1          119                 ?   \n",
       "   7            SER             A               1          185                 ?   \n",
       "   8            THR             A               1          186                 ?   \n",
       "   9            ILE             A               1          187                 ?   \n",
       "   10           ASP             A               1          188                 ?   \n",
       "   11           LEU             A               1          189                 ?   \n",
       "   12           THR             A               1          190                 ?   \n",
       "   13           CYS             A               1          191                 ?   \n",
       "   14           LEU             A               1          206                 ?   \n",
       "   15           ALA             A               1          221                 ?   \n",
       "   16           TYR             A               1          227                 ?   \n",
       "   17           MET             A               1          228                 ?   \n",
       "   18           ALA             A               1          229                 ?   \n",
       "   19           PHE             A               1          230                 ?   \n",
       "   20           LEU             A               1          231                 ?   \n",
       "   21           GLU             A               1          235                 ?   \n",
       "   22           LEU             A               1          254                 ?   \n",
       "   23           LEU             A               1          329                 ?   \n",
       "   24           TYR             A               1          330                 ?   \n",
       "   25           CYS             A               1          331                 ?   \n",
       "   26           MET             A               1          333                 ?   \n",
       "   27           GLY             A               1          334                 ?   \n",
       "   28           SER             A               1          335                 ?   \n",
       "   29           THR             A               1          336                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          141          ARG            A                  1              108   \n",
       "   1          144          THR            A                  1              111   \n",
       "   2          145          LYS            A                  1              112   \n",
       "   3          146          LEU            A                  1              113   \n",
       "   4          148          LEU            A                  1              115   \n",
       "   5          149          ILE            A                  1              116   \n",
       "   6          152          HIS            A                  1              119   \n",
       "   7          218          SER            A                  1              185   \n",
       "   8          219          THR            A                  1              186   \n",
       "   9          220          ILE            A                  1              187   \n",
       "   10         221          ASP            A                  1              188   \n",
       "   11         222          LEU            A                  1              189   \n",
       "   12         223          THR            A                  1              190   \n",
       "   13         224          CYS            A                  1              191   \n",
       "   14         239          LEU            A                  1              206   \n",
       "   15         254          ALA            A                  1              221   \n",
       "   16         260          TYR            A                  1              227   \n",
       "   17         261          MET            A                  1              228   \n",
       "   18         262          ALA            A                  1              229   \n",
       "   19         263          PHE            A                  1              230   \n",
       "   20         264          LEU            A                  1              231   \n",
       "   21         268          GLU            A                  1              235   \n",
       "   22         287          LEU            A                  1              254   \n",
       "   23         362          LEU            A                  1              329   \n",
       "   24         363          TYR            A                  1              330   \n",
       "   25         364          CYS            A                  1              331   \n",
       "   26         366          MET            A                  1              333   \n",
       "   27         367          GLY            A                  1              334   \n",
       "   28         368          SER            A                  1              335   \n",
       "   29         369          THR            A                  1              336   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q13191                    141   \n",
       "   1                      UNP                 Q13191                    144   \n",
       "   2                      UNP                 Q13191                    145   \n",
       "   3                      UNP                 Q13191                    146   \n",
       "   4                      UNP                 Q13191                    148   \n",
       "   5                      UNP                 Q13191                    149   \n",
       "   6                      UNP                 Q13191                    152   \n",
       "   7                      UNP                 Q13191                    218   \n",
       "   8                      UNP                 Q13191                    219   \n",
       "   9                      UNP                 Q13191                    220   \n",
       "   10                     UNP                 Q13191                    221   \n",
       "   11                     UNP                 Q13191                    222   \n",
       "   12                     UNP                 Q13191                    223   \n",
       "   13                     UNP                 Q13191                    224   \n",
       "   14                     UNP                 Q13191                    239   \n",
       "   15                     UNP                 Q13191                    254   \n",
       "   16                     UNP                 Q13191                    260   \n",
       "   17                     UNP                 Q13191                    261   \n",
       "   18                     UNP                 Q13191                    262   \n",
       "   19                     UNP                 Q13191                    263   \n",
       "   20                     UNP                 Q13191                    264   \n",
       "   21                     UNP                 Q13191                    268   \n",
       "   22                     UNP                 Q13191                    287   \n",
       "   23                     UNP                 Q13191                    362   \n",
       "   24                     UNP                 Q13191                    363   \n",
       "   25                     UNP                 Q13191                    364   \n",
       "   26                     UNP                 Q13191                    366   \n",
       "   27                     UNP                 Q13191                    367   \n",
       "   28                     UNP                 Q13191                    368   \n",
       "   29                     UNP                 Q13191                    369   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       R  \n",
       "   1                       T  \n",
       "   2                       K  \n",
       "   3                       L  \n",
       "   4                       L  \n",
       "   5                       I  \n",
       "   6                       H  \n",
       "   7                       S  \n",
       "   8                       T  \n",
       "   9                       I  \n",
       "   10                      D  \n",
       "   11                      L  \n",
       "   12                      T  \n",
       "   13                      C  \n",
       "   14                      L  \n",
       "   15                      A  \n",
       "   16                      Y  \n",
       "   17                      M  \n",
       "   18                      A  \n",
       "   19                      F  \n",
       "   20                      L  \n",
       "   21                      E  \n",
       "   22                      L  \n",
       "   23                      L  \n",
       "   24                      Y  \n",
       "   25                      C  \n",
       "   26                      M  \n",
       "   27                      G  \n",
       "   28                      S  \n",
       "   29                      T  }],\n",
       " '8qns': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   3472           SER             E               2            2   \n",
       "   3478           TYR             E               2            3   \n",
       "   3490           CYS             E               2            4   \n",
       "   3496           ARG             E               2            5   \n",
       "   3507           GLN             E               2            6   \n",
       "   3516           GLU             E               2            7   \n",
       "   3525           GLY             E               2            8   \n",
       "   3529           LYS             E               2            9   \n",
       "   3538           ASP             E               2           10   \n",
       "   3546           ARG             E               2           11   \n",
       "   3557           ILE             E               2           12   \n",
       "   3565           ILE             E               2           13   \n",
       "   3573           PHE             E               2           14   \n",
       "   3584           VAL             E               2           15   \n",
       "   3591           THR             E               2           16   \n",
       "   3598           LYS             E               2           17   \n",
       "   3607           GLU             E               2           18   \n",
       "   3616           ASP             E               2           19   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   3472                 ?           2          SER            M   \n",
       "   3478                 ?           3          TYR            M   \n",
       "   3490                 ?           4          CYS            M   \n",
       "   3496                 ?           5          ARG            M   \n",
       "   3507                 ?           6          GLN            M   \n",
       "   3516                 ?           7          GLU            M   \n",
       "   3525                 ?           8          GLY            M   \n",
       "   3529                 ?           9          LYS            M   \n",
       "   3538                 ?          10          ASP            M   \n",
       "   3546                 ?          11          ARG            M   \n",
       "   3557                 ?          12          ILE            M   \n",
       "   3565                 ?          13          ILE            M   \n",
       "   3573                 ?          14          PHE            M   \n",
       "   3584                 ?          15          VAL            M   \n",
       "   3591                 ?          16          THR            M   \n",
       "   3598                 ?          17          LYS            M   \n",
       "   3607                 ?          18          GLU            M   \n",
       "   3616                 ?          19          ASP            M   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   3472                  1                2                     UNP   \n",
       "   3478                  1                3                     UNP   \n",
       "   3490                  1                4                     UNP   \n",
       "   3496                  1                5                     UNP   \n",
       "   3507                  1                6                     UNP   \n",
       "   3516                  1                7                     UNP   \n",
       "   3525                  1                8                     UNP   \n",
       "   3529                  1                9                     UNP   \n",
       "   3538                  1               10                     UNP   \n",
       "   3546                  1               11                     UNP   \n",
       "   3557                  1               12                     UNP   \n",
       "   3565                  1               13                     UNP   \n",
       "   3573                  1               14                     UNP   \n",
       "   3584                  1               15                     UNP   \n",
       "   3591                  1               16                     UNP   \n",
       "   3598                  1               17                     UNP   \n",
       "   3607                  1               18                     UNP   \n",
       "   3616                  1               19                     UNP   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   3472                 Q8VEA4                      2                      S  \n",
       "   3478                 Q8VEA4                      3                      Y  \n",
       "   3490                 Q8VEA4                      4                      C  \n",
       "   3496                 Q8VEA4                      5                      R  \n",
       "   3507                 Q8VEA4                      6                      Q  \n",
       "   3516                 Q8VEA4                      7                      E  \n",
       "   3525                 Q8VEA4                      8                      G  \n",
       "   3529                 Q8VEA4                      9                      K  \n",
       "   3538                 Q8VEA4                     10                      D  \n",
       "   3546                 Q8VEA4                     11                      R  \n",
       "   3557                 Q8VEA4                     12                      I  \n",
       "   3565                 Q8VEA4                     13                      I  \n",
       "   3573                 Q8VEA4                     14                      F  \n",
       "   3584                 Q8VEA4                     15                      V  \n",
       "   3591                 Q8VEA4                     16                      T  \n",
       "   3598                 Q8VEA4                     17                      K  \n",
       "   3607                 Q8VEA4                     18                      E  \n",
       "   3616                 Q8VEA4                     19                      D  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            LEU             A               1          243                 ?   \n",
       "   1            PRO             A               1          244                 ?   \n",
       "   2            GLN             A               1          245                 ?   \n",
       "   3            TYR             A               1          246                 ?   \n",
       "   4            ASP             A               1          398                 ?   \n",
       "   5            SER             A               1          399                 ?   \n",
       "   6            SER             A               1          400                 ?   \n",
       "   7            LEU             A               1          401                 ?   \n",
       "   8            PRO             A               1          402                 ?   \n",
       "   9            THR             A               1          403                 ?   \n",
       "   10           VAL             A               1          404                 ?   \n",
       "   11           GLY             A               1          405                 ?   \n",
       "   12           VAL             A               1          406                 ?   \n",
       "   13           PHE             A               1          407                 ?   \n",
       "   14           ALA             A               1          408                 ?   \n",
       "   15           LYS             A               1          409                 ?   \n",
       "   16           ALA             A               1          410                 ?   \n",
       "   17           THR             A               1          411                 ?   \n",
       "   18           GLU             A               1          457                 ?   \n",
       "   19           ASP             A               1          458                 ?   \n",
       "   20           TYR             A               1          459                 ?   \n",
       "   21           ASP             A               1          499                 ?   \n",
       "   22           ASN             A               1          501                 ?   \n",
       "   23           LYS             A               1          505                 ?   \n",
       "   24           ILE             A               1          509                 ?   \n",
       "   25           HIS             A               1          510                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          343          LEU            A                  1              243   \n",
       "   1          344          PRO            A                  1              244   \n",
       "   2          345          GLN            A                  1              245   \n",
       "   3          346          TYR            A                  1              246   \n",
       "   4          498          ASP            A                  1              398   \n",
       "   5          499          SER            A                  1              399   \n",
       "   6          500          SER            A                  1              400   \n",
       "   7          501          LEU            A                  1              401   \n",
       "   8          502          PRO            A                  1              402   \n",
       "   9          503          THR            A                  1              403   \n",
       "   10         504          VAL            A                  1              404   \n",
       "   11         505          GLY            A                  1              405   \n",
       "   12         506          VAL            A                  1              406   \n",
       "   13         507          PHE            A                  1              407   \n",
       "   14         508          ALA            A                  1              408   \n",
       "   15         509          LYS            A                  1              409   \n",
       "   16         510          ALA            A                  1              410   \n",
       "   17         511          THR            A                  1              411   \n",
       "   18         557          GLU            A                  1              457   \n",
       "   19         558          ASP            A                  1              458   \n",
       "   20         559          TYR            A                  1              459   \n",
       "   21         599          ASP            A                  1              499   \n",
       "   22         601          ASN            A                  1              501   \n",
       "   23         605          LYS            A                  1              505   \n",
       "   24         609          ILE            A                  1              509   \n",
       "   25         610          HIS            A                  1              510   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9Z0X1                    343   \n",
       "   1                      UNP                 Q9Z0X1                    344   \n",
       "   2                      UNP                 Q9Z0X1                    345   \n",
       "   3                      UNP                 Q9Z0X1                    346   \n",
       "   4                      UNP                 Q9Z0X1                    498   \n",
       "   5                      UNP                 Q9Z0X1                    499   \n",
       "   6                      UNP                 Q9Z0X1                    500   \n",
       "   7                      UNP                 Q9Z0X1                    501   \n",
       "   8                      UNP                 Q9Z0X1                    502   \n",
       "   9                      UNP                 Q9Z0X1                    503   \n",
       "   10                     UNP                 Q9Z0X1                    504   \n",
       "   11                     UNP                 Q9Z0X1                    505   \n",
       "   12                     UNP                 Q9Z0X1                    506   \n",
       "   13                     UNP                 Q9Z0X1                    507   \n",
       "   14                     UNP                 Q9Z0X1                    508   \n",
       "   15                     UNP                 Q9Z0X1                    509   \n",
       "   16                     UNP                 Q9Z0X1                    510   \n",
       "   17                     UNP                 Q9Z0X1                    511   \n",
       "   18                     UNP                 Q9Z0X1                    557   \n",
       "   19                     UNP                 Q9Z0X1                    558   \n",
       "   20                     UNP                 Q9Z0X1                    559   \n",
       "   21                     UNP                 Q9Z0X1                    599   \n",
       "   22                     UNP                 Q9Z0X1                    601   \n",
       "   23                     UNP                 Q9Z0X1                    605   \n",
       "   24                     UNP                 Q9Z0X1                    609   \n",
       "   25                     UNP                 Q9Z0X1                    610   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       L  \n",
       "   1                       P  \n",
       "   2                       Q  \n",
       "   3                       Y  \n",
       "   4                       D  \n",
       "   5                       S  \n",
       "   6                       S  \n",
       "   7                       L  \n",
       "   8                       P  \n",
       "   9                       T  \n",
       "   10                      V  \n",
       "   11                      G  \n",
       "   12                      V  \n",
       "   13                      F  \n",
       "   14                      A  \n",
       "   15                      K  \n",
       "   16                      A  \n",
       "   17                      T  \n",
       "   18                      E  \n",
       "   19                      D  \n",
       "   20                      Y  \n",
       "   21                      D  \n",
       "   22                      N  \n",
       "   23                      K  \n",
       "   24                      I  \n",
       "   25                      H  }],\n",
       " '8r4b': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   5956           GLN             B               2           13   \n",
       "   5965           ALA             B               2           14   \n",
       "   5970           GLY             B               2           15   \n",
       "   5974           GLY             B               2           16   \n",
       "   5978           SER             B               2           17   \n",
       "   ...            ...           ...             ...          ...   \n",
       "   6698           ASP             B               2          115   \n",
       "   6706           TYR             B               2          116   \n",
       "   6711           TRP             B               2          117   \n",
       "   6716           GLY             B               2          118   \n",
       "   6720           GLN             B               2          119   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   5956                 ?          13          GLN            B   \n",
       "   5965                 ?          14          ALA            B   \n",
       "   5970                 ?          15          GLY            B   \n",
       "   5974                 ?          16          GLY            B   \n",
       "   5978                 ?          17          SER            B   \n",
       "   ...                ...         ...          ...          ...   \n",
       "   6698                 ?         115          ASP            B   \n",
       "   6706                 ?         116          TYR            B   \n",
       "   6711                 ?         117          TRP            B   \n",
       "   6716                 ?         118          GLY            B   \n",
       "   6720                 ?         119          GLN            B   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   5956                  1               13                       ?   \n",
       "   5965                  1               14                       ?   \n",
       "   5970                  1               15                       ?   \n",
       "   5974                  1               16                       ?   \n",
       "   5978                  1               17                       ?   \n",
       "   ...                 ...              ...                     ...   \n",
       "   6698                  1              115                       ?   \n",
       "   6706                  1              116                       ?   \n",
       "   6711                  1              117                       ?   \n",
       "   6716                  1              118                       ?   \n",
       "   6720                  1              119                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   5956                      ?                      ?                      ?  \n",
       "   5965                      ?                      ?                      ?  \n",
       "   5970                      ?                      ?                      ?  \n",
       "   5974                      ?                      ?                      ?  \n",
       "   5978                      ?                      ?                      ?  \n",
       "   ...                     ...                    ...                    ...  \n",
       "   6698                      ?                      ?                      ?  \n",
       "   6706                      ?                      ?                      ?  \n",
       "   6711                      ?                      ?                      ?  \n",
       "   6716                      ?                      ?                      ?  \n",
       "   6720                      ?                      ?                      ?  \n",
       "   \n",
       "   [107 rows x 14 columns],\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            GLN             A               1          440                 ?   \n",
       "   1            SER             A               1          441                 ?   \n",
       "   2            LEU             A               1          452                 ?   \n",
       "   3            HIS             A               1          454                 ?   \n",
       "   4            GLN             A               1          456                 ?   \n",
       "   5            ASP             A               1          509                 ?   \n",
       "   6            GLU             A               1          510                 ?   \n",
       "   7            LEU             A               1          511                 ?   \n",
       "   8            GLU             A               1          597                 ?   \n",
       "   9            LYS             A               1          619                 ?   \n",
       "   10           SER             A               1          620                 ?   \n",
       "   11           VAL             A               1          622                 ?   \n",
       "   12           LEU             A               1          623                 ?   \n",
       "   13           HIS             A               1          624                 ?   \n",
       "   14           PRO             A               1          625                 ?   \n",
       "   15           ASP             A               1          626                 ?   \n",
       "   16           SER             A               1          627                 ?   \n",
       "   17           ILE             A               1          628                 ?   \n",
       "   18           TYR             A               1          629                 ?   \n",
       "   19           GLY             A               1          630                 ?   \n",
       "   20           THR             A               1          631                 ?   \n",
       "   21           PRO             A               1          632                 ?   \n",
       "   22           LEU             A               1          633                 ?   \n",
       "   23           ALA             A               1          634                 ?   \n",
       "   24           TYR             A               1          680                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          435          GLN            A                  1              440   \n",
       "   1          436          SER            A                  1              441   \n",
       "   2          447          LEU            A                  1              452   \n",
       "   3          449          HIS            A                  1              454   \n",
       "   4          451          GLN            A                  1              456   \n",
       "   5          504          ASP            A                  1              509   \n",
       "   6          505          GLU            A                  1              510   \n",
       "   7          506          LEU            A                  1              511   \n",
       "   8          592          GLU            A                  1              597   \n",
       "   9          614          LYS            A                  1              619   \n",
       "   10         615          SER            A                  1              620   \n",
       "   11         617          VAL            A                  1              622   \n",
       "   12         618          LEU            A                  1              623   \n",
       "   13         619          HIS            A                  1              624   \n",
       "   14         620          PRO            A                  1              625   \n",
       "   15         621          ASP            A                  1              626   \n",
       "   16         622          SER            A                  1              627   \n",
       "   17         623          ILE            A                  1              628   \n",
       "   18         624          TYR            A                  1              629   \n",
       "   19         625          GLY            A                  1              630   \n",
       "   20         626          THR            A                  1              631   \n",
       "   21         627          PRO            A                  1              632   \n",
       "   22         628          LEU            A                  1              633   \n",
       "   23         629          ALA            A                  1              634   \n",
       "   24         675          TYR            A                  1              680   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q8KC98                    435   \n",
       "   1                      UNP                 Q8KC98                    436   \n",
       "   2                      UNP                 Q8KC98                    447   \n",
       "   3                      UNP                 Q8KC98                    449   \n",
       "   4                      UNP                 Q8KC98                    451   \n",
       "   5                      UNP                 Q8KC98                    504   \n",
       "   6                      UNP                 Q8KC98                    505   \n",
       "   7                      UNP                 Q8KC98                    506   \n",
       "   8                      UNP                 Q8KC98                    592   \n",
       "   9                      UNP                 Q8KC98                    614   \n",
       "   10                     UNP                 Q8KC98                    615   \n",
       "   11                     UNP                 Q8KC98                    617   \n",
       "   12                     UNP                 Q8KC98                    618   \n",
       "   13                     UNP                 Q8KC98                    619   \n",
       "   14                     UNP                 Q8KC98                    620   \n",
       "   15                     UNP                 Q8KC98                    621   \n",
       "   16                     UNP                 Q8KC98                    622   \n",
       "   17                     UNP                 Q8KC98                    623   \n",
       "   18                     UNP                 Q8KC98                    624   \n",
       "   19                     UNP                 Q8KC98                    625   \n",
       "   20                     UNP                 Q8KC98                    626   \n",
       "   21                     UNP                 Q8KC98                    627   \n",
       "   22                     UNP                 Q8KC98                    628   \n",
       "   23                     UNP                 Q8KC98                    629   \n",
       "   24                     UNP                 Q8KC98                    675   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       Q  \n",
       "   1                       S  \n",
       "   2                       L  \n",
       "   3                       H  \n",
       "   4                       Q  \n",
       "   5                       D  \n",
       "   6                       E  \n",
       "   7                       L  \n",
       "   8                       E  \n",
       "   9                       K  \n",
       "   10                      S  \n",
       "   11                      V  \n",
       "   12                      L  \n",
       "   13                      H  \n",
       "   14                      P  \n",
       "   15                      D  \n",
       "   16                      S  \n",
       "   17                      I  \n",
       "   18                      Y  \n",
       "   19                      G  \n",
       "   20                      T  \n",
       "   21                      P  \n",
       "   22                      L  \n",
       "   23                      A  \n",
       "   24                      Y  },\n",
       "  {'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   6725           GLN             C               3            3   \n",
       "   6734           LEU             C               3            4   \n",
       "   6742           GLN             C               3            5   \n",
       "   6747           GLU             C               3            6   \n",
       "   6756           SER             C               3            7   \n",
       "   ...            ...           ...             ...          ...   \n",
       "   7535           THR             C               3          114   \n",
       "   7542           GLN             C               3          115   \n",
       "   7547           VAL             C               3          116   \n",
       "   7554           THR             C               3          117   \n",
       "   7561           VAL             C               3          118   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   6725                 ?           3          GLN            C   \n",
       "   6734                 ?           4          LEU            C   \n",
       "   6742                 ?           5          GLN            C   \n",
       "   6747                 ?           6          GLU            C   \n",
       "   6756                 ?           7          SER            C   \n",
       "   ...                ...         ...          ...          ...   \n",
       "   7535                 ?         114          THR            C   \n",
       "   7542                 ?         115          GLN            C   \n",
       "   7547                 ?         116          VAL            C   \n",
       "   7554                 ?         117          THR            C   \n",
       "   7561                 ?         118          VAL            C   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   6725                  1                3                       ?   \n",
       "   6734                  1                4                       ?   \n",
       "   6742                  1                5                       ?   \n",
       "   6747                  1                6                       ?   \n",
       "   6756                  1                7                       ?   \n",
       "   ...                 ...              ...                     ...   \n",
       "   7535                  1              114                       ?   \n",
       "   7542                  1              115                       ?   \n",
       "   7547                  1              116                       ?   \n",
       "   7554                  1              117                       ?   \n",
       "   7561                  1              118                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   6725                      ?                      ?                      ?  \n",
       "   6734                      ?                      ?                      ?  \n",
       "   6742                      ?                      ?                      ?  \n",
       "   6747                      ?                      ?                      ?  \n",
       "   6756                      ?                      ?                      ?  \n",
       "   ...                     ...                    ...                    ...  \n",
       "   7535                      ?                      ?                      ?  \n",
       "   7542                      ?                      ?                      ?  \n",
       "   7547                      ?                      ?                      ?  \n",
       "   7554                      ?                      ?                      ?  \n",
       "   7561                      ?                      ?                      ?  \n",
       "   \n",
       "   [116 rows x 14 columns],\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            TRP             A               1           31                 ?   \n",
       "   1            TYR             A               1           32                 ?   \n",
       "   2            LYS             A               1           34                 ?   \n",
       "   3            TYR             A               1           50                 ?   \n",
       "   4            ASP             A               1           51                 ?   \n",
       "   5            SER             A               1           77                 ?   \n",
       "   6            ARG             A               1           99                 ?   \n",
       "   7            TRP             A               1          118                 ?   \n",
       "   8            PHE             A               1          120                 ?   \n",
       "   9            GLY             A               1          121                 ?   \n",
       "   10           GLN             A               1          140                 ?   \n",
       "   11           SER             A               1          142                 ?   \n",
       "   12           SER             A               1          143                 ?   \n",
       "   13           SER             A               1          162                 ?   \n",
       "   14           LEU             A               1          163                 ?   \n",
       "   15           SER             A               1          164                 ?   \n",
       "   16           GLY             A               1          165                 ?   \n",
       "   17           LEU             A               1          183                 ?   \n",
       "   18           SER             A               1          184                 ?   \n",
       "   19           LEU             A               1          185                 ?   \n",
       "   20           SER             A               1          186                 ?   \n",
       "   21           SER             A               1          187                 ?   \n",
       "   22           GLU             A               1          204                 ?   \n",
       "   23           SER             A               1          206                 ?   \n",
       "   24           SER             A               1          208                 ?   \n",
       "   25           GLU             A               1          226                 ?   \n",
       "   26           GLN             A               1          228                 ?   \n",
       "   27           GLU             A               1          248                 ?   \n",
       "   28           GLN             A               1          250                 ?   \n",
       "   29           GLU             A               1          270                 ?   \n",
       "   30           GLN             A               1          272                 ?   \n",
       "   31           LYS             A               1          292                 ?   \n",
       "   32           TRP             A               1          294                 ?   \n",
       "   33           GLU             A               1          401                 ?   \n",
       "   34           ILE             A               1          402                 ?   \n",
       "   35           ASN             A               1          405                 ?   \n",
       "   36           PHE             A               1          407                 ?   \n",
       "   37           SER             A               1          408                 ?   \n",
       "   38           SER             A               1          409                 ?   \n",
       "   39           SER             A               1          410                 ?   \n",
       "   40           GLY             A               1          411                 ?   \n",
       "   41           PHE             A               1          412                 ?   \n",
       "   42           GLN             A               1          430                 ?   \n",
       "   43           GLY             A               1          431                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           26          TRP            A                  1               31   \n",
       "   1           27          TYR            A                  1               32   \n",
       "   2           29          LYS            A                  1               34   \n",
       "   3           45          TYR            A                  1               50   \n",
       "   4           46          ASP            A                  1               51   \n",
       "   5           72          SER            A                  1               77   \n",
       "   6           94          ARG            A                  1               99   \n",
       "   7          113          TRP            A                  1              118   \n",
       "   8          115          PHE            A                  1              120   \n",
       "   9          116          GLY            A                  1              121   \n",
       "   10         135          GLN            A                  1              140   \n",
       "   11         137          SER            A                  1              142   \n",
       "   12         138          SER            A                  1              143   \n",
       "   13         157          SER            A                  1              162   \n",
       "   14         158          LEU            A                  1              163   \n",
       "   15         159          SER            A                  1              164   \n",
       "   16         160          GLY            A                  1              165   \n",
       "   17         178          LEU            A                  1              183   \n",
       "   18         179          SER            A                  1              184   \n",
       "   19         180          LEU            A                  1              185   \n",
       "   20         181          SER            A                  1              186   \n",
       "   21         182          SER            A                  1              187   \n",
       "   22         199          GLU            A                  1              204   \n",
       "   23         201          SER            A                  1              206   \n",
       "   24         203          SER            A                  1              208   \n",
       "   25         221          GLU            A                  1              226   \n",
       "   26         223          GLN            A                  1              228   \n",
       "   27         243          GLU            A                  1              248   \n",
       "   28         245          GLN            A                  1              250   \n",
       "   29         265          GLU            A                  1              270   \n",
       "   30         267          GLN            A                  1              272   \n",
       "   31         287          LYS            A                  1              292   \n",
       "   32         289          TRP            A                  1              294   \n",
       "   33         396          GLU            A                  1              401   \n",
       "   34         397          ILE            A                  1              402   \n",
       "   35         400          ASN            A                  1              405   \n",
       "   36         402          PHE            A                  1              407   \n",
       "   37         403          SER            A                  1              408   \n",
       "   38         404          SER            A                  1              409   \n",
       "   39         405          SER            A                  1              410   \n",
       "   40         406          GLY            A                  1              411   \n",
       "   41         407          PHE            A                  1              412   \n",
       "   42         425          GLN            A                  1              430   \n",
       "   43         426          GLY            A                  1              431   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q8KC98                     26   \n",
       "   1                      UNP                 Q8KC98                     27   \n",
       "   2                      UNP                 Q8KC98                     29   \n",
       "   3                      UNP                 Q8KC98                     45   \n",
       "   4                      UNP                 Q8KC98                     46   \n",
       "   5                      UNP                 Q8KC98                     72   \n",
       "   6                      UNP                 Q8KC98                     94   \n",
       "   7                      UNP                 Q8KC98                    113   \n",
       "   8                      UNP                 Q8KC98                    115   \n",
       "   9                      UNP                 Q8KC98                    116   \n",
       "   10                     UNP                 Q8KC98                    135   \n",
       "   11                     UNP                 Q8KC98                    137   \n",
       "   12                     UNP                 Q8KC98                    138   \n",
       "   13                     UNP                 Q8KC98                    157   \n",
       "   14                     UNP                 Q8KC98                    158   \n",
       "   15                     UNP                 Q8KC98                    159   \n",
       "   16                     UNP                 Q8KC98                    160   \n",
       "   17                     UNP                 Q8KC98                    178   \n",
       "   18                     UNP                 Q8KC98                    179   \n",
       "   19                     UNP                 Q8KC98                    180   \n",
       "   20                     UNP                 Q8KC98                    181   \n",
       "   21                     UNP                 Q8KC98                    182   \n",
       "   22                     UNP                 Q8KC98                    199   \n",
       "   23                     UNP                 Q8KC98                    201   \n",
       "   24                     UNP                 Q8KC98                    203   \n",
       "   25                     UNP                 Q8KC98                    221   \n",
       "   26                     UNP                 Q8KC98                    223   \n",
       "   27                     UNP                 Q8KC98                    243   \n",
       "   28                     UNP                 Q8KC98                    245   \n",
       "   29                     UNP                 Q8KC98                    265   \n",
       "   30                     UNP                 Q8KC98                    267   \n",
       "   31                     UNP                 Q8KC98                    287   \n",
       "   32                     UNP                 Q8KC98                    289   \n",
       "   33                     UNP                 Q8KC98                    396   \n",
       "   34                     UNP                 Q8KC98                    397   \n",
       "   35                     UNP                 Q8KC98                    400   \n",
       "   36                     UNP                 Q8KC98                    402   \n",
       "   37                     UNP                 Q8KC98                    403   \n",
       "   38                     UNP                 Q8KC98                    404   \n",
       "   39                     UNP                 Q8KC98                    405   \n",
       "   40                     UNP                 Q8KC98                    406   \n",
       "   41                     UNP                 Q8KC98                    407   \n",
       "   42                     UNP                 Q8KC98                    425   \n",
       "   43                     UNP                 Q8KC98                    426   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       W  \n",
       "   1                       Y  \n",
       "   2                       K  \n",
       "   3                       Y  \n",
       "   4                       D  \n",
       "   5                       S  \n",
       "   6                       R  \n",
       "   7                       W  \n",
       "   8                       F  \n",
       "   9                       G  \n",
       "   10                      Q  \n",
       "   11                      S  \n",
       "   12                      S  \n",
       "   13                      S  \n",
       "   14                      L  \n",
       "   15                      S  \n",
       "   16                      G  \n",
       "   17                      L  \n",
       "   18                      S  \n",
       "   19                      L  \n",
       "   20                      S  \n",
       "   21                      S  \n",
       "   22                      E  \n",
       "   23                      S  \n",
       "   24                      S  \n",
       "   25                      E  \n",
       "   26                      Q  \n",
       "   27                      E  \n",
       "   28                      Q  \n",
       "   29                      E  \n",
       "   30                      Q  \n",
       "   31                      K  \n",
       "   32                      W  \n",
       "   33                      E  \n",
       "   34                      I  \n",
       "   35                      N  \n",
       "   36                      F  \n",
       "   37                      S  \n",
       "   38                      S  \n",
       "   39                      S  \n",
       "   40                      G  \n",
       "   41                      F  \n",
       "   42                      Q  \n",
       "   43                      G  }],\n",
       " '8tb7': [{'mod':       label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   13431           ZOB             E               5            .   \n",
       "   \n",
       "         pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   13431                 ?         601          ZOB            R   \n",
       "   \n",
       "         pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   13431                  1              601                       ?   \n",
       "   \n",
       "         pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   13431                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            PHE             A               1           85                 ?   \n",
       "   1            VAL             A               1           88                 ?   \n",
       "   2            VAL             A               1          142                 ?   \n",
       "   3            ILE             A               1          145                 ?   \n",
       "   4            ASN             A               1          146                 ?   \n",
       "   5            VAL             A               1          147                 ?   \n",
       "   6            ARG             A               1          149                 ?   \n",
       "   7            TYR             A               1          150                 ?   \n",
       "   8            TYR             A               1          152                 ?   \n",
       "   9            VAL             A               1          153                 ?   \n",
       "   10           LEU             A               1          232                 ?   \n",
       "   11           ILE             A               1          233                 ?   \n",
       "   12           LEU             A               1          234                 ?   \n",
       "   13           VAL             A               1          236                 ?   \n",
       "   14           TYR             A               1          237                 ?   \n",
       "   15           MET             A               1          240                 ?   \n",
       "   16           LYS             A               1          362                 ?   \n",
       "   17           VAL             A               1          364                 ?   \n",
       "   18           LYS             A               1          365                 ?   \n",
       "   19           ALA             A               1          366                 ?   \n",
       "   20           ALA             A               1          367                 ?   \n",
       "   21           VAL             A               1          368                 ?   \n",
       "   22           VAL             A               1          369                 ?   \n",
       "   23           LEU             A               1          370                 ?   \n",
       "   24           LEU             A               1          371                 ?   \n",
       "   25           ALA             A               1          372                 ?   \n",
       "   26           VAL             A               1          373                 ?   \n",
       "   27           GLY             A               1          374                 ?   \n",
       "   28           GLN             A               1          376                 ?   \n",
       "   29           ASN             A               1          418                 ?   \n",
       "   30           PHE             A               1          421                 ?   \n",
       "   31           TYR             A               1          422                 ?   \n",
       "   32           LEU             A               1          425                 ?   \n",
       "   33           ASN             A               1          426                 ?   \n",
       "   34           ARG             A               1          427                 ?   \n",
       "   35           GLN             A               1          428                 ?   \n",
       "   36           ILE             A               1          429                 ?   \n",
       "   37           ARG             A               1          430                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           85          PHE            R                  1               85   \n",
       "   1           88          VAL            R                  1               88   \n",
       "   2          142          VAL            R                  1              142   \n",
       "   3          145          ILE            R                  1              145   \n",
       "   4          146          ASN            R                  1              146   \n",
       "   5          147          VAL            R                  1              147   \n",
       "   6          149          ARG            R                  1              149   \n",
       "   7          150          TYR            R                  1              150   \n",
       "   8          152          TYR            R                  1              152   \n",
       "   9          153          VAL            R                  1              153   \n",
       "   10         232          LEU            R                  1              232   \n",
       "   11         233          ILE            R                  1              233   \n",
       "   12         234          LEU            R                  1              234   \n",
       "   13         236          VAL            R                  1              236   \n",
       "   14         237          TYR            R                  1              237   \n",
       "   15         240          MET            R                  1              240   \n",
       "   16         362          LYS            R                  1              362   \n",
       "   17         364          VAL            R                  1              364   \n",
       "   18         365          LYS            R                  1              365   \n",
       "   19         366          ALA            R                  1              366   \n",
       "   20         367          ALA            R                  1              367   \n",
       "   21         368          VAL            R                  1              368   \n",
       "   22         369          VAL            R                  1              369   \n",
       "   23         370          LEU            R                  1              370   \n",
       "   24         371          LEU            R                  1              371   \n",
       "   25         372          ALA            R                  1              372   \n",
       "   26         373          VAL            R                  1              373   \n",
       "   27         374          GLY            R                  1              374   \n",
       "   28         376          GLN            R                  1              376   \n",
       "   29         418          ASN            R                  1              418   \n",
       "   30         421          PHE            R                  1              421   \n",
       "   31         422          TYR            R                  1              422   \n",
       "   32         425          LEU            R                  1              425   \n",
       "   33         426          ASN            R                  1              426   \n",
       "   34         427          ARG            R                  1              427   \n",
       "   35         428          GLN            R                  1              428   \n",
       "   36         429          ILE            R                  1              429   \n",
       "   37         430          ARG            R                  1              430   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9BZJ8                     76   \n",
       "   1                      UNP                 Q9BZJ8                     79   \n",
       "   2                      UNP                 Q9BZJ8                    133   \n",
       "   3                      UNP                 Q9BZJ8                    136   \n",
       "   4                      UNP                 Q9BZJ8                    137   \n",
       "   5                      UNP                 Q9BZJ8                    138   \n",
       "   6                      UNP                 Q9BZJ8                    140   \n",
       "   7                      UNP                 Q9BZJ8                    141   \n",
       "   8                      UNP                 Q9BZJ8                    143   \n",
       "   9                      UNP                 Q9BZJ8                    144   \n",
       "   10                     UNP                 Q9BZJ8                    223   \n",
       "   11                     UNP                 Q9BZJ8                    224   \n",
       "   12                     UNP                 Q9BZJ8                    225   \n",
       "   13                     UNP                 Q9BZJ8                    227   \n",
       "   14                     UNP                 Q9BZJ8                    228   \n",
       "   15                     UNP                 Q9BZJ8                    231   \n",
       "   16                       ?                      ?                      ?   \n",
       "   17                       ?                      ?                      ?   \n",
       "   18                     UNP                 Q9BZJ8                    284   \n",
       "   19                     UNP                 Q9BZJ8                    285   \n",
       "   20                     UNP                 Q9BZJ8                    286   \n",
       "   21                     UNP                 Q9BZJ8                    287   \n",
       "   22                     UNP                 Q9BZJ8                    288   \n",
       "   23                     UNP                 Q9BZJ8                    289   \n",
       "   24                     UNP                 Q9BZJ8                    290   \n",
       "   25                     UNP                 Q9BZJ8                    291   \n",
       "   26                     UNP                 Q9BZJ8                    292   \n",
       "   27                     UNP                 Q9BZJ8                    293   \n",
       "   28                     UNP                 Q9BZJ8                    295   \n",
       "   29                     UNP                 Q9BZJ8                    337   \n",
       "   30                     UNP                 Q9BZJ8                    340   \n",
       "   31                     UNP                 Q9BZJ8                    341   \n",
       "   32                     UNP                 Q9BZJ8                    344   \n",
       "   33                     UNP                 Q9BZJ8                    345   \n",
       "   34                     UNP                 Q9BZJ8                    346   \n",
       "   35                     UNP                 Q9BZJ8                    347   \n",
       "   36                     UNP                 Q9BZJ8                    348   \n",
       "   37                     UNP                 Q9BZJ8                    349   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       F  \n",
       "   1                       V  \n",
       "   2                       V  \n",
       "   3                       I  \n",
       "   4                       N  \n",
       "   5                       V  \n",
       "   6                       R  \n",
       "   7                       Y  \n",
       "   8                       Y  \n",
       "   9                       V  \n",
       "   10                      L  \n",
       "   11                      I  \n",
       "   12                      L  \n",
       "   13                      V  \n",
       "   14                      Y  \n",
       "   15                      M  \n",
       "   16                      ?  \n",
       "   17                      ?  \n",
       "   18                      K  \n",
       "   19                      A  \n",
       "   20                      A  \n",
       "   21                      V  \n",
       "   22                      V  \n",
       "   23                      L  \n",
       "   24                      L  \n",
       "   25                      A  \n",
       "   26                      V  \n",
       "   27                      G  \n",
       "   28                      Q  \n",
       "   29                      N  \n",
       "   30                      F  \n",
       "   31                      Y  \n",
       "   32                      L  \n",
       "   33                      N  \n",
       "   34                      R  \n",
       "   35                      Q  \n",
       "   36                      I  \n",
       "   37                      R  }],\n",
       " '8uk6': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   4678           WVK             C               3            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   4678                 ?         802          WVK            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   4678                  1              802                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   4678                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            GLY             A               1          281                 ?   \n",
       "   1            PHE             A               1          282                 ?   \n",
       "   2            LEU             A               1          283                 ?   \n",
       "   3            HIS             A               1          284                 ?   \n",
       "   4            HIS             A               1          287                 ?   \n",
       "   5            GLY             A               1          512                 ?   \n",
       "   6            THR             A               1          513                 ?   \n",
       "   7            ILE             A               1          514                 ?   \n",
       "   8            MET             A               1          515                 ?   \n",
       "   9            SER             A               1          516                 ?   \n",
       "   10           LYS             A               1          517                 ?   \n",
       "   11           ILE             A               1          520                 ?   \n",
       "   12           TRP             A               1          532                 ?   \n",
       "   13           ARG             A               1          536                 ?   \n",
       "   14           LEU             A               1          537                 ?   \n",
       "   15           TYR             A               1          538                 ?   \n",
       "   16           THR             A               1          539                 ?   \n",
       "   17           GLN             A               1          718                 ?   \n",
       "   18           LEU             A               1          719                 ?   \n",
       "   19           PHE             A               1          720                 ?   \n",
       "   20           PRO             A               1          725                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          262          GLY            A                  1              281   \n",
       "   1          263          PHE            A                  1              282   \n",
       "   2          264          LEU            A                  1              283   \n",
       "   3          265          HIS            A                  1              284   \n",
       "   4          268          HIS            A                  1              287   \n",
       "   5          493          GLY            A                  1              512   \n",
       "   6          494          THR            A                  1              513   \n",
       "   7          495          ILE            A                  1              514   \n",
       "   8          496          MET            A                  1              515   \n",
       "   9          497          SER            A                  1              516   \n",
       "   10         498          LYS            A                  1              517   \n",
       "   11         501          ILE            A                  1              520   \n",
       "   12         513          TRP            A                  1              532   \n",
       "   13         517          ARG            A                  1              536   \n",
       "   14         518          LEU            A                  1              537   \n",
       "   15         519          TYR            A                  1              538   \n",
       "   16         520          THR            A                  1              539   \n",
       "   17         699          GLN            A                  1              718   \n",
       "   18         700          LEU            A                  1              719   \n",
       "   19         701          PHE            A                  1              720   \n",
       "   20         706          PRO            A                  1              725   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP             A0A1D8PQM9                    262   \n",
       "   1                      UNP             A0A1D8PQM9                    263   \n",
       "   2                      UNP             A0A1D8PQM9                    264   \n",
       "   3                      UNP             A0A1D8PQM9                    265   \n",
       "   4                      UNP             A0A1D8PQM9                    268   \n",
       "   5                      UNP             A0A1D8PQM9                    493   \n",
       "   6                      UNP             A0A1D8PQM9                    494   \n",
       "   7                      UNP             A0A1D8PQM9                    495   \n",
       "   8                      UNP             A0A1D8PQM9                    496   \n",
       "   9                      UNP             A0A1D8PQM9                    497   \n",
       "   10                     UNP             A0A1D8PQM9                    498   \n",
       "   11                     UNP             A0A1D8PQM9                    501   \n",
       "   12                     UNP             A0A1D8PQM9                    513   \n",
       "   13                     UNP             A0A1D8PQM9                    517   \n",
       "   14                     UNP             A0A1D8PQM9                    518   \n",
       "   15                     UNP             A0A1D8PQM9                    519   \n",
       "   16                     UNP             A0A1D8PQM9                    520   \n",
       "   17                     UNP             A0A1D8PQM9                    699   \n",
       "   18                     UNP             A0A1D8PQM9                    700   \n",
       "   19                     UNP             A0A1D8PQM9                    701   \n",
       "   20                     UNP             A0A1D8PQM9                    706   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       G  \n",
       "   1                       F  \n",
       "   2                       L  \n",
       "   3                       H  \n",
       "   4                       H  \n",
       "   5                       G  \n",
       "   6                       T  \n",
       "   7                       I  \n",
       "   8                       M  \n",
       "   9                       S  \n",
       "   10                      K  \n",
       "   11                      I  \n",
       "   12                      W  \n",
       "   13                      R  \n",
       "   14                      L  \n",
       "   15                      Y  \n",
       "   16                      T  \n",
       "   17                      Q  \n",
       "   18                      L  \n",
       "   19                      F  \n",
       "   20                      P  }],\n",
       " '8v81': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   9231           WG5             L               6            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   9231                 ?        1511          WG5            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   9231                  1             1511                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   9231                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            GLU             A               1           90                 ?   \n",
       "   1            LYS             A               1           93                 ?   \n",
       "   2            PHE             A               1          308                 ?   \n",
       "   3            PHE             A               1          309                 ?   \n",
       "   4            ILE             A               1          342                 ?   \n",
       "   5            VAL             A               1          343                 ?   \n",
       "   6            ARG             A               1          345                 ?   \n",
       "   7            MET             A               1          346                 ?   \n",
       "   8            GLN             A               1          351                 ?   \n",
       "   9            VAL             A               1          918                 ?   \n",
       "   10           GLY             A               1          919                 ?   \n",
       "   11           VAL             A               1          920                 ?   \n",
       "   12           ALA             A               1          921                 ?   \n",
       "   13           ASP             A               1          922                 ?   \n",
       "   14           THR             A               1          923                 ?   \n",
       "   15           LEU             A               1          925                 ?   \n",
       "   16           LEU             A               1          995                 ?   \n",
       "   17           ILE             A               1          998                 ?   \n",
       "   18           VAL             A               1          999                 ?   \n",
       "   19           ALA             A               1         1002                 ?   \n",
       "   20           LEU             A               1         1133                 ?   \n",
       "   21           MET             A               1         1135                 ?   \n",
       "   22           ASN             A               1         1136                 ?   \n",
       "   23           ILE             A               1         1137                 ?   \n",
       "   24           SER             A               1         1139                 ?   \n",
       "   25           THR             A               1         1140                 ?   \n",
       "   26           TRP             A               1         1143                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           92          GLU            A                  1               90   \n",
       "   1           95          LYS            A                  1               93   \n",
       "   2          310          PHE            A                  1              308   \n",
       "   3          311          PHE            A                  1              309   \n",
       "   4          344          ILE            A                  1              342   \n",
       "   5          345          VAL            A                  1              343   \n",
       "   6          347          ARG            A                  1              345   \n",
       "   7          348          MET            A                  1              346   \n",
       "   8          353          GLN            A                  1              351   \n",
       "   9          920          VAL            A                  1              918   \n",
       "   10         921          GLY            A                  1              919   \n",
       "   11         922          VAL            A                  1              920   \n",
       "   12         923          ALA            A                  1              921   \n",
       "   13         924          ASP            A                  1              922   \n",
       "   14         925          THR            A                  1              923   \n",
       "   15         927          LEU            A                  1              925   \n",
       "   16         997          LEU            A                  1              995   \n",
       "   17        1000          ILE            A                  1              998   \n",
       "   18        1001          VAL            A                  1              999   \n",
       "   19        1004          ALA            A                  1             1002   \n",
       "   20        1135          LEU            A                  1             1133   \n",
       "   21        1137          MET            A                  1             1135   \n",
       "   22        1138          ASN            A                  1             1136   \n",
       "   23        1139          ILE            A                  1             1137   \n",
       "   24        1141          SER            A                  1             1139   \n",
       "   25        1142          THR            A                  1             1140   \n",
       "   26        1145          TRP            A                  1             1143   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 P13569                     92   \n",
       "   1                      UNP                 P13569                     95   \n",
       "   2                      UNP                 P13569                    310   \n",
       "   3                      UNP                 P13569                    311   \n",
       "   4                      UNP                 P13569                    344   \n",
       "   5                      UNP                 P13569                    345   \n",
       "   6                      UNP                 P13569                    347   \n",
       "   7                      UNP                 P13569                    348   \n",
       "   8                      UNP                 P13569                    353   \n",
       "   9                      UNP                 P13569                    920   \n",
       "   10                     UNP                 P13569                    921   \n",
       "   11                     UNP                 P13569                    922   \n",
       "   12                     UNP                 P13569                    923   \n",
       "   13                     UNP                 P13569                    924   \n",
       "   14                     UNP                 P13569                    925   \n",
       "   15                     UNP                 P13569                    927   \n",
       "   16                     UNP                 P13569                    997   \n",
       "   17                     UNP                 P13569                   1000   \n",
       "   18                     UNP                 P13569                   1001   \n",
       "   19                     UNP                 P13569                   1004   \n",
       "   20                     UNP                 P13569                   1135   \n",
       "   21                     UNP                 P13569                   1137   \n",
       "   22                     UNP                 P13569                   1138   \n",
       "   23                     UNP                 P13569                   1139   \n",
       "   24                     UNP                 P13569                   1141   \n",
       "   25                     UNP                 P13569                   1142   \n",
       "   26                     UNP                 P13569                   1145   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       E  \n",
       "   1                       K  \n",
       "   2                       F  \n",
       "   3                       F  \n",
       "   4                       I  \n",
       "   5                       V  \n",
       "   6                       R  \n",
       "   7                       M  \n",
       "   8                       Q  \n",
       "   9                       V  \n",
       "   10                      G  \n",
       "   11                      V  \n",
       "   12                      A  \n",
       "   13                      D  \n",
       "   14                      T  \n",
       "   15                      L  \n",
       "   16                      L  \n",
       "   17                      I  \n",
       "   18                      V  \n",
       "   19                      A  \n",
       "   20                      L  \n",
       "   21                      M  \n",
       "   22                      N  \n",
       "   23                      I  \n",
       "   24                      S  \n",
       "   25                      T  \n",
       "   26                      W  }],\n",
       " '8vsd': [{'mod':       label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   3950            PHE             B               2            1   \n",
       "   3961            ASN             B               2            2   \n",
       "   3969            LEU             B               2            3   \n",
       "   3977            ASP             B               2            4   \n",
       "   3985            VAL             B               2            5   \n",
       "   ...             ...           ...             ...          ...   \n",
       "   10893           ILE             C               3          350   \n",
       "   10901           HIS             C               3          351   \n",
       "   10911           ARG             C               3          352   \n",
       "   10922           ASN             C               3          353   \n",
       "   10930           CYS             C               3          354   \n",
       "   \n",
       "         pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   3950                  ?           1          PHE            A   \n",
       "   3961                  ?           2          ASN            A   \n",
       "   3969                  ?           3          LEU            A   \n",
       "   3977                  ?           4          ASP            A   \n",
       "   3985                  ?           5          VAL            A   \n",
       "   ...                 ...         ...          ...          ...   \n",
       "   10893                 ?         421          ILE            B   \n",
       "   10901                 ?         422          HIS            B   \n",
       "   10911                 ?         423          ARG            B   \n",
       "   10922                 ?         424          ASN            B   \n",
       "   10930                 ?         425          CYS            B   \n",
       "   \n",
       "         pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   3950                   1                1                     UNP   \n",
       "   3961                   1                2                     UNP   \n",
       "   3969                   1                3                     UNP   \n",
       "   3977                   1                4                     UNP   \n",
       "   3985                   1                5                     UNP   \n",
       "   ...                  ...              ...                     ...   \n",
       "   10893                  1              350                     UNP   \n",
       "   10901                  1              351                     UNP   \n",
       "   10911                  1              352                     UNP   \n",
       "   10922                  1              353                     UNP   \n",
       "   10930                  1              354                     UNP   \n",
       "   \n",
       "         pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   3950                  P06756                     31                      F  \n",
       "   3961                  P06756                     32                      N  \n",
       "   3969                  P06756                     33                      L  \n",
       "   3977                  P06756                     34                      D  \n",
       "   3985                  P06756                     35                      V  \n",
       "   ...                      ...                    ...                    ...  \n",
       "   10893                 P26012                    463                      I  \n",
       "   10901                 P26012                    464                      H  \n",
       "   10911                 P26012                    465                      R  \n",
       "   10922                 P26012                    466                      N  \n",
       "   10930                 P26012                    467                      C  \n",
       "   \n",
       "   [942 rows x 14 columns],\n",
       "   'site':     label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   942           LEU             D               4          129   \n",
       "   943           LEU             D               4          131   \n",
       "   944           PRO             D               4          164   \n",
       "   945           GLU             D               4          165   \n",
       "   946           TRP             D               4          166   \n",
       "   947           ASN             D               4          208   \n",
       "   948           GLY             D               4          209   \n",
       "   949           THR             D               4          211   \n",
       "   950           THR             D               4          212   \n",
       "   951           GLY             D               4          213   \n",
       "   952           ARG             D               4          214   \n",
       "   953           ARG             D               4          215   \n",
       "   954           GLY             D               4          216   \n",
       "   955           ASP             D               4          217   \n",
       "   956           LEU             D               4          218   \n",
       "   957           THR             D               4          220   \n",
       "   958           ILE             D               4          221   \n",
       "   959           HIS             D               4          222   \n",
       "   960           GLY             D               4          223   \n",
       "   961           MET             D               4          224   \n",
       "   962           ASN             D               4          225   \n",
       "   963           PRO             E               4           43   \n",
       "   964           LEU             E               4           44   \n",
       "   965           PRO             E               4           45   \n",
       "   966           GLU             E               4           46   \n",
       "   967           ALA             E               4           47   \n",
       "   \n",
       "       pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   942                 ?         129          LEU            E   \n",
       "   943                 ?         131          LEU            E   \n",
       "   944                 ?         164          PRO            E   \n",
       "   945                 ?         165          GLU            E   \n",
       "   946                 ?         166          TRP            E   \n",
       "   947                 ?         208          ASN            E   \n",
       "   948                 ?         209          GLY            E   \n",
       "   949                 ?         211          THR            E   \n",
       "   950                 ?         212          THR            E   \n",
       "   951                 ?         213          GLY            E   \n",
       "   952                 ?         214          ARG            E   \n",
       "   953                 ?         215          ARG            E   \n",
       "   954                 ?         216          GLY            E   \n",
       "   955                 ?         217          ASP            E   \n",
       "   956                 ?         218          LEU            E   \n",
       "   957                 ?         220          THR            E   \n",
       "   958                 ?         221          ILE            E   \n",
       "   959                 ?         222          HIS            E   \n",
       "   960                 ?         223          GLY            E   \n",
       "   961                 ?         224          MET            E   \n",
       "   962                 ?         225          ASN            E   \n",
       "   963                 ?          43          PRO            F   \n",
       "   964                 ?          44          LEU            F   \n",
       "   965                 ?          45          PRO            F   \n",
       "   966                 ?          46          GLU            F   \n",
       "   967                 ?          47          ALA            F   \n",
       "   \n",
       "       pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   942                  1              129                     UNP   \n",
       "   943                  1              131                     UNP   \n",
       "   944                  1              164                     UNP   \n",
       "   945                  1              165                     UNP   \n",
       "   946                  1              166                     UNP   \n",
       "   947                  1              208                     UNP   \n",
       "   948                  1              209                     UNP   \n",
       "   949                  1              211                     UNP   \n",
       "   950                  1              212                     UNP   \n",
       "   951                  1              213                     UNP   \n",
       "   952                  1              214                     UNP   \n",
       "   953                  1              215                     UNP   \n",
       "   954                  1              216                     UNP   \n",
       "   955                  1              217                     UNP   \n",
       "   956                  1              218                     UNP   \n",
       "   957                  1              220                     UNP   \n",
       "   958                  1              221                     UNP   \n",
       "   959                  1              222                     UNP   \n",
       "   960                  1              223                     UNP   \n",
       "   961                  1              224                     UNP   \n",
       "   962                  1              225                     UNP   \n",
       "   963                  1               43                     UNP   \n",
       "   964                  1               44                     UNP   \n",
       "   965                  1               45                     UNP   \n",
       "   966                  1               46                     UNP   \n",
       "   967                  1               47                     UNP   \n",
       "   \n",
       "       pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   942                 P01137                    158                      L  \n",
       "   943                 P01137                    160                      L  \n",
       "   944                 P01137                    193                      P  \n",
       "   945                 P01137                    194                      E  \n",
       "   946                 P01137                    195                      W  \n",
       "   947                 P01137                    237                      N  \n",
       "   948                 P01137                    238                      G  \n",
       "   949                 P01137                    240                      T  \n",
       "   950                 P01137                    241                      T  \n",
       "   951                 P01137                    242                      G  \n",
       "   952                 P01137                    243                      R  \n",
       "   953                 P01137                    244                      R  \n",
       "   954                 P01137                    245                      G  \n",
       "   955                 P01137                    246                      D  \n",
       "   956                 P01137                    247                      L  \n",
       "   957                 P01137                    249                      T  \n",
       "   958                 P01137                    250                      I  \n",
       "   959                 P01137                    251                      H  \n",
       "   960                 P01137                    252                      G  \n",
       "   961                 P01137                    253                      M  \n",
       "   962                 P01137                    254                      N  \n",
       "   963                 P01137                     72                      P  \n",
       "   964                 P01137                     73                      L  \n",
       "   965                 P01137                     74                      P  \n",
       "   966                 P01137                     75                      E  \n",
       "   967                 P01137                     76                      A  }],\n",
       " '8wgb': [{'mod':       label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   16777           W9R             H               7            .   \n",
       "   \n",
       "         pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   16777                 ?        1002          W9R            R   \n",
       "   \n",
       "         pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   16777                  1             1002                       ?   \n",
       "   \n",
       "         pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   16777                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            THR             E               5          607                 ?   \n",
       "   1            MET             E               5          611                 ?   \n",
       "   2            ARG             E               5          623                 ?   \n",
       "   3            ARG             E               5          624                 ?   \n",
       "   4            LEU             E               5          627                 ?   \n",
       "   5            LEU             E               5          712                 ?   \n",
       "   6            LYS             E               5          713                 ?   \n",
       "   7            CYS             E               5          714                 ?   \n",
       "   8            ASP             E               5          715                 ?   \n",
       "   9            ILE             E               5          716                 ?   \n",
       "   10           SER             E               5          717                 ?   \n",
       "   11           ASP             E               5          718                 ?   \n",
       "   12           SER             E               5          720                 ?   \n",
       "   13           LEU             E               5          721                 ?   \n",
       "   14           ILE             E               5          722                 ?   \n",
       "   15           LEU             E               5          724                 ?   \n",
       "   16           PHE             E               5          773                 ?   \n",
       "   17           GLN             E               5          786                 ?   \n",
       "   18           THR             E               5          787                 ?   \n",
       "   19           LEU             E               5          790                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          639          THR            R                  1              607   \n",
       "   1          643          MET            R                  1              611   \n",
       "   2          655          ARG            R                  1              623   \n",
       "   3          656          ARG            R                  1              624   \n",
       "   4          659          LEU            R                  1              627   \n",
       "   5          744          LEU            R                  1              712   \n",
       "   6          745          LYS            R                  1              713   \n",
       "   7          746          CYS            R                  1              714   \n",
       "   8          747          ASP            R                  1              715   \n",
       "   9          748          ILE            R                  1              716   \n",
       "   10         749          SER            R                  1              717   \n",
       "   11         750          ASP            R                  1              718   \n",
       "   12         752          SER            R                  1              720   \n",
       "   13         753          LEU            R                  1              721   \n",
       "   14         754          ILE            R                  1              722   \n",
       "   15         756          LEU            R                  1              724   \n",
       "   16         805          PHE            R                  1              773   \n",
       "   17         818          GLN            R                  1              786   \n",
       "   18         819          THR            R                  1              787   \n",
       "   19         822          LEU            R                  1              790   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q14833                    639   \n",
       "   1                      UNP                 Q14833                    643   \n",
       "   2                      UNP                 Q14833                    655   \n",
       "   3                      UNP                 Q14833                    656   \n",
       "   4                      UNP                 Q14833                    659   \n",
       "   5                      UNP                 Q14833                    744   \n",
       "   6                      UNP                 Q14833                    745   \n",
       "   7                      UNP                 Q14833                    746   \n",
       "   8                      UNP                 Q14833                    747   \n",
       "   9                      UNP                 Q14833                    748   \n",
       "   10                     UNP                 Q14833                    749   \n",
       "   11                     UNP                 Q14833                    750   \n",
       "   12                     UNP                 Q14833                    752   \n",
       "   13                     UNP                 Q14833                    753   \n",
       "   14                     UNP                 Q14833                    754   \n",
       "   15                     UNP                 Q14833                    756   \n",
       "   16                     UNP                 Q14833                    805   \n",
       "   17                     UNP                 Q14833                    818   \n",
       "   18                     UNP                 Q14833                    819   \n",
       "   19                     UNP                 Q14833                    822   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       T  \n",
       "   1                       M  \n",
       "   2                       R  \n",
       "   3                       R  \n",
       "   4                       L  \n",
       "   5                       L  \n",
       "   6                       K  \n",
       "   7                       C  \n",
       "   8                       D  \n",
       "   9                       I  \n",
       "   10                      S  \n",
       "   11                      D  \n",
       "   12                      S  \n",
       "   13                      L  \n",
       "   14                      I  \n",
       "   15                      L  \n",
       "   16                      F  \n",
       "   17                      Q  \n",
       "   18                      T  \n",
       "   19                      L  }],\n",
       " '8x0f': [{'mod':       label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   12017           XRQ             E               4            .   \n",
       "   \n",
       "         pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   12017                 ?         903          XRQ            A   \n",
       "   \n",
       "         pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   12017                  1              903                       ?   \n",
       "   \n",
       "         pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   12017                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            ILE             A               1          605                 ?   \n",
       "   1            ILE             A               1          631                 ?   \n",
       "   2            SER             A               1          634                 ?   \n",
       "   3            PRO             A               1          635                 ?   \n",
       "   4            ALA             A               1          636                 ?   \n",
       "   5            SER             A               1          638                 ?   \n",
       "   6            TYR             A               1          639                 ?   \n",
       "   7            SER             A               1          640                 ?   \n",
       "   8            LEU             A               1          642                 ?   \n",
       "   9            VAL             A               1          720                 ?   \n",
       "   10           VAL             A               1          721                 ?   \n",
       "   11           PRO             A               1          723                 ?   \n",
       "   12           LEU             A               1          724                 ?   \n",
       "   13           ASN             A               1          727                 ?   \n",
       "   14           ILE             A               1          731                 ?   \n",
       "   15           THR             A               1          760                 ?   \n",
       "   16           THR             A               1          761                 ?   \n",
       "   17           CYS             A               1          762                 ?   \n",
       "   18           ILE             A               1          764                 ?   \n",
       "   19           TRP             A               1          765                 ?   \n",
       "   20           LEU             A               1          766                 ?   \n",
       "   21           PHE             A               1          768                 ?   \n",
       "   22           VAL             A               1          769                 ?   \n",
       "   23           TYR             A               1          772                 ?   \n",
       "   24           MET             A               1          782                 ?   \n",
       "   25           SER             A               1          785                 ?   \n",
       "   26           VAL             A               1          786                 ?   \n",
       "   27           SER             A               1          787                 ?   \n",
       "   28           SER             A               1          789                 ?   \n",
       "   29           ALA             A               1          790                 ?   \n",
       "   30           LEU             A               1          793                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          625          ILE            A                  1              605   \n",
       "   1          651          ILE            A                  1              631   \n",
       "   2          654          SER            A                  1              634   \n",
       "   3          655          PRO            A                  1              635   \n",
       "   4          656          ALA            A                  1              636   \n",
       "   5          658          SER            A                  1              638   \n",
       "   6          659          TYR            A                  1              639   \n",
       "   7          660          SER            A                  1              640   \n",
       "   8          662          LEU            A                  1              642   \n",
       "   9          740          VAL            A                  1              720   \n",
       "   10         741          VAL            A                  1              721   \n",
       "   11         743          PRO            A                  1              723   \n",
       "   12         744          LEU            A                  1              724   \n",
       "   13         747          ASN            A                  1              727   \n",
       "   14         751          ILE            A                  1              731   \n",
       "   15         780          THR            A                  1              760   \n",
       "   16         781          THR            A                  1              761   \n",
       "   17         782          CYS            A                  1              762   \n",
       "   18         784          ILE            A                  1              764   \n",
       "   19         785          TRP            A                  1              765   \n",
       "   20         786          LEU            A                  1              766   \n",
       "   21         788          PHE            A                  1              768   \n",
       "   22         789          VAL            A                  1              769   \n",
       "   23         792          TYR            A                  1              772   \n",
       "   24         802          MET            A                  1              782   \n",
       "   25         805          SER            A                  1              785   \n",
       "   26         806          VAL            A                  1              786   \n",
       "   27         807          SER            A                  1              787   \n",
       "   28         809          SER            A                  1              789   \n",
       "   29         810          ALA            A                  1              790   \n",
       "   30         813          LEU            A                  1              793   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 P41594                    625   \n",
       "   1                      UNP                 P41594                    651   \n",
       "   2                      UNP                 P41594                    654   \n",
       "   3                      UNP                 P41594                    655   \n",
       "   4                      UNP                 P41594                    656   \n",
       "   5                      UNP                 P41594                    658   \n",
       "   6                      UNP                 P41594                    659   \n",
       "   7                      UNP                 P41594                    660   \n",
       "   8                      UNP                 P41594                    662   \n",
       "   9                      UNP                 P41594                    740   \n",
       "   10                     UNP                 P41594                    741   \n",
       "   11                     UNP                 P41594                    743   \n",
       "   12                     UNP                 P41594                    744   \n",
       "   13                     UNP                 P41594                    747   \n",
       "   14                     UNP                 P41594                    751   \n",
       "   15                     UNP                 P41594                    780   \n",
       "   16                     UNP                 P41594                    781   \n",
       "   17                     UNP                 P41594                    782   \n",
       "   18                     UNP                 P41594                    784   \n",
       "   19                     UNP                 P41594                    785   \n",
       "   20                     UNP                 P41594                    786   \n",
       "   21                     UNP                 P41594                    788   \n",
       "   22                     UNP                 P41594                    789   \n",
       "   23                     UNP                 P41594                    792   \n",
       "   24                     UNP                 P41594                    802   \n",
       "   25                     UNP                 P41594                    805   \n",
       "   26                     UNP                 P41594                    806   \n",
       "   27                     UNP                 P41594                    807   \n",
       "   28                     UNP                 P41594                    809   \n",
       "   29                     UNP                 P41594                    810   \n",
       "   30                     UNP                 P41594                    813   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       I  \n",
       "   1                       I  \n",
       "   2                       S  \n",
       "   3                       P  \n",
       "   4                       A  \n",
       "   5                       S  \n",
       "   6                       Y  \n",
       "   7                       S  \n",
       "   8                       L  \n",
       "   9                       V  \n",
       "   10                      V  \n",
       "   11                      P  \n",
       "   12                      L  \n",
       "   13                      N  \n",
       "   14                      I  \n",
       "   15                      T  \n",
       "   16                      T  \n",
       "   17                      C  \n",
       "   18                      I  \n",
       "   19                      W  \n",
       "   20                      L  \n",
       "   21                      F  \n",
       "   22                      V  \n",
       "   23                      Y  \n",
       "   24                      M  \n",
       "   25                      S  \n",
       "   26                      V  \n",
       "   27                      S  \n",
       "   28                      S  \n",
       "   29                      A  \n",
       "   30                      A  }],\n",
       " '9b96': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   5073         A1AJA             B               2            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   5073                 ?         701        A1AJA            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   5073                  1              701                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   5073                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            SER             A               1          455                 ?   \n",
       "   1            PHE             A               1          456                 ?   \n",
       "   2            PRO             A               1          457                 ?   \n",
       "   3            LEU             A               1          458                 ?   \n",
       "   4            GLY             A               1          460                 ?   \n",
       "   5            GLY             A               1          461                 ?   \n",
       "   6            ARG             A               1          462                 ?   \n",
       "   7            TYR             A               1          484                 ?   \n",
       "   8            ASP             A               1          486                 ?   \n",
       "   9            TRP             A               1          487                 ?   \n",
       "   10           TYR             A               1          564                 ?   \n",
       "   11           ARG             A               1          567                 ?   \n",
       "   12           CYS             A               1          568                 ?   \n",
       "   13           TRP             A               1          571                 ?   \n",
       "   14           ASN             A               1          572                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0          434          SER            A                  1              455   \n",
       "   1          435          PHE            A                  1              456   \n",
       "   2          436          PRO            A                  1              457   \n",
       "   3          437          LEU            A                  1              458   \n",
       "   4          439          GLY            A                  1              460   \n",
       "   5          440          GLY            A                  1              461   \n",
       "   6          441          ARG            A                  1              462   \n",
       "   7          463          TYR            A                  1              484   \n",
       "   8          465          ASP            A                  1              486   \n",
       "   9          466          TRP            A                  1              487   \n",
       "   10         543          TYR            A                  1              564   \n",
       "   11         546          ARG            A                  1              567   \n",
       "   12         547          CYS            A                  1              568   \n",
       "   13         550          TRP            A                  1              571   \n",
       "   14         551          ASN            A                  1              572   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9Y2J8                    434   \n",
       "   1                      UNP                 Q9Y2J8                    435   \n",
       "   2                      UNP                 Q9Y2J8                    436   \n",
       "   3                      UNP                 Q9Y2J8                    437   \n",
       "   4                      UNP                 Q9Y2J8                    439   \n",
       "   5                      UNP                 Q9Y2J8                    440   \n",
       "   6                      UNP                 Q9Y2J8                    441   \n",
       "   7                      UNP                 Q9Y2J8                    463   \n",
       "   8                      UNP                 Q9Y2J8                    465   \n",
       "   9                      UNP                 Q9Y2J8                    466   \n",
       "   10                     UNP                 Q9Y2J8                    543   \n",
       "   11                     UNP                 Q9Y2J8                    546   \n",
       "   12                     UNP                 Q9Y2J8                    547   \n",
       "   13                     UNP                 Q9Y2J8                    550   \n",
       "   14                     UNP                 Q9Y2J8                    551   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       S  \n",
       "   1                       F  \n",
       "   2                       P  \n",
       "   3                       L  \n",
       "   4                       G  \n",
       "   5                       G  \n",
       "   6                       R  \n",
       "   7                       Y  \n",
       "   8                       D  \n",
       "   9                       W  \n",
       "   10                      Y  \n",
       "   11                      R  \n",
       "   12                      C  \n",
       "   13                      W  \n",
       "   14                      N  }],\n",
       " '9bjk': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   2481         A1APU             D               4            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   2481                 ?         502        A1APU            R   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   2481                  1              502                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   2481                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            ILE             B               2           91                 ?   \n",
       "   1            LEU             B               2           94                 ?   \n",
       "   2            TYR             B               2           95                 ?   \n",
       "   3            VAL             B               2           98                 ?   \n",
       "   4            THR             B               2          140                 ?   \n",
       "   5            LEU             B               2          141                 ?   \n",
       "   6            PHE             B               2          143                 ?   \n",
       "   7            GLN             B               2          144                 ?   \n",
       "   8            ASN             B               2          147                 ?   \n",
       "   9            TYR             B               2          148                 ?   \n",
       "   10           TRP             B               2          153                 ?   \n",
       "   11           CYS             B               2          160                 ?   \n",
       "   12           VAL             B               2          163                 ?   \n",
       "   13           ILE             B               2          164                 ?   \n",
       "   14           ASP             B               2          167                 ?   \n",
       "   15           TYR             B               2          168                 ?   \n",
       "   16           CYS             B               2          237                 ?   \n",
       "   17           THR             B               2          238                 ?   \n",
       "   18           LEU             B               2          239                 ?   \n",
       "   19           TRP             B               2          338                 ?   \n",
       "   20           HIS             B               2          339                 ?   \n",
       "   21           PHE             B               2          340                 ?   \n",
       "   22           ILE             B               2          342                 ?   \n",
       "   23           ALA             B               2          343                 ?   \n",
       "   24           LEU             B               2          344                 ?   \n",
       "   25           TYR             B               2          346                 ?   \n",
       "   26           THR             B               2          347                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           71          ILE            R                  1               91   \n",
       "   1           74          LEU            R                  1               94   \n",
       "   2           75          TYR            R                  1               95   \n",
       "   3           78          VAL            R                  1               98   \n",
       "   4          120          THR            R                  1              140   \n",
       "   5          121          LEU            R                  1              141   \n",
       "   6          123          PHE            R                  1              143   \n",
       "   7          124          GLN            R                  1              144   \n",
       "   8          127          ASN            R                  1              147   \n",
       "   9          128          TYR            R                  1              148   \n",
       "   10         133          TRP            R                  1              153   \n",
       "   11         140          CYS            R                  1              160   \n",
       "   12         143          VAL            R                  1              163   \n",
       "   13         144          ILE            R                  1              164   \n",
       "   14         147          ASP            R                  1              167   \n",
       "   15         148          TYR            R                  1              168   \n",
       "   16         217          CYS            R                  1              237   \n",
       "   17         218          THR            R                  1              238   \n",
       "   18         219          LEU            R                  1              239   \n",
       "   19         318          TRP            R                  1              338   \n",
       "   20         319          HIS            R                  1              339   \n",
       "   21         320          PHE            R                  1              340   \n",
       "   22         322          ILE            R                  1              342   \n",
       "   23         323          ALA            R                  1              343   \n",
       "   24         324          LEU            R                  1              344   \n",
       "   25         326          TYR            R                  1              346   \n",
       "   26         327          THR            R                  1              347   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 P42866                     71   \n",
       "   1                      UNP                 P42866                     74   \n",
       "   2                      UNP                 P42866                     75   \n",
       "   3                      UNP                 P42866                     78   \n",
       "   4                      UNP                 P42866                    120   \n",
       "   5                      UNP                 P42866                    121   \n",
       "   6                      UNP                 P42866                    123   \n",
       "   7                      UNP                 P42866                    124   \n",
       "   8                      UNP                 P42866                    127   \n",
       "   9                      UNP                 P42866                    128   \n",
       "   10                     UNP                 P42866                    133   \n",
       "   11                     UNP                 P42866                    140   \n",
       "   12                     UNP                 P42866                    143   \n",
       "   13                     UNP                 P42866                    144   \n",
       "   14                     UNP                 P42866                    147   \n",
       "   15                     UNP                 P42866                    148   \n",
       "   16                     UNP                 P42866                    217   \n",
       "   17                     UNP                 P42866                    218   \n",
       "   18                     UNP                 P42866                    219   \n",
       "   19                     UNP                 P42866                    318   \n",
       "   20                     UNP                 P42866                    319   \n",
       "   21                     UNP                 P42866                    320   \n",
       "   22                     UNP                 P42866                    322   \n",
       "   23                     UNP                 P42866                    323   \n",
       "   24                     UNP                 P42866                    324   \n",
       "   25                     UNP                 P42866                    326   \n",
       "   26                     UNP                 P42866                    327   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       I  \n",
       "   1                       L  \n",
       "   2                       Y  \n",
       "   3                       V  \n",
       "   4                       T  \n",
       "   5                       L  \n",
       "   6                       F  \n",
       "   7                       Q  \n",
       "   8                       N  \n",
       "   9                       Y  \n",
       "   10                      W  \n",
       "   11                      C  \n",
       "   12                      V  \n",
       "   13                      I  \n",
       "   14                      D  \n",
       "   15                      Y  \n",
       "   16                      C  \n",
       "   17                      T  \n",
       "   18                      L  \n",
       "   19                      W  \n",
       "   20                      H  \n",
       "   21                      F  \n",
       "   22                      I  \n",
       "   23                      A  \n",
       "   24                      L  \n",
       "   25                      Y  \n",
       "   26                      T  }],\n",
       " '9ckc': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   3490           ARG             E               2            1   \n",
       "   3501           MET             E               2            2   \n",
       "   3509           LYS             E               2            3   \n",
       "   3518           LEU             E               2            4   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   3490                 ?         522          ARG            E   \n",
       "   3501                 ?         523          MET            E   \n",
       "   3509                 ?         524          LYS            E   \n",
       "   3518                 ?         525          LEU            E   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   3490                  1                1                     UNP   \n",
       "   3501                  1                2                     UNP   \n",
       "   3509                  1                3                     UNP   \n",
       "   3518                  1                4                     UNP   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   3490                 P09874                    522                      R  \n",
       "   3501                 P09874                    523                      M  \n",
       "   3509                 P09874                    524                      K  \n",
       "   3518                 P09874                    525                      L  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            GLU             A               1           47                 ?   \n",
       "   1            ASN             A               1           50                 ?   \n",
       "   2            HIS             A               1           51                 ?   \n",
       "   3            ARG             A               1           58                 ?   \n",
       "   4            ASP             A               1          188                 ?   \n",
       "   5            GLU             A               1          189                 ?   \n",
       "   6            GLU             A               1          190                 ?   \n",
       "   7            SER             A               1          192                 ?   \n",
       "   8            MET             A               1          347                 ?   \n",
       "   9            LEU             A               1          351                 ?   \n",
       "   10           GLN             A               1          354                 ?   \n",
       "   11           TRP             A               1          356                 ?   \n",
       "   12           LEU             A               1          386                 ?   \n",
       "   13           LYS             A               1          387                 ?   \n",
       "   14           LEU             A               1          388                 ?   \n",
       "   15           ARG             A               1          390                 ?   \n",
       "   16           LEU             A               1          391                 ?   \n",
       "   17           MET             A               1          393                 ?   \n",
       "   18           GLY             A               1          394                 ?   \n",
       "   19           LEU             A               1          395                 ?   \n",
       "   20           GLU             A               1          425                 ?   \n",
       "   21           GLU             A               1          429                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           47          GLU            A                  1               47   \n",
       "   1           50          ASN            A                  1               50   \n",
       "   2           51          HIS            A                  1               51   \n",
       "   3           58          ARG            A                  1               58   \n",
       "   4          188          ASP            A                  1              188   \n",
       "   5          189          GLU            A                  1              189   \n",
       "   6          190          GLU            A                  1              190   \n",
       "   7          192          SER            A                  1              192   \n",
       "   8          347          MET            A                  1              347   \n",
       "   9          351          LEU            A                  1              351   \n",
       "   10         354          GLN            A                  1              354   \n",
       "   11         356          TRP            A                  1              356   \n",
       "   12         386          LEU            A                  1              386   \n",
       "   13         387          LYS            A                  1              387   \n",
       "   14         388          LEU            A                  1              388   \n",
       "   15         390          ARG            A                  1              390   \n",
       "   16         391          LEU            A                  1              391   \n",
       "   17         393          MET            A                  1              393   \n",
       "   18         394          GLY            A                  1              394   \n",
       "   19         395          LEU            A                  1              395   \n",
       "   20         425          GLU            A                  1              425   \n",
       "   21         429          GLU            A                  1              429   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9NRG4                     47   \n",
       "   1                      UNP                 Q9NRG4                     50   \n",
       "   2                      UNP                 Q9NRG4                     51   \n",
       "   3                      UNP                 Q9NRG4                     58   \n",
       "   4                      UNP                 Q9NRG4                    188   \n",
       "   5                      UNP                 Q9NRG4                    189   \n",
       "   6                      UNP                 Q9NRG4                    190   \n",
       "   7                      UNP                 Q9NRG4                    192   \n",
       "   8                      UNP                 Q9NRG4                    347   \n",
       "   9                      UNP                 Q9NRG4                    351   \n",
       "   10                     UNP                 Q9NRG4                    354   \n",
       "   11                     UNP                 Q9NRG4                    356   \n",
       "   12                     UNP                 Q9NRG4                    386   \n",
       "   13                     UNP                 Q9NRG4                    387   \n",
       "   14                     UNP                 Q9NRG4                    388   \n",
       "   15                     UNP                 Q9NRG4                    390   \n",
       "   16                     UNP                 Q9NRG4                    391   \n",
       "   17                     UNP                 Q9NRG4                    393   \n",
       "   18                     UNP                 Q9NRG4                    394   \n",
       "   19                     UNP                 Q9NRG4                    395   \n",
       "   20                     UNP                 Q9NRG4                    425   \n",
       "   21                     UNP                 Q9NRG4                    429   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       E  \n",
       "   1                       N  \n",
       "   2                       H  \n",
       "   3                       R  \n",
       "   4                       D  \n",
       "   5                       E  \n",
       "   6                       E  \n",
       "   7                       S  \n",
       "   8                       M  \n",
       "   9                       L  \n",
       "   10                      Q  \n",
       "   11                      W  \n",
       "   12                      L  \n",
       "   13                      K  \n",
       "   14                      L  \n",
       "   15                      R  \n",
       "   16                      L  \n",
       "   17                      M  \n",
       "   18                      G  \n",
       "   19                      L  \n",
       "   20                      E  \n",
       "   21                      E  }],\n",
       " '9d3e': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   7113           EBX             F               6            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   7113                 ?         502          EBX            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index  \n",
       "   7113                  1              502  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            LEU             A               1           87                 ?   \n",
       "   1            VAL             A               1           88                 ?   \n",
       "   2            THR             A               1           91                 ?   \n",
       "   3            PHE             A               1           92                 ?   \n",
       "   4            ALA             A               1           93                 ?   \n",
       "   5            TYR             A               1           95                 ?   \n",
       "   6            LYS             A               1           96                 ?   \n",
       "   7            ALA             A               1           98                 ?   \n",
       "   8            ARG             A               1           99                 ?   \n",
       "   9            SER             A               1          100                 ?   \n",
       "   10           MET             A               1          101                 ?   \n",
       "   11           THR             A               1          102                 ?   \n",
       "   12           ASP             A               1          103                 ?   \n",
       "   13           LEU             A               1          106                 ?   \n",
       "   14           ALA             A               1          107                 ?   \n",
       "   15           ARG             A               1          164                 ?   \n",
       "   16           GLU             A               1          382                 ?   \n",
       "   17           LYS             A               1          384                 ?   \n",
       "   18           ALA             A               1          385                 ?   \n",
       "   19           ILE             A               1          386                 ?   \n",
       "   20           VAL             A               1          388                 ?   \n",
       "   21           ILE             A               1          389                 ?   \n",
       "   22           LEU             A               1          448                 ?   \n",
       "   23           TYR             A               1          449                 ?   \n",
       "   24           ILE             A               1          452                 ?   \n",
       "   25           GLY             A               1          453                 ?   \n",
       "   26           GLN             A               1          454                 ?   \n",
       "   27           LYS             A               1          455                 ?   \n",
       "   28           PHE             A               1          456                 ?   \n",
       "   29           ARG             A               1          457                 ?   \n",
       "   30           TYR             A               1          459                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \n",
       "   0           66          LEU            A                  1               87  \n",
       "   1           67          VAL            A                  1               88  \n",
       "   2           70          THR            A                  1               91  \n",
       "   3           71          PHE            A                  1               92  \n",
       "   4           72          ALA            A                  1               93  \n",
       "   5           74          TYR            A                  1               95  \n",
       "   6           75          LYS            A                  1               96  \n",
       "   7           77          ALA            A                  1               98  \n",
       "   8           78          ARG            A                  1               99  \n",
       "   9           79          SER            A                  1              100  \n",
       "   10          80          MET            A                  1              101  \n",
       "   11          81          THR            A                  1              102  \n",
       "   12          82          ASP            A                  1              103  \n",
       "   13          85          LEU            A                  1              106  \n",
       "   14          86          ALA            A                  1              107  \n",
       "   15         143          ARG            A                  1              164  \n",
       "   16         361          GLU            A                  1              382  \n",
       "   17         363          LYS            A                  1              384  \n",
       "   18         364          ALA            A                  1              385  \n",
       "   19         365          ILE            A                  1              386  \n",
       "   20         367          VAL            A                  1              388  \n",
       "   21         368          ILE            A                  1              389  \n",
       "   22         427          LEU            A                  1              448  \n",
       "   23         428          TYR            A                  1              449  \n",
       "   24         431          ILE            A                  1              452  \n",
       "   25         432          GLY            A                  1              453  \n",
       "   26         433          GLN            A                  1              454  \n",
       "   27         434          LYS            A                  1              455  \n",
       "   28         435          PHE            A                  1              456  \n",
       "   29         436          ARG            A                  1              457  \n",
       "   30         438          TYR            A                  1              459  },\n",
       "  {'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   7144         A1A1W             G               7            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   7144                 ?         503        A1A1W            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index  \n",
       "   7144                  1              503  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            LYS             A               1          143                 ?   \n",
       "   1            TYR             A               1          146                 ?   \n",
       "   2            ALA             A               1          147                 ?   \n",
       "   3            PHE             A               1          150                 ?   \n",
       "   4            ASN             A               1          151                 ?   \n",
       "   5            ILE             A               1          199                 ?   \n",
       "   6            SER             A               1          200                 ?   \n",
       "   7            SER             A               1          202                 ?   \n",
       "   8            THR             A               1          203                 ?   \n",
       "   9            PHE             A               1          206                 ?   \n",
       "   10           ASN             A               1          207                 ?   \n",
       "   11           PRO             A               1          220                 ?   \n",
       "   12           TYR             A               1          222                 ?   \n",
       "   13           LEU             A               1          236                 ?   \n",
       "   14           GLU             A               1          239                 ?   \n",
       "   15           LEU             A               1          240                 ?   \n",
       "   16           GLY             A               1          243                 ?   \n",
       "   17           PHE             A               1          244                 ?   \n",
       "   18           GLN             A               1          400                 ?   \n",
       "   19           ILE             A               1          401                 ?   \n",
       "   20           PRO             A               1          402                 ?   \n",
       "   21           HIS             A               1          403                 ?   \n",
       "   22           ASN             A               1          404                 ?   \n",
       "   23           MET             A               1          405                 ?   \n",
       "   24           VAL             A               1          406                 ?   \n",
       "   25           LEU             A               1          407                 ?   \n",
       "   26           LEU             A               1          408                 ?   \n",
       "   27           ALA             A               1          411                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \n",
       "   0          122          LYS            A                  1              143  \n",
       "   1          125          TYR            A                  1              146  \n",
       "   2          126          ALA            A                  1              147  \n",
       "   3          129          PHE            A                  1              150  \n",
       "   4          130          ASN            A                  1              151  \n",
       "   5          178          ILE            A                  1              199  \n",
       "   6          179          SER            A                  1              200  \n",
       "   7          181          SER            A                  1              202  \n",
       "   8          182          THR            A                  1              203  \n",
       "   9          185          PHE            A                  1              206  \n",
       "   10         186          ASN            A                  1              207  \n",
       "   11         199          PRO            A                  1              220  \n",
       "   12         201          TYR            A                  1              222  \n",
       "   13         215          LEU            A                  1              236  \n",
       "   14         218          GLU            A                  1              239  \n",
       "   15         219          LEU            A                  1              240  \n",
       "   16         222          GLY            A                  1              243  \n",
       "   17         223          PHE            A                  1              244  \n",
       "   18         379          GLN            A                  1              400  \n",
       "   19         380          ILE            A                  1              401  \n",
       "   20         381          PRO            A                  1              402  \n",
       "   21         382          HIS            A                  1              403  \n",
       "   22         383          ASN            A                  1              404  \n",
       "   23         384          MET            A                  1              405  \n",
       "   24         385          VAL            A                  1              406  \n",
       "   25         386          LEU            A                  1              407  \n",
       "   26         387          LEU            A                  1              408  \n",
       "   27         390          ALA            A                  1              411  }],\n",
       " '9dnm': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   5209           ODN             D               4            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   5209                 ?         501          ODN            A   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   5209                  1              501                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   5209                      ?                      ?                      ?  ,\n",
       "   'site':   label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0           GLU             C               3          134                 ?   \n",
       "   1           GLN             C               3          349                 ?   \n",
       "   2           TYR             C               3          350                 ?   \n",
       "   3           LYS             C               3          351                 ?   \n",
       "   4           CYS             C               3          352                 ?   \n",
       "   5           PRO             C               3          353                 ?   \n",
       "   6           GLU             C               3          384                 ?   \n",
       "   7           LYS             C               3          385                 ?   \n",
       "   8           ARG             C               3          386                 ?   \n",
       "   9           GLY             C               3          387                 ?   \n",
       "   \n",
       "     auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0         134          GLU            A                  1              134   \n",
       "   1         349          GLN            A                  1              349   \n",
       "   2         350          TYR            A                  1              350   \n",
       "   3         351          LYS            A                  1              351   \n",
       "   4         352          CYS            A                  1              352   \n",
       "   5         353          PRO            A                  1              353   \n",
       "   6         384          GLU            A                  1              384   \n",
       "   7         385          LYS            A                  1              385   \n",
       "   8         386          ARG            A                  1              386   \n",
       "   9         387          GLY            A                  1              387   \n",
       "   \n",
       "     pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                     UNP                 P29066                    134   \n",
       "   1                     UNP                 P29066                    248   \n",
       "   2                     UNP                 P29066                    249   \n",
       "   3                     UNP                 P29066                    250   \n",
       "   4                     UNP                 P29066                    251   \n",
       "   5                     UNP                 P29066                    252   \n",
       "   6                     UNP                 P29066                    283   \n",
       "   7                     UNP                 P29066                    284   \n",
       "   8                     UNP                 P29066                    285   \n",
       "   9                     UNP                 P29066                    286   \n",
       "   \n",
       "     pdbx_sifts_xref_db_res  \n",
       "   0                      E  \n",
       "   1                      Q  \n",
       "   2                      Y  \n",
       "   3                      K  \n",
       "   4                      C  \n",
       "   5                      P  \n",
       "   6                      E  \n",
       "   7                      K  \n",
       "   8                      R  \n",
       "   9                      G  }],\n",
       " '9fvb': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   4757           GLN             C               2            1   \n",
       "   4776           VAL             C               2            2   \n",
       "   4792           GLN             C               2            3   \n",
       "   4809           LEU             C               2            4   \n",
       "   4828           VAL             C               2            5   \n",
       "   ...            ...           ...             ...          ...   \n",
       "   6518           VAL             C               2          117   \n",
       "   6534           THR             C               2          118   \n",
       "   6548           VAL             C               2          119   \n",
       "   6564           SER             C               2          120   \n",
       "   6575           SER             C               2          121   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   4757                 ?           1          GLN            C   \n",
       "   4776                 ?           2          VAL            C   \n",
       "   4792                 ?           3          GLN            C   \n",
       "   4809                 ?           4          LEU            C   \n",
       "   4828                 ?           5          VAL            C   \n",
       "   ...                ...         ...          ...          ...   \n",
       "   6518                 ?         117          VAL            C   \n",
       "   6534                 ?         118          THR            C   \n",
       "   6548                 ?         119          VAL            C   \n",
       "   6564                 ?         120          SER            C   \n",
       "   6575                 ?         121          SER            C   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   4757                  1                1                       ?   \n",
       "   4776                  1                2                       ?   \n",
       "   4792                  1                3                       ?   \n",
       "   4809                  1                4                       ?   \n",
       "   4828                  1                5                       ?   \n",
       "   ...                 ...              ...                     ...   \n",
       "   6518                  1              117                       ?   \n",
       "   6534                  1              118                       ?   \n",
       "   6548                  1              119                       ?   \n",
       "   6564                  1              120                       ?   \n",
       "   6575                  1              121                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   4757                      ?                      ?                      ?  \n",
       "   4776                      ?                      ?                      ?  \n",
       "   4792                      ?                      ?                      ?  \n",
       "   4809                      ?                      ?                      ?  \n",
       "   4828                      ?                      ?                      ?  \n",
       "   ...                     ...                    ...                    ...  \n",
       "   6518                      ?                      ?                      ?  \n",
       "   6534                      ?                      ?                      ?  \n",
       "   6548                      ?                      ?                      ?  \n",
       "   6564                      ?                      ?                      ?  \n",
       "   6575                      ?                      ?                      ?  \n",
       "   \n",
       "   [121 rows x 14 columns],\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            LEU             A               1           49                 ?   \n",
       "   1            GLY             A               1           50                 ?   \n",
       "   2            ASP             A               1           51                 ?   \n",
       "   3            ASP             A               1           52                 ?   \n",
       "   4            ARG             A               1           53                 ?   \n",
       "   5            ALA             A               1           54                 ?   \n",
       "   6            MET             A               1           55                 ?   \n",
       "   7            LEU             A               1           56                 ?   \n",
       "   8            GLN             A               1           57                 ?   \n",
       "   9            GLN             A               1           58                 ?   \n",
       "   10           THR             A               1           60                 ?   \n",
       "   11           LEU             A               1           61                 ?   \n",
       "   12           ASP             A               1           63                 ?   \n",
       "   13           MET             A               1           74                 ?   \n",
       "   14           LEU             A               1           76                 ?   \n",
       "   15           TRP             A               1           77                 ?   \n",
       "   16           ILE             A               1           78                 ?   \n",
       "   17           PRO             A               1           79                 ?   \n",
       "   18           MET             A               1          112                 ?   \n",
       "   19           LEU             A               1          113                 ?   \n",
       "   20           GLN             A               1          114                 ?   \n",
       "   21           LYS             A               1          115                 ?   \n",
       "   22           PHE             A               1          116                 ?   \n",
       "   23           ASN             A               1          117                 ?   \n",
       "   24           TRP             A               1          118                 ?   \n",
       "   25           SER             A               1          223                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           45          LEU            A                  1               49   \n",
       "   1           46          GLY            A                  1               50   \n",
       "   2           47          ASP            A                  1               51   \n",
       "   3           48          ASP            A                  1               52   \n",
       "   4           49          ARG            A                  1               53   \n",
       "   5           50          ALA            A                  1               54   \n",
       "   6           51          MET            A                  1               55   \n",
       "   7           52          LEU            A                  1               56   \n",
       "   8           53          GLN            A                  1               57   \n",
       "   9           54          GLN            A                  1               58   \n",
       "   10          56          THR            A                  1               60   \n",
       "   11          57          LEU            A                  1               61   \n",
       "   12          59          ASP            A                  1               63   \n",
       "   13          70          MET            A                  1               74   \n",
       "   14          72          LEU            A                  1               76   \n",
       "   15          73          TRP            A                  1               77   \n",
       "   16          74          ILE            A                  1               78   \n",
       "   17          75          PRO            A                  1               79   \n",
       "   18         108          MET            A                  1              112   \n",
       "   19         109          LEU            A                  1              113   \n",
       "   20         110          GLN            A                  1              114   \n",
       "   21         111          LYS            A                  1              115   \n",
       "   22         112          PHE            A                  1              116   \n",
       "   23         113          ASN            A                  1              117   \n",
       "   24         114          TRP            A                  1              118   \n",
       "   25         219          SER            A                  1              223   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                      UNP                 Q9KR64                     67   \n",
       "   1                      UNP                 Q9KR64                     68   \n",
       "   2                      UNP                 Q9KR64                     69   \n",
       "   3                      UNP                 Q9KR64                     70   \n",
       "   4                      UNP                 Q9KR64                     71   \n",
       "   5                      UNP                 Q9KR64                     72   \n",
       "   6                      UNP                 Q9KR64                     73   \n",
       "   7                      UNP                 Q9KR64                     74   \n",
       "   8                      UNP                 Q9KR64                     75   \n",
       "   9                      UNP                 Q9KR64                     76   \n",
       "   10                     UNP                 Q9KR64                     78   \n",
       "   11                     UNP                 Q9KR64                     79   \n",
       "   12                     UNP                 Q9KR64                     81   \n",
       "   13                     UNP                 Q9KR64                     92   \n",
       "   14                     UNP                 Q9KR64                     94   \n",
       "   15                     UNP                 Q9KR64                     95   \n",
       "   16                     UNP                 Q9KR64                     96   \n",
       "   17                     UNP                 Q9KR64                     97   \n",
       "   18                     UNP                 Q9KR64                    130   \n",
       "   19                     UNP                 Q9KR64                    131   \n",
       "   20                     UNP                 Q9KR64                    132   \n",
       "   21                     UNP                 Q9KR64                    133   \n",
       "   22                     UNP                 Q9KR64                    134   \n",
       "   23                     UNP                 Q9KR64                    135   \n",
       "   24                     UNP                 Q9KR64                    136   \n",
       "   25                     UNP                 Q9KR64                    241   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       L  \n",
       "   1                       G  \n",
       "   2                       D  \n",
       "   3                       D  \n",
       "   4                       R  \n",
       "   5                       A  \n",
       "   6                       M  \n",
       "   7                       L  \n",
       "   8                       Q  \n",
       "   9                       Q  \n",
       "   10                      T  \n",
       "   11                      L  \n",
       "   12                      D  \n",
       "   13                      M  \n",
       "   14                      L  \n",
       "   15                      W  \n",
       "   16                      I  \n",
       "   17                      P  \n",
       "   18                      M  \n",
       "   19                      L  \n",
       "   20                      Q  \n",
       "   21                      K  \n",
       "   22                      F  \n",
       "   23                      N  \n",
       "   24                      W  \n",
       "   25                      S  }],\n",
       " '9ivm': [{'mod':      label_comp_id label_asym_id label_entity_id label_seq_id  \\\n",
       "   9357         A1EEC             G               7            .   \n",
       "   \n",
       "        pdbx_PDB_ins_code auth_seq_id auth_comp_id auth_asym_id  \\\n",
       "   9357                 ?         501        A1EEC            R   \n",
       "   \n",
       "        pdbx_PDB_model_num pdbx_label_index pdbx_sifts_xref_db_name  \\\n",
       "   9357                  1              501                       ?   \n",
       "   \n",
       "        pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num pdbx_sifts_xref_db_res  \n",
       "   9357                      ?                      ?                      ?  ,\n",
       "   'site':    label_comp_id label_asym_id label_entity_id label_seq_id pdbx_PDB_ins_code  \\\n",
       "   0            PHE             E               5            4                 ?   \n",
       "   1            ASP             E               5            7                 ?   \n",
       "   2            VAL             E               5            8                 ?   \n",
       "   3            TYR             E               5           11                 ?   \n",
       "   4            LEU             E               5           12                 ?   \n",
       "   5            PRO             F               6          114                 ?   \n",
       "   6            GLU             F               6          115                 ?   \n",
       "   7            GLU             F               6          116                 ?   \n",
       "   8            GLN             F               6          117                 ?   \n",
       "   9            LEU             F               6          118                 ?   \n",
       "   10           LEU             F               6          119                 ?   \n",
       "   11           TYR             F               6          122                 ?   \n",
       "   12           ILE             F               6          123                 ?   \n",
       "   13           TYR             F               6          125                 ?   \n",
       "   14           LYS             F               6          174                 ?   \n",
       "   15           ASP             F               6          175                 ?   \n",
       "   16           ALA             F               6          176                 ?   \n",
       "   17           ALA             F               6          177                 ?   \n",
       "   18           LEU             F               6          178                 ?   \n",
       "   19           LYS             F               6          179                 ?   \n",
       "   20           TRP             F               6          180                 ?   \n",
       "   21           TYR             F               6          182                 ?   \n",
       "   \n",
       "      auth_seq_id auth_comp_id auth_asym_id pdbx_PDB_model_num pdbx_label_index  \\\n",
       "   0           12          PHE            P                  1                4   \n",
       "   1           15          ASP            P                  1                7   \n",
       "   2           16          VAL            P                  1                8   \n",
       "   3           19          TYR            P                  1               11   \n",
       "   4           20          LEU            P                  1               12   \n",
       "   5          137          PRO            R                  1              114   \n",
       "   6          138          GLU            R                  1              115   \n",
       "   7          139          GLU            R                  1              116   \n",
       "   8          140          GLN            R                  1              117   \n",
       "   9          141          LEU            R                  1              118   \n",
       "   10         142          LEU            R                  1              119   \n",
       "   11         145          TYR            R                  1              122   \n",
       "   12         146          ILE            R                  1              123   \n",
       "   13         148          TYR            R                  1              125   \n",
       "   14         197          LYS            R                  1              174   \n",
       "   15         198          ASP            R                  1              175   \n",
       "   16         199          ALA            R                  1              176   \n",
       "   17         200          ALA            R                  1              177   \n",
       "   18         201          LEU            R                  1              178   \n",
       "   19         202          LYS            R                  1              179   \n",
       "   20         203          TRP            R                  1              180   \n",
       "   21         205          TYR            R                  1              182   \n",
       "   \n",
       "      pdbx_sifts_xref_db_name pdbx_sifts_xref_db_acc pdbx_sifts_xref_db_num  \\\n",
       "   0                        ?                      ?                      ?   \n",
       "   1                        ?                      ?                      ?   \n",
       "   2                        ?                      ?                      ?   \n",
       "   3                        ?                      ?                      ?   \n",
       "   4                        ?                      ?                      ?   \n",
       "   5                      UNP                 P43220                    137   \n",
       "   6                      UNP                 P43220                    138   \n",
       "   7                      UNP                 P43220                    139   \n",
       "   8                      UNP                 P43220                    140   \n",
       "   9                      UNP                 P43220                    141   \n",
       "   10                     UNP                 P43220                    142   \n",
       "   11                     UNP                 P43220                    145   \n",
       "   12                     UNP                 P43220                    146   \n",
       "   13                     UNP                 P43220                    148   \n",
       "   14                     UNP                 P43220                    197   \n",
       "   15                     UNP                 P43220                    198   \n",
       "   16                     UNP                 P43220                    199   \n",
       "   17                     UNP                 P43220                    200   \n",
       "   18                     UNP                 P43220                    201   \n",
       "   19                     UNP                 P43220                    202   \n",
       "   20                     UNP                 P43220                    203   \n",
       "   21                     UNP                 P43220                    205   \n",
       "   \n",
       "      pdbx_sifts_xref_db_res  \n",
       "   0                       ?  \n",
       "   1                       ?  \n",
       "   2                       ?  \n",
       "   3                       ?  \n",
       "   4                       ?  \n",
       "   5                       P  \n",
       "   6                       E  \n",
       "   7                       E  \n",
       "   8                       Q  \n",
       "   9                       L  \n",
       "   10                      L  \n",
       "   11                      Y  \n",
       "   12                      I  \n",
       "   13                      Y  \n",
       "   14                      K  \n",
       "   15                      D  \n",
       "   16                      A  \n",
       "   17                      A  \n",
       "   18                      L  \n",
       "   19                      K  \n",
       "   20                      W  \n",
       "   21                      Y  }]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../training_data/7.Extra_set/news_sites.pkl\", \"rb\") as f:\n",
    "    extra_sites = pickle.load(f)\n",
    "\n",
    "extra_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5894eae5-9db6-4fdc-9cc8-1ed8a13c8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c707ac99-20d4-4643-a23e-9777e05b3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import Cif\n",
    "from utils.pocket_utils import get_pocket_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce7c39cf-e136-4a0c-92a5-6071c44b8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cif.path = \"../training_data/7.Extra_set/cifs\"\n",
    "# Cif.original_cifs_path = \"../training_data/7.Extra_set/origcifs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e703020a-a76f-47fb-9fb3-23ec2947cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_pocket = lambda pdb, pocket, pocket_color: get_pocket_view(\n",
    "    pdb = pdb, \n",
    "    cif = Cif(pdb, f\"../training_data/7.Extra_set/origcifs/{pdb}_updated.cif.gz\"),\n",
    "    pocket = pocket,\n",
    "    pockets_path = \"../training_data/7.Extra_set/pockets\",\n",
    "    sites = extra_sites[pdb],\n",
    "    elements_colors = ((\"site\", \"black\"), (\"mod\", \"purple\")),\n",
    "    pocket_color = pocket_color\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bf478ca-1424-42fe-9f8c-d7a58b27d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_extra_preds = lambda pdb: (\n",
    "    probs\n",
    "    .merge(extra[\"Label_label\"], left_index=True, right_index=True)\n",
    "    .loc[lambda x: (\n",
    "        ( x.index.str.contains(pdb) )\n",
    "        & ( \n",
    "            ( x[1] > 0.5 ) \n",
    "            | ( x[\"Label_label\"] == 1 )\n",
    "        )\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21847b0c-7ecf-46a7-ae92-cc2001d15513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_color(pred):\n",
    "    if pred[1] >= 0.5:\n",
    "        if pred[\"Label_label\"] == 1: # TP\n",
    "            return \"#009E73\" # dark green\n",
    "        else: # FP\n",
    "            return \"#56B4E9\" # light blue\n",
    "    else:\n",
    "        if pred[\"Label_label\"] == 1: # FN\n",
    "            return \"#D55E00\" # orange/vermillion\n",
    "\n",
    "def view_predictions(pdb):\n",
    "    preds = get_extra_preds(pdb)\n",
    "    print(preds)\n",
    "\n",
    "    colors = []\n",
    "    for pocket, pred in preds.iterrows():\n",
    "        v, pcolors, sites_colors = view_pocket(\n",
    "            pdb, \n",
    "            pocket.split(\"_\")[-1], \n",
    "            get_pred_color(pred)\n",
    "        )\n",
    "        colors.extend(pcolors)\n",
    "\n",
    "    return v, colors, sites_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b53aae-8ed4-4e70-9c56-3e402e9b65ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a19368c9-be83-4c87-84de-f5784b8b558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8v81_pocket1</th>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.978047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8uk6_pocket1</th>\n",
       "      <td>0.053351</td>\n",
       "      <td>0.946649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gqu_pocket1</th>\n",
       "      <td>0.067621</td>\n",
       "      <td>0.932379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8f4s_pocket1</th>\n",
       "      <td>0.076614</td>\n",
       "      <td>0.923386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8qni_pocket1</th>\n",
       "      <td>0.081667</td>\n",
       "      <td>0.918333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket1</th>\n",
       "      <td>0.092922</td>\n",
       "      <td>0.907078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket17</th>\n",
       "      <td>0.160410</td>\n",
       "      <td>0.839590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket4</th>\n",
       "      <td>0.164947</td>\n",
       "      <td>0.835053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9dnm_pocket2</th>\n",
       "      <td>0.343567</td>\n",
       "      <td>0.656433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket63</th>\n",
       "      <td>0.439920</td>\n",
       "      <td>0.560080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket6</th>\n",
       "      <td>0.501554</td>\n",
       "      <td>0.498446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8v81_pocket43</th>\n",
       "      <td>0.512679</td>\n",
       "      <td>0.487322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket29</th>\n",
       "      <td>0.576324</td>\n",
       "      <td>0.423676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket2</th>\n",
       "      <td>0.624642</td>\n",
       "      <td>0.375358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket25</th>\n",
       "      <td>0.662872</td>\n",
       "      <td>0.337128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket54</th>\n",
       "      <td>0.755049</td>\n",
       "      <td>0.244951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7yg5_pocket3</th>\n",
       "      <td>0.758742</td>\n",
       "      <td>0.241258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8uk6_pocket28</th>\n",
       "      <td>0.791634</td>\n",
       "      <td>0.208366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket25</th>\n",
       "      <td>0.826651</td>\n",
       "      <td>0.173349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8jp0_pocket4</th>\n",
       "      <td>0.862584</td>\n",
       "      <td>0.137416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1\n",
       "8v81_pocket1   0.021953  0.978047\n",
       "8uk6_pocket1   0.053351  0.946649\n",
       "7gqu_pocket1   0.067621  0.932379\n",
       "8f4s_pocket1   0.076614  0.923386\n",
       "8qni_pocket1   0.081667  0.918333\n",
       "8jp0_pocket1   0.092922  0.907078\n",
       "8v81_pocket17  0.160410  0.839590\n",
       "8v81_pocket4   0.164947  0.835053\n",
       "9dnm_pocket2   0.343567  0.656433\n",
       "7yg5_pocket63  0.439920  0.560080\n",
       "7yg5_pocket6   0.501554  0.498446\n",
       "8v81_pocket43  0.512679  0.487322\n",
       "7yg5_pocket29  0.576324  0.423676\n",
       "7yg5_pocket2   0.624642  0.375358\n",
       "7yg5_pocket25  0.662872  0.337128\n",
       "7yg5_pocket54  0.755049  0.244951\n",
       "7yg5_pocket3   0.758742  0.241258\n",
       "8uk6_pocket28  0.791634  0.208366\n",
       "8jp0_pocket25  0.826651  0.173349\n",
       "8jp0_pocket4   0.862584  0.137416"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = fit.predict_proba(extra).sort_values(1, ascending=False)\n",
    "probs.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e32bf4f-fdb7-487f-834e-5431e1c06bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0         1  Label_label\n",
      "8f4s_pocket1  0.076614  0.923386            1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fb3bd61c154fec80f53758f57f01d2",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viz(bg_color='#F7F7F7', color_data={'data': [{'color': 'white'}], 'nonSelectedColor': None, 'keepColors': Fals…"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, colors, sites_colors = view_predictions(\"8f4s\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85c18fe2-4ea3-4969-83bf-c44b015e0f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v.color(colors, keep_colors=True, keep_representations=True)\n",
    "v.color_residues(sites_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73fe92-cf54-4dd6-b79b-aac068a39c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d94f0515-a2c3-496a-8dab-ff0e4fa4f993",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "096bb5e7-f90e-447e-837e-af97213baa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efa7c1d3-249f-4858-8c93-fc19f7be06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer using background data\n",
    "explainer = shap.Explainer(fit.predict_proba, train.drop(columns=[\"Label_label\"]), feature_names=train.drop(columns=[\"Label_label\"]).columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b4695c3-606b-4be5-bf07-b599c638cc3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 1237it [19:43,  1.04it/s]                                                               \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # Generate SHAP values\n",
    "    shap_values = explainer.shap_values(test.drop(columns=[\"Label_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00cf1d5f-ca79-4313-a48b-a26ae08c1264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x950 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values[:,:,1], test.drop(columns=[\"Label_label\"]), feature_names=test.drop(columns=[\"Label_label\"]).columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c33af6-d138-4f22-9ea4-d2bbfea08c5a",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bff1efee-9679-4a8b-997e-986dfb594e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['Amino acids_label_comp_id_X', 'Graphein_hphob_fauchere']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_F</th>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.064198</td>\n",
       "      <td>1.962867e-08</td>\n",
       "      <td>30</td>\n",
       "      <td>0.118825</td>\n",
       "      <td>0.054210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_W</th>\n",
       "      <td>0.069156</td>\n",
       "      <td>0.046113</td>\n",
       "      <td>2.337020e-09</td>\n",
       "      <td>30</td>\n",
       "      <td>0.092363</td>\n",
       "      <td>0.045950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_O_NH_1_energy</th>\n",
       "      <td>0.066547</td>\n",
       "      <td>0.069669</td>\n",
       "      <td>6.659841e-06</td>\n",
       "      <td>30</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.031486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_G</th>\n",
       "      <td>0.055891</td>\n",
       "      <td>0.051236</td>\n",
       "      <td>8.533103e-07</td>\n",
       "      <td>30</td>\n",
       "      <td>0.081675</td>\n",
       "      <td>0.030107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_Neff</th>\n",
       "      <td>0.043247</td>\n",
       "      <td>0.048949</td>\n",
       "      <td>1.982779e-05</td>\n",
       "      <td>30</td>\n",
       "      <td>0.067881</td>\n",
       "      <td>0.018614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_NH_O_2_energy</th>\n",
       "      <td>-0.016004</td>\n",
       "      <td>0.032681</td>\n",
       "      <td>9.940259e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>-0.032450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Graphein_numbercodons</th>\n",
       "      <td>-0.016662</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>9.983413e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>-0.031014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Graphein_a_a_swiss_prot</th>\n",
       "      <td>-0.017669</td>\n",
       "      <td>0.030909</td>\n",
       "      <td>9.980222e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.002114</td>\n",
       "      <td>-0.033224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Graphein_refractivity</th>\n",
       "      <td>-0.018238</td>\n",
       "      <td>0.021799</td>\n",
       "      <td>9.999596e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.007268</td>\n",
       "      <td>-0.029208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Graphein_a_a_composition</th>\n",
       "      <td>-0.020989</td>\n",
       "      <td>0.022093</td>\n",
       "      <td>9.999928e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>-0.009871</td>\n",
       "      <td>-0.032107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             importance    stddev       p_value   n  p99_high  \\\n",
       "Amino acids_label_comp_id_F    0.086517  0.064198  1.962867e-08  30  0.118825   \n",
       "Amino acids_label_comp_id_W    0.069156  0.046113  2.337020e-09  30  0.092363   \n",
       "DSSP_O_NH_1_energy             0.066547  0.069669  6.659841e-06  30  0.101608   \n",
       "Amino acids_label_comp_id_G    0.055891  0.051236  8.533103e-07  30  0.081675   \n",
       "HHBlits_Neff                   0.043247  0.048949  1.982779e-05  30  0.067881   \n",
       "...                                 ...       ...           ...  ..       ...   \n",
       "DSSP_NH_O_2_energy            -0.016004  0.032681  9.940259e-01  30  0.000443   \n",
       "Graphein_numbercodons         -0.016662  0.028518  9.983413e-01  30 -0.002310   \n",
       "Graphein_a_a_swiss_prot       -0.017669  0.030909  9.980222e-01  30 -0.002114   \n",
       "Graphein_refractivity         -0.018238  0.021799  9.999596e-01  30 -0.007268   \n",
       "Graphein_a_a_composition      -0.020989  0.022093  9.999928e-01  30 -0.009871   \n",
       "\n",
       "                              p99_low  \n",
       "Amino acids_label_comp_id_F  0.054210  \n",
       "Amino acids_label_comp_id_W  0.045950  \n",
       "DSSP_O_NH_1_energy           0.031486  \n",
       "Amino acids_label_comp_id_G  0.030107  \n",
       "HHBlits_Neff                 0.018614  \n",
       "...                               ...  \n",
       "DSSP_NH_O_2_energy          -0.032450  \n",
       "Graphein_numbercodons       -0.031014  \n",
       "Graphein_a_a_swiss_prot     -0.033224  \n",
       "Graphein_refractivity       -0.029208  \n",
       "Graphein_a_a_composition    -0.032107  \n",
       "\n",
       "[164 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    importance = fit.feature_importance(\n",
    "        traindataset,\n",
    "        subsample_size=500,\n",
    "        num_shuffle_sets=30\n",
    "    )\n",
    "\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b91223e5-bad5-4938-88f0-204bd08eb49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_F</th>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.064198</td>\n",
       "      <td>1.962867e-08</td>\n",
       "      <td>30</td>\n",
       "      <td>0.118825</td>\n",
       "      <td>0.054210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_W</th>\n",
       "      <td>0.069156</td>\n",
       "      <td>0.046113</td>\n",
       "      <td>2.337020e-09</td>\n",
       "      <td>30</td>\n",
       "      <td>0.092363</td>\n",
       "      <td>0.045950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_O_NH_1_energy</th>\n",
       "      <td>0.066547</td>\n",
       "      <td>0.069669</td>\n",
       "      <td>6.659841e-06</td>\n",
       "      <td>30</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.031486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_G</th>\n",
       "      <td>0.055891</td>\n",
       "      <td>0.051236</td>\n",
       "      <td>8.533103e-07</td>\n",
       "      <td>30</td>\n",
       "      <td>0.081675</td>\n",
       "      <td>0.030107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_Neff</th>\n",
       "      <td>0.043247</td>\n",
       "      <td>0.048949</td>\n",
       "      <td>1.982779e-05</td>\n",
       "      <td>30</td>\n",
       "      <td>0.067881</td>\n",
       "      <td>0.018614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_V</th>\n",
       "      <td>0.040770</td>\n",
       "      <td>0.056201</td>\n",
       "      <td>2.150144e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.069053</td>\n",
       "      <td>0.012487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_I</th>\n",
       "      <td>0.038608</td>\n",
       "      <td>0.064851</td>\n",
       "      <td>1.419807e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.071244</td>\n",
       "      <td>0.005972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_L</th>\n",
       "      <td>0.034914</td>\n",
       "      <td>0.054458</td>\n",
       "      <td>7.395423e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.062320</td>\n",
       "      <td>0.007508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_H</th>\n",
       "      <td>0.034377</td>\n",
       "      <td>0.053061</td>\n",
       "      <td>6.707708e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.061079</td>\n",
       "      <td>0.007674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_secondary structure_E</th>\n",
       "      <td>0.032514</td>\n",
       "      <td>0.042640</td>\n",
       "      <td>1.236479e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.053972</td>\n",
       "      <td>0.011055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_S</th>\n",
       "      <td>0.030925</td>\n",
       "      <td>0.057252</td>\n",
       "      <td>3.048602e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.059736</td>\n",
       "      <td>0.002113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_Y</th>\n",
       "      <td>0.029724</td>\n",
       "      <td>0.055825</td>\n",
       "      <td>3.384245e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.057818</td>\n",
       "      <td>0.001630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_secondary structure_G</th>\n",
       "      <td>0.028460</td>\n",
       "      <td>0.041763</td>\n",
       "      <td>4.113305e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.007443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_secondary structure_B</th>\n",
       "      <td>0.025935</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>1.840360e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.043671</td>\n",
       "      <td>0.008199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_M</th>\n",
       "      <td>0.024906</td>\n",
       "      <td>0.055456</td>\n",
       "      <td>1.004937e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.052814</td>\n",
       "      <td>-0.003002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_M</th>\n",
       "      <td>0.024696</td>\n",
       "      <td>0.043413</td>\n",
       "      <td>2.054992e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_H</th>\n",
       "      <td>0.021666</td>\n",
       "      <td>0.033427</td>\n",
       "      <td>6.679301e-04</td>\n",
       "      <td>30</td>\n",
       "      <td>0.038488</td>\n",
       "      <td>0.004844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_R</th>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.058527</td>\n",
       "      <td>3.190890e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.050045</td>\n",
       "      <td>-0.008862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_P</th>\n",
       "      <td>0.020564</td>\n",
       "      <td>0.047763</td>\n",
       "      <td>1.266023e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>-0.003472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_A</th>\n",
       "      <td>0.019780</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>1.780721e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.044516</td>\n",
       "      <td>-0.004957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_secondary structure_T</th>\n",
       "      <td>0.019528</td>\n",
       "      <td>0.036031</td>\n",
       "      <td>2.974109e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.037660</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProDy_essa</th>\n",
       "      <td>0.018497</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>2.870209e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.044263</td>\n",
       "      <td>-0.007269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_K</th>\n",
       "      <td>0.018008</td>\n",
       "      <td>0.058215</td>\n",
       "      <td>5.045805e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.047305</td>\n",
       "      <td>-0.011288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProDy_mechstiff</th>\n",
       "      <td>0.017675</td>\n",
       "      <td>0.058585</td>\n",
       "      <td>5.460879e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.047158</td>\n",
       "      <td>-0.011807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_C</th>\n",
       "      <td>0.016996</td>\n",
       "      <td>0.044670</td>\n",
       "      <td>2.303898e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.039476</td>\n",
       "      <td>-0.005484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_NH_O_2_relidx</th>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.036177</td>\n",
       "      <td>8.299639e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>-0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Melodia_phi</th>\n",
       "      <td>0.016703</td>\n",
       "      <td>0.031587</td>\n",
       "      <td>3.555673e-03</td>\n",
       "      <td>30</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_G</th>\n",
       "      <td>0.016449</td>\n",
       "      <td>0.052684</td>\n",
       "      <td>4.896361e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.042962</td>\n",
       "      <td>-0.010064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransferEntropy_TE</th>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.056445</td>\n",
       "      <td>6.121619e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>-0.012009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_T</th>\n",
       "      <td>0.016206</td>\n",
       "      <td>0.070846</td>\n",
       "      <td>1.101158e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>-0.019446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amino acids_label_comp_id_Q</th>\n",
       "      <td>0.014920</td>\n",
       "      <td>0.050806</td>\n",
       "      <td>5.928489e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.040488</td>\n",
       "      <td>-0.010648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_D-&gt;M</th>\n",
       "      <td>0.013365</td>\n",
       "      <td>0.043694</td>\n",
       "      <td>5.229974e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.035354</td>\n",
       "      <td>-0.008623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_O_NH_1_relidx</th>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.047546</td>\n",
       "      <td>6.922055e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.037153</td>\n",
       "      <td>-0.010701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProDy_rmsf</th>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.050967</td>\n",
       "      <td>1.128153e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.037169</td>\n",
       "      <td>-0.014129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_D-&gt;D</th>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.032794</td>\n",
       "      <td>3.827645e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.027499</td>\n",
       "      <td>-0.005508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Melodia_arc_len</th>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.041382</td>\n",
       "      <td>9.713380e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>-0.010786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_phi</th>\n",
       "      <td>0.009492</td>\n",
       "      <td>0.031017</td>\n",
       "      <td>5.222131e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>-0.006117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHBlits_M-&gt;M</th>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.040238</td>\n",
       "      <td>1.564043e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.027796</td>\n",
       "      <td>-0.012703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_secondary structure_-</th>\n",
       "      <td>0.007352</td>\n",
       "      <td>0.037784</td>\n",
       "      <td>1.476757e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.026366</td>\n",
       "      <td>-0.011663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSSP_NH_O_1_relidx</th>\n",
       "      <td>0.005913</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>1.902676e-01</td>\n",
       "      <td>30</td>\n",
       "      <td>0.024216</td>\n",
       "      <td>-0.012390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             importance    stddev       p_value   n  p99_high  \\\n",
       "Amino acids_label_comp_id_F    0.086517  0.064198  1.962867e-08  30  0.118825   \n",
       "Amino acids_label_comp_id_W    0.069156  0.046113  2.337020e-09  30  0.092363   \n",
       "DSSP_O_NH_1_energy             0.066547  0.069669  6.659841e-06  30  0.101608   \n",
       "Amino acids_label_comp_id_G    0.055891  0.051236  8.533103e-07  30  0.081675   \n",
       "HHBlits_Neff                   0.043247  0.048949  1.982779e-05  30  0.067881   \n",
       "Amino acids_label_comp_id_V    0.040770  0.056201  2.150144e-04  30  0.069053   \n",
       "Amino acids_label_comp_id_I    0.038608  0.064851  1.419807e-03  30  0.071244   \n",
       "Amino acids_label_comp_id_L    0.034914  0.054458  7.395423e-04  30  0.062320   \n",
       "Amino acids_label_comp_id_H    0.034377  0.053061  6.707708e-04  30  0.061079   \n",
       "DSSP_secondary structure_E     0.032514  0.042640  1.236479e-04  30  0.053972   \n",
       "Amino acids_label_comp_id_S    0.030925  0.057252  3.048602e-03  30  0.059736   \n",
       "Amino acids_label_comp_id_Y    0.029724  0.055825  3.384245e-03  30  0.057818   \n",
       "DSSP_secondary structure_G     0.028460  0.041763  4.113305e-04  30  0.049477   \n",
       "DSSP_secondary structure_B     0.025935  0.035243  1.840360e-04  30  0.043671   \n",
       "Amino acids_label_comp_id_M    0.024906  0.055456  1.004937e-02  30  0.052814   \n",
       "HHBlits_M                      0.024696  0.043413  2.054992e-03  30  0.046544   \n",
       "HHBlits_H                      0.021666  0.033427  6.679301e-04  30  0.038488   \n",
       "Amino acids_label_comp_id_R    0.020592  0.058527  3.190890e-02  30  0.050045   \n",
       "HHBlits_P                      0.020564  0.047763  1.266023e-02  30  0.044600   \n",
       "Amino acids_label_comp_id_A    0.019780  0.049155  1.780721e-02  30  0.044516   \n",
       "DSSP_secondary structure_T     0.019528  0.036031  2.974109e-03  30  0.037660   \n",
       "ProDy_essa                     0.018497  0.051200  2.870209e-02  30  0.044263   \n",
       "Amino acids_label_comp_id_K    0.018008  0.058215  5.045805e-02  30  0.047305   \n",
       "ProDy_mechstiff                0.017675  0.058585  5.460879e-02  30  0.047158   \n",
       "Amino acids_label_comp_id_C    0.016996  0.044670  2.303898e-02  30  0.039476   \n",
       "DSSP_NH_O_2_relidx             0.016794  0.036177  8.299639e-03  30  0.035000   \n",
       "Melodia_phi                    0.016703  0.031587  3.555673e-03  30  0.032599   \n",
       "HHBlits_G                      0.016449  0.052684  4.896361e-02  30  0.042962   \n",
       "TransferEntropy_TE             0.016397  0.056445  6.121619e-02  30  0.044803   \n",
       "Amino acids_label_comp_id_T    0.016206  0.070846  1.101158e-01  30  0.051859   \n",
       "Amino acids_label_comp_id_Q    0.014920  0.050806  5.928489e-02  30  0.040488   \n",
       "HHBlits_D->M                   0.013365  0.043694  5.229974e-02  30  0.035354   \n",
       "DSSP_O_NH_1_relidx             0.013226  0.047546  6.922055e-02  30  0.037153   \n",
       "ProDy_rmsf                     0.011520  0.050967  1.128153e-01  30  0.037169   \n",
       "HHBlits_D->D                   0.010996  0.032794  3.827645e-02  30  0.027499   \n",
       "Melodia_arc_len                0.010040  0.041382  9.713380e-02  30  0.030865   \n",
       "DSSP_phi                       0.009492  0.031017  5.222131e-02  30  0.025101   \n",
       "HHBlits_M->M                   0.007546  0.040238  1.564043e-01  30  0.027796   \n",
       "DSSP_secondary structure_-     0.007352  0.037784  1.476757e-01  30  0.026366   \n",
       "DSSP_NH_O_1_relidx             0.005913  0.036371  1.902676e-01  30  0.024216   \n",
       "\n",
       "                              p99_low  \n",
       "Amino acids_label_comp_id_F  0.054210  \n",
       "Amino acids_label_comp_id_W  0.045950  \n",
       "DSSP_O_NH_1_energy           0.031486  \n",
       "Amino acids_label_comp_id_G  0.030107  \n",
       "HHBlits_Neff                 0.018614  \n",
       "Amino acids_label_comp_id_V  0.012487  \n",
       "Amino acids_label_comp_id_I  0.005972  \n",
       "Amino acids_label_comp_id_L  0.007508  \n",
       "Amino acids_label_comp_id_H  0.007674  \n",
       "DSSP_secondary structure_E   0.011055  \n",
       "Amino acids_label_comp_id_S  0.002113  \n",
       "Amino acids_label_comp_id_Y  0.001630  \n",
       "DSSP_secondary structure_G   0.007443  \n",
       "DSSP_secondary structure_B   0.008199  \n",
       "Amino acids_label_comp_id_M -0.003002  \n",
       "HHBlits_M                    0.002849  \n",
       "HHBlits_H                    0.004844  \n",
       "Amino acids_label_comp_id_R -0.008862  \n",
       "HHBlits_P                   -0.003472  \n",
       "Amino acids_label_comp_id_A -0.004957  \n",
       "DSSP_secondary structure_T   0.001395  \n",
       "ProDy_essa                  -0.007269  \n",
       "Amino acids_label_comp_id_K -0.011288  \n",
       "ProDy_mechstiff             -0.011807  \n",
       "Amino acids_label_comp_id_C -0.005484  \n",
       "DSSP_NH_O_2_relidx          -0.001412  \n",
       "Melodia_phi                  0.000807  \n",
       "HHBlits_G                   -0.010064  \n",
       "TransferEntropy_TE          -0.012009  \n",
       "Amino acids_label_comp_id_T -0.019446  \n",
       "Amino acids_label_comp_id_Q -0.010648  \n",
       "HHBlits_D->M                -0.008623  \n",
       "DSSP_O_NH_1_relidx          -0.010701  \n",
       "ProDy_rmsf                  -0.014129  \n",
       "HHBlits_D->D                -0.005508  \n",
       "Melodia_arc_len             -0.010786  \n",
       "DSSP_phi                    -0.006117  \n",
       "HHBlits_M->M                -0.012703  \n",
       "DSSP_secondary structure_-  -0.011663  \n",
       "DSSP_NH_O_1_relidx          -0.012390  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance.iloc[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa33f43-f63a-43d5-beec-8dc295a8084a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165bff4-37b9-4268-9f8a-21d91487292a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d08fca02-7bb4-4de6-91ba-22d7e22d2a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "masker cannot be None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the SHAP explainer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.11/site-packages/shap/explainers/_explainer.py:182\u001b[0m, in \u001b[0;36mExplainer.__init__\u001b[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermutation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m=\u001b[39m explainers\u001b[38;5;241m.\u001b[39mPermutationExplainer\n\u001b[0;32m--> 182\u001b[0m     \u001b[43mexplainers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPermutationExplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinearize_link\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinearize_link\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m=\u001b[39m explainers\u001b[38;5;241m.\u001b[39mPartitionExplainer\n",
      "File \u001b[0;32m~/miniconda3/envs/autogluon/lib/python3.11/site-packages/shap/explainers/_permutation.py:50\u001b[0m, in \u001b[0;36mPermutationExplainer.__init__\u001b[0;34m(self, model, masker, link, feature_names, linearize_link, seed, **call_args)\u001b[0m\n\u001b[1;32m     47\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masker \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasker cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, masker, link\u001b[38;5;241m=\u001b[39mlink, linearize_link\u001b[38;5;241m=\u001b[39mlinearize_link, feature_names\u001b[38;5;241m=\u001b[39mfeature_names)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, Model):\n",
      "\u001b[0;31mValueError\u001b[0m: masker cannot be None."
     ]
    }
   ],
   "source": [
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer( lambda x: fit.predict_proba(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ecdfd-8576-460f-93e4-ea07ea53f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test.drop(columns=[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ca4fa-05bc-4aff-9587-30882c0feb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP values\n",
    "shap.summary_plot(shap_values, X_test.drop(columns=[\"label\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pybiomed]",
   "language": "python",
   "name": "conda-env-pybiomed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
